{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87735cbe",
   "metadata": {},
   "source": [
    "## Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "47f0bae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib # efficiently serializes python objects and is used in saving and loading machine learning models\n",
    "import random\n",
    "random.seed(42)\n",
    "import h5py\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Reformating the hdf5 file and preparing all the data for testing in a GNN\n",
    "# and saving it as multiple hdf5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d83828",
   "metadata": {},
   "source": [
    "## Reading and arranging data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cb5323b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hdf5_to_dict(hdf5_file):\n",
    "    \"\"\"\n",
    "    Convert HDF5 file to Python dictionary\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    _hdf5_to_dict(hdf5_file, data_dict)\n",
    "    return data_dict\n",
    "# Initializes an empty dictionary and calls a function to recursively\n",
    "# fill this dictionary with data from the hdf5 file.\n",
    "\n",
    "\n",
    "def _hdf5_to_dict(group, dic):\n",
    "    \"\"\"\n",
    "    Convert HDF5 group to dictionary recursively\n",
    "    \"\"\"\n",
    "    for key, item in group.items():\n",
    "        if isinstance(item, h5py.Group):\n",
    "            subgroup = {}\n",
    "            _hdf5_to_dict(item, subgroup)\n",
    "            dic[key] = subgroup\n",
    "        else:\n",
    "            dic[key] = np.array(item)\n",
    "# Iterates over items in the hdf5 group. If the item is a group, \n",
    "# it creates a new dictionary and calls itself recursively. If the item\n",
    "# is a dataset, it converts it to a numpy array and stores it in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ebca3ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell features of 100 events (100 X 180k)\n",
    "with h5py.File('/storage/mxg1065/cellFeatures_100evs.hdf5', 'r') as f:\n",
    "    # Convert HDF5 to dictionary\n",
    "    data_dict = hdf5_to_dict(f)\n",
    "# Calls the methods to open the hdf5 file and save it to a data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c6186f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f.close()\n",
    "#Closes the hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fe7e1589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#storing 100 events as dict data_0, data_1 ... data_99 not sure why I am doing this\n",
    "dynamic_variables = {}\n",
    "\n",
    "#Generate variable names dynamically and assign values to them\n",
    "for i in range(100):\n",
    "    var_name = f\"data_{i}\"\n",
    "    dynamic_variables[var_name] = data_dict[var_name]\n",
    "\n",
    "#This dictionary is similar to the previous one but it is on memory and not on hdf5\n",
    "#dynamic_variables = data_dict - check if this runs or if there was an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "269328c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alligning data feature-wise for scaling\n",
    "data_100 = np.concatenate([value for key, value in list(dynamic_variables.items())])\n",
    "\n",
    "#Collect all the values from dynamic_variables and concatenate them along the first\n",
    "#axis resulting in a single numpy array with all the event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fa0da959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18765200, 8)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_100.shape\n",
    "#Checks the shape of data_100. There are 8 features, but 100 times\n",
    "#the number of cells in the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7c4fc188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# truth labels for each neighbour edges\n",
    "hf_neighbor_truth_100evs = h5py.File(\"/storage/mxg1065/neighborLabels100Events.hdf5\", 'r')\n",
    "#Pulls the neighbor truth data from the hdf5 file.\n",
    "neighbor_truth_100evs = hf_neighbor_truth_100evs.get(\"neighborLabels100Events\")[:]\n",
    "#Puts the neighbor truth data into an array.\n",
    "hf_neighbor_truth_100evs.close()\n",
    "#Closes the neighbor truth hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bebb894b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1250242)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_truth_100evs.shape\n",
    "#Checks the shape of the neighbor truth array. 100 events,\n",
    "#each with 1.25 million unique neighbor pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367cb0c",
   "metadata": {},
   "source": [
    "## Collecteing indices of neighbour edges based on truth (1,0,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "94453364-f87e-47f7-b758-3717bd7133ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberOfDatapoints = len(neighbor_truth_100evs)\n",
    "numberOfDatapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7af66115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There are 5 types of neighbor labeling \n",
    "true = [] # true (same cluster cell pair): 1\n",
    "bkg_lone = [] # fake/bkg (lone/non-participating cell pair): 0\n",
    "bkg_cluster_lone = [] # fake/bkg (cluster-lone cell pair): 2\n",
    "bkg_lone_cluster = [] # fake/bkg (lone-cluster cell pair): 3\n",
    "bkg_cluster_cluster = [] # fake/bkg (cluster-cluster cell pair): 4\n",
    "\n",
    "for i in range(numberOfDatapoints):\n",
    "    true.append(list(np.where(neighbor_truth_100evs[i]==1)[0]))\n",
    "    bkg_lone.append(list(np.where(neighbor_truth_100evs[i]==0)[0]))\n",
    "    bkg_cluster_lone.append(list(np.where(neighbor_truth_100evs[i]==2)[0]))\n",
    "    bkg_lone_cluster.append(list(np.where(neighbor_truth_100evs[i]==3)[0]))\n",
    "    bkg_cluster_cluster.append(list(np.where(neighbor_truth_100evs[i]==4)[0]))\n",
    "\n",
    "# np.where returns a tuple (n,) where the conditions within the method are true. In the first \n",
    "# case, we find the indices where the neighbor pairs are considered true (=1, neighboring cells\n",
    "# within the same cluster). We obtain the first element [0], turning it into a list and appending\n",
    "# that to the empty true list. It's a similar concept for the other lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "446aa576-c894-472d-8808-cc89e0e11813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95782"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true[0])\n",
    "#True list and others have information for 100 events, each with a\n",
    "#different number of neighbor pairs that qualify as true. For example,\n",
    "#the first event has 95,782 true pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34beaa6",
   "metadata": {},
   "source": [
    "## Collecting the number of pairs of each edge (true, bkg_lone, bkg_cluster_lone, bkg_cluster_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c4f79639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking length is important since its complicated to deal with awkward arrays\n",
    "# Try make sample as n x m matrix (so n: events, m: true/bkg samples)\n",
    "\n",
    "true_pairNumber = []\n",
    "bkg_lone_pairNumber = []\n",
    "bkg_cluster_lone_pairNumber = []\n",
    "bkg_lone_cluster_pairNumber = []\n",
    "bkg_cluster_cluster_pairNumber = []\n",
    "for i in range(numberOfDatapoints):\n",
    "    true_pairNumber.append(len(true[i]))\n",
    "    bkg_lone_pairNumber.append(len(bkg_lone[i]))\n",
    "    bkg_cluster_lone_pairNumber.append(len(bkg_cluster_lone[i]))\n",
    "    bkg_lone_cluster_pairNumber.append(len(bkg_lone_cluster[i]))\n",
    "    bkg_cluster_cluster_pairNumber.append(len(bkg_cluster_cluster[i]))\n",
    "    \n",
    "# Makes arrays that stores the number of pairs within each type of cell-cell relationship \n",
    "# in the arrays from the previous cell. It then populates these arrays with the appropriate\n",
    "# data.\n",
    "\n",
    "true_pairNumber = np.array(true_pairNumber)\n",
    "bkg_lone_pairNumber = np.array(bkg_lone_pairNumber)\n",
    "bkg_cluster_lone_pairNumber = np.array(bkg_cluster_lone_pairNumber)\n",
    "bkg_lone_cluster_pairNumber = np.array(bkg_lone_cluster_pairNumber)\n",
    "bkg_cluster_cluster_pairNumber = np.array(bkg_cluster_cluster_pairNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2a7ccf10-26b0-4ae7-8c0a-c6e1948b31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates new arrays that represent training and testing dataset\n",
    "# We performa a training-testing data-split of 70%-30%\n",
    "\n",
    "# Train sample\n",
    "true_70 = true[:70]\n",
    "bkg_lone_70 = bkg_lone[:70]\n",
    "bkg_cluster_lone_70 = bkg_cluster_lone[:70]\n",
    "bkg_lone_cluster_70 = bkg_lone_cluster[:70]\n",
    "bkg_cluster_cluster_70 = bkg_cluster_cluster[:70]\n",
    "\n",
    "# Test sample\n",
    "true_30 = true[70:]\n",
    "bkg_lone_30 = bkg_lone[70:]\n",
    "bkg_cluster_lone_30 = bkg_cluster_lone[70:]\n",
    "bkg_lone_cluster_30 = bkg_lone_cluster[70:]\n",
    "bkg_cluster_cluster_30 = bkg_cluster_cluster[70:]\n",
    "\n",
    "# Train pairNumber\n",
    "true_pairNumber_70 = true_pairNumber[:70]\n",
    "bkg_lone_pairNumber_70 = bkg_lone_pairNumber[:70]\n",
    "bkg_cluster_lone_pairNumber_70 = bkg_cluster_lone_pairNumber[:70]\n",
    "bkg_lone_cluster_pairNumber_70 = bkg_lone_cluster_pairNumber[:70]\n",
    "bkg_cluster_cluster_pairNumber_70 = bkg_cluster_cluster_pairNumber[:70]\n",
    "\n",
    "# Test pairNumber\n",
    "true_pairNumber_30 = true_pairNumber[70:]\n",
    "bkg_lone_pairNumber_30 = bkg_lone_pairNumber[70:]\n",
    "bkg_cluster_lone_pairNumber_30 = bkg_cluster_lone_pairNumber[70:]\n",
    "bkg_lone_cluster_pairNumber_30 = bkg_lone_cluster_pairNumber[70:]\n",
    "bkg_cluster_cluster_pairNumber_30 = bkg_cluster_cluster_pairNumber[70:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc842415",
   "metadata": {},
   "source": [
    "## Checking the minimum pair_number size of each type of edges (true, bkg_l, bkg_c_l, bkg_c_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2e475c95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45600\n",
      "926119\n",
      "41654\n",
      "45689\n",
      "3334\n"
     ]
    }
   ],
   "source": [
    "print(min(true_pairNumber_70))\n",
    "print(min(bkg_lone_pairNumber_70))\n",
    "print(min(bkg_cluster_lone_pairNumber_70))\n",
    "print(min(bkg_lone_cluster_pairNumber_70))\n",
    "print(min(bkg_cluster_cluster_pairNumber_70))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddfc679",
   "metadata": {},
   "source": [
    "### Based on minimum pairNumber concluding true: 15k, bkg_l: 6_5k, bkg_c_l: 6_5k, bkg_c_c: 2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a1b91fdc-3eaf-4788-aa35-28bcea49c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSetTrueIndices = np.array([random.sample(row, 33000) for row in true_70])\n",
    "trainingSetBkgLoneLoneIndices = np.array([random.sample(row, 10000) for row in bkg_lone_70])\n",
    "trainingSetBkgClusterLoneIndices = np.array([random.sample(row, 10000) for row in bkg_cluster_lone_70])\n",
    "trainingSetBkgLoneClusterIndices = np.array([random.sample(row, 10000) for row in bkg_lone_cluster_70])\n",
    "trainingSetClusterClusterIndices = np.array([random.sample(row, 3000) for row in bkg_cluster_cluster_70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2faa7c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingSetBkgTotalIndices = np.concatenate((trainingSetBkgLoneLoneIndices,\n",
    "                                             trainingSetBkgLoneLoneIndices,\n",
    "                                             trainingSetBkgLoneClusterIndices,\n",
    "                                             trainingSetClusterClusterIndices),\n",
    "                                            axis =1)\n",
    "#Concatenates all of the sampled background arrays into one sampled background array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d8397e31-8599-401c-8047-a0ea7b9ee085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 33000)\n",
      "(70, 33000)\n"
     ]
    }
   ],
   "source": [
    "print(trainingSetTrueIndices.shape)\n",
    "print(trainingSetBkgTotalIndices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "24d529f7-7a91-4bcb-a806-951e6d0657c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 66000)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingSetTotalIndices = np.concatenate((trainingSetTrueIndices ,trainingSetBkgTotalIndices), axis =1)\n",
    "trainingSetTotalIndices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12240a26-7199-48f9-b10b-9db2e427657c",
   "metadata": {},
   "source": [
    "### Creating the four labels (1,0,2,3) for our four situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "43cd6858-891c-4eee-b224-50fc9ca1db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label 0 if both cells are non-participating (Lone-Lone)\n",
    "trainingLabelsBkgLoneEdges = np.zeros((70,10000), dtype=int)\n",
    "\n",
    "# Label 2 if first cell is from a cluster and the second cell is non-participating (Cluster-Lone)\n",
    "trainingLabelsBkgClusterLoneEdges = np.ones((70,10000), dtype=int)*2\n",
    "\n",
    "# Label 3 if first cell is non-participating and the second cell is from a cluster (Lone-Cluster)\n",
    "trainingLabelsBkgLoneClusterEdges = np.ones((70,10000), dtype=int)*3\n",
    "\n",
    "# Label 4 for cells from two different clusters (Cluster-Cluster)\n",
    "trainingLabelsBkgClusterClusterEdges = np.ones((70,3000), dtype=int)*4\n",
    "\n",
    "# Concatenate the background arrays\n",
    "trainingLabelsBkgTotalEdges = np.concatenate((trainingLabelsBkgLoneEdges, trainingLabelsBkgLoneClusterEdges,\n",
    "                                    trainingLabelsBkgClusterLoneEdges, trainingLabelsBkgClusterClusterEdges),\n",
    "                                    axis =1)\n",
    "\n",
    "# Label 1 if both cells belong to the same cluster (True)\n",
    "trainingLabelsTrueEdges = np.ones((70, 33000), dtype=int)\n",
    "\n",
    "# Creates a background array in the shape of the sampled background array. Creates an array of ones in the shape of the sampled true array. \n",
    "# These are to create arrays to show the truth value of the sampled arrays. I am going to call these truth arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "382580e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingLabelsTotal = np.concatenate((trainingLabelsTrueEdges,trainingLabelsBkgTotalEdges), axis =1)\n",
    "# Concatenate the truth arrays together to create one truth array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "41c55e24-2cc8-4dfb-be78-c2778a31e0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 4, 4, 4],\n",
       "       [1, 1, 1, ..., 4, 4, 4],\n",
       "       [1, 1, 1, ..., 4, 4, 4],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 4, 4, 4],\n",
       "       [1, 1, 1, ..., 4, 4, 4],\n",
       "       [1, 1, 1, ..., 4, 4, 4]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingLabelsTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "25ac3ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 66000)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingLabelsTotal.shape\n",
    "# Check the shape of the truth array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "81067b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "randomIndexTraining = []\n",
    "for i in range(len(trainingLabelsTotal)):\n",
    "    arr = np.arange(len(trainingLabelsTotal[0]))\n",
    "    np.random.shuffle(arr)\n",
    "    randomIndexTraining.append(arr)\n",
    "#Creates an array that releases a random sequence of indices. I’m going to call this the random indices array.\n",
    "\n",
    "randomIndexTraining = np.array(randomIndexTraining)\n",
    "#Converts the random indices array into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7b1eb62d-c3f8-4dde-8613-14b5d433a898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 66000)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomIndexTraining.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3fe41806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Randomizing training sample\n",
    "randomTrainingIndicesTotal = []\n",
    "for i in range(len(trainingLabelsTotal)):\n",
    "    randomTrainingIndicesTotal.append(trainingSetTotalIndices[i][randomIndexTraining[i]])\n",
    "#Uses the random indices array to randomize the training array and saves it into a randomized training array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ad297532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Randomizing truth of training sample according to the sample\n",
    "randomTrainingLabelsTotal = []\n",
    "for i in range(len(trainingLabelsTotal)):\n",
    "    randomTrainingLabelsTotal.append(trainingLabelsTotal[i][randomIndexTraining[i]])\n",
    "#Uses the random indices array to randomize the truth array and saves it into a randomized truth array.\n",
    "randomTrainingIndicesTotal = np.array(randomTrainingIndicesTotal)\n",
    "#Converts the randomized training array into a numpy array.\n",
    "randomTrainingLabelsTotal = np.array(randomTrainingLabelsTotal)\n",
    "#Converts the randomized truth array into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "87b38456-10d8-4272-ac81-36fdd7741d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 66000)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomTrainingLabelsTotal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "467e95b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File('/storage/mxg1065/MultiClassGNN/totalTrainingLabelsRandom.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"totalTrainingLabelsRandom\", data = randomTrainingLabelsTotal)\n",
    "#Saves the randomized truth array to an hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4baafb44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 66000)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomTrainingIndicesTotal.shape\n",
    "#Prints the shape of the randomized training array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "568e5fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_neighbor_pairs_unique_sorted= h5py.File(\"/storage/mxg1065/neighborPairsUniqueSorted.hdf5\", 'r')\n",
    "neighbor_pairs_unique_sorted = hf_neighbor_pairs_unique_sorted.get(\"neighbor_pair\")[:]\n",
    "hf_neighbor_pairs_unique_sorted.close()\n",
    "#Retrieves the sorted neighbor pairs from an hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9820b81f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Arranging training edges with training indices\n",
    "totalTrainingEdgesRandom = []\n",
    "for i in range(len(trainingLabelsTotal)):\n",
    "    totalTrainingEdgesRandom.append(neighbor_pairs_unique_sorted[randomTrainingIndicesTotal[i]])\n",
    "#Creates a training edges array by using the randomized training indices on the sorted neighbor pairs array.\n",
    "\n",
    "totalTrainingEdgesRandom = np.array(totalTrainingEdgesRandom)\n",
    "#Convert the training edges array into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2dee2224",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 66000, 2)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalTrainingEdgesRandom.shape\n",
    "#70 events, each with 54,000 pair numbers, each with two cell indices that are neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "19b98eae-3111-4512-8103-4e9f386774dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 1250242, 2)\n",
      "(70, 1250242, 2)\n"
     ]
    }
   ],
   "source": [
    "# Arranging training edges with training indices\n",
    "uniDirEdgeIndicesTrain = []\n",
    "for i in range(len(trainingLabelsTotal)):\n",
    "    uniDirEdgeIndicesTrain.append(neighbor_pairs_unique_sorted)\n",
    "\n",
    "uniDirEdgeIndicesTrain = np.array(uniDirEdgeIndicesTrain)\n",
    "print(uniDirEdgeIndicesTrain.shape)\n",
    "\n",
    "\n",
    "totalTrainingEdgesRandom = []\n",
    "for i in range(len(trainingLabelsTotal)):\n",
    "    totalTrainingEdgesRandom.append(neighbor_pairs_unique_sorted)\n",
    "    # totalTrainingEdgesRandom.append(neighbor_pairs_unique[total_training_indices_rand[i]])\n",
    "#Creates a training edges array by using the randomized training indices on the sorted neighbor pairs array.\n",
    "\n",
    "totalTrainingEdgesRandom = np.array(totalTrainingEdgesRandom)\n",
    "#Convert the training edges array into a numpy array.\n",
    "\n",
    "print(totalTrainingEdgesRandom.shape)\n",
    "#70 events, each with 54,000 pair numbers, each with two cell indices that are neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "82e95696-221e-47a9-b02b-2e1949893603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createArray(inputData, numberOfData, source, BD):\n",
    "    outputData = []\n",
    "    for i in range(numberOfData):\n",
    "        _outputData = []\n",
    "        for pair in inputData[i]:\n",
    "            if BD:\n",
    "                if source:\n",
    "                    _outputData.append(pair[0])\n",
    "                    _outputData.append(pair[1])\n",
    "                else:\n",
    "                    _outputData.append(pair[1])\n",
    "                    _outputData.append(pair[0])\n",
    "            else:\n",
    "                if source:\n",
    "                    _outputData.append(pair[0])\n",
    "                else:\n",
    "                    _outputData.append(pair[1])\n",
    "        outputData.append(_outputData)\n",
    "    return outputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "86e27b6b-495e-4d22-954c-a6e390f4506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 2500484)\n",
      "(70, 2500484)\n",
      "(70, 1250242)\n",
      "(70, 1250242)\n"
     ]
    }
   ],
   "source": [
    "train_edge_source_BD = createArray(uniDirEdgeIndicesTrain, 70, True, True)\n",
    "train_edge_source_BD = np.array(train_edge_source_BD)\n",
    "#Converts the train_edge_source_BD array into a numpy array.\n",
    "\n",
    "train_edge_dest_BD = createArray(uniDirEdgeIndicesTrain, 70, False, True)\n",
    "train_edge_dest_BD = np.array(train_edge_dest_BD)\n",
    "#Converts the train_edge_dest_BD array into a numpy array.\n",
    "\n",
    "\n",
    "train_edge_source_noBD = createArray(totalTrainingEdgesRandom, 70, True, False)\n",
    "train_edge_source_noBD = np.array(train_edge_source_noBD)\n",
    "train_edge_dest_noBD = createArray(totalTrainingEdgesRandom, 70, False, False)\n",
    "train_edge_dest_noBD = np.array(train_edge_dest_noBD)\n",
    "\n",
    "print(train_edge_source_BD.shape) #The bidirectionality doubles the number of pairs counted from 30,000 to 60,000\n",
    "#Prints the shape of the train_edge_source_BD array.\n",
    "print(train_edge_dest_BD.shape)\n",
    "#Prints the shape of the train_edge_dest_BD array.\n",
    "print(train_edge_source_noBD.shape)\n",
    "#Prints the shape of the train_edge_source_noBD array.\n",
    "print(train_edge_dest_noBD.shape)\n",
    "#Prints the shape of the train_edge_dest_noBD array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "854930bf-c9f3-43a2-adc3-797462f4f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/storage/mxg1065/MultiClassGNN/train_edge_source_BD_70evs.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"train_edge_source_BD\", data = train_edge_source_BD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/train_edge_dest_BD_70evs.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"train_edge_dest_BD\", data = train_edge_dest_BD)\n",
    "    \n",
    "    \n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/train_edge_source_noBD_70evs.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"train_edge_source_noBD\", data = train_edge_source_noBD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/train_edge_dest_noBD_70evs.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"train_edge_dest_noBD\", data = train_edge_dest_noBD)\n",
    "#Saves the BD and noBD arrays into hdf5 files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ee2aa-ab23-46a8-bc35-8d4ae10f1491",
   "metadata": {},
   "source": [
    "# Scaling of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fc27685a-f1fa-438c-a03c-36befb60b9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keys = list(dynamic_variables.keys())\n",
    "values = list(dynamic_variables.values())\n",
    "#Gets the keys and variables from the dynamic variables array and stores them in arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2e11b853-2e12-4a5d-b58d-905c075f90c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictDymanicVariables = dict(zip(keys, values))\n",
    "#Creates a rearranged dictionary out of the keys and sorted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "dd3beeee-1712-424c-a533-343cd4d0478a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_70 = np.concatenate([value for key, value in list(dictDymanicVariables.items())[:70]])\n",
    "#Create a training data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "418ad827-d013-41a9-8506-e77a387d6c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_30 = np.concatenate([value for key, value in list(dynamic_variables.items())[70:]])\n",
    "#Create a data testing array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7d2033d0-5ee1-4406-b5bb-1d5a504ab0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/storage/mxg1065/MultiClassGNN/multi_scaler_neighbor_data_70.save']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "cellFeatures_trainS = scaler.fit_transform(data_70)\n",
    "scaler_filename = \"/storage/mxg1065/MultiClassGNN/multi_scaler_neighbor_data_70.save\"\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "#Scales the training data with a minmaxscaler, put that scaled data into a training features array, and then save that scaler into a .save file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e9cd0ff9-88e9-4425-91a6-3c21d19ca841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cellFeatures_trainS_reshape = cellFeatures_trainS.reshape(70, 187652, 8)\n",
    "#Reshape the cell feature training array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6d1dd5e0-256e-4faf-9776-e62ac26a5422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = joblib.load('/storage/mxg1065/MultiClassGNN/multi_scaler_neighbor_data_70.save') \n",
    "cellFeatures_testS = scaler.transform(data_30)\n",
    "#Retrieve the scaler from the .save file. Use the scaler on the test data to create a cell feature test data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ac254b2e-a0e3-4f43-a216-d0d45485f6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cellFeatures_testS_reshape = cellFeatures_testS.reshape(30, 187652, 8)\n",
    "#Reshape the cell feature testing array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7776ef70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating Scaled Cell Feature file\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_cellFeaturesScaled_train_70evs.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_cellFeatures_trainS\", data = cellFeatures_trainS_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "eeaade9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_cellFeaturesScaled_test_30evs.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_cellFeatures_testS\", data = cellFeatures_testS_reshape)\n",
    "#Save the cell features arrays into hdf5 files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab58f33-8670-4510-b164-7eb7a6c3d45b",
   "metadata": {},
   "source": [
    "### for test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f56e65e6-00b9-409e-8cb6-27da7acec071",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingSetTrueIndices = np.array([random.sample(row, 33000) for row in true_30])\n",
    "testingSetBkgLoneLoneIndices = np.array([random.sample(row, 10000) for row in bkg_lone_30])\n",
    "testingSetBkgClusterLoneIndices = np.array([random.sample(row, 10000) for row in bkg_cluster_lone_30])\n",
    "testingSetBkgLoneClusterIndices = np.array([random.sample(row, 10000) for row in bkg_lone_cluster_30])\n",
    "testingSetBkgClusterClusterIndices = np.array([random.sample(row, 3000) for row in bkg_cluster_cluster_30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4b9fe182-1a51-411b-92a6-ee8e4048a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingSetBkgTotalIndices = np.concatenate((testingSetBkgLoneLoneIndices,\n",
    "                                             testingSetBkgLoneLoneIndices,\n",
    "                                             testingSetBkgLoneClusterIndices,\n",
    "                                             testingSetClusterClusterIndices),\n",
    "                                            axis =1)\n",
    "#Concatenates all of the sampled background arrays into one sampled background array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f6a477b9-213a-4f18-8a0a-03bf0e3f0eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 66000)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testingSetTotalIndices = np.concatenate((testingSetTrueIndices ,testingSetBkgTotalIndices), axis =1)\n",
    "testingSetTotalIndices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "426872ac-a8c9-4511-b776-29787198a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label 0 if both cells are non-participating (Lone-Lone)\n",
    "testingLabelsBkgLoneEdges = np.zeros((30,10000), dtype=int)\n",
    "\n",
    "# Label 2 if first cell is from a cluster and the second cell is non-participating (Cluster-Lone)\n",
    "testingLabelsBkgClusterLoneEdges = np.ones((30,10000), dtype=int)*2\n",
    "\n",
    "# Label 3 if first cell is non-participating and the second cell is from a cluster (Lone-Cluster)\n",
    "testingLabelsBkgLoneClusterEdges = np.ones((30,10000), dtype=int)*3\n",
    "\n",
    "# Label 4 for cells from two different clusters (Cluster-Cluster)\n",
    "testingLabelsBkgClusterClusterEdges = np.ones((30,3000), dtype=int)*4\n",
    "\n",
    "# Concatenate the background arrays\n",
    "testingLabelsBkgTotalEdges = np.concatenate((testingLabelsBkgLoneEdges, testingLabelsBkgLoneClusterEdges,\n",
    "                                    testingLabelsBkgClusterLoneEdges, testingLabelsBkgClusterClusterEdges),\n",
    "                                    axis =1)\n",
    "\n",
    "# Label 1 if both cells belong to the same cluster (True)\n",
    "testingLabelsTrueEdges = np.ones((30, 33000), dtype=int)\n",
    "\n",
    "# Creates a background array in the shape of the sampled background array. Creates an array of ones in the shape of the sampled true array. \n",
    "# These are to create arrays to show the truth value of the sampled arrays. I am going to call these truth arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8aa19687-3de4-4980-9645-e9d5813866dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingLabelsTotal = np.concatenate((testingLabelsTrueEdges,testingLabelsBkgTotalEdges), axis =1)\n",
    "# Concatenate the truth arrays together to create one truth array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "aed38889-790e-4578-a72c-3817c003e297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 66000)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testingLabelsTotal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "202dc635-abd3-4874-a628-e39a43b1b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomIndexTesting = []\n",
    "for i in range(len(testingLabelsTotal)):\n",
    "    arr = np.arange(len(testingLabelsTotal[0]))\n",
    "    np.random.shuffle(arr)\n",
    "    randomIndexTesting.append(arr)\n",
    "#Creates an array that releases a random sequence of indices. I’m going to call this the random indices array.\n",
    "\n",
    "randomIndexTesting = np.array(randomIndexTesting)\n",
    "#Converts the random indices array into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "27dfdc9e-b22e-4c8e-bb43-422b729c6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomizing testing sample\n",
    "randomTestingIndicesTotal = []\n",
    "for i in range(len(testingLabelsTotal)):\n",
    "    randomTestingIndicesTotal.append(testingSetTotalIndices[i][randomIndexTesting[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9d87d870-b601-4e01-888d-2647007859d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomizing truth of testing sample according to the sample\n",
    "randomTestingLabelsTotal = []\n",
    "for i in range(len(testingLabelsTotal)):\n",
    "    randomTestingLabelsTotal.append(testingLabelsTotal[i][randomIndexTesting[i]])\n",
    "#Uses the random indices array to randomize the truth array and saves it into a randomized truth array.\n",
    "randomTestingIndicesTotal = np.array(randomTestingIndicesTotal)\n",
    "#Converts the randomized testing array into a numpy array.\n",
    "randomTestingLabelsTotal = np.array(randomTestingLabelsTotal)\n",
    "#Converts the randomized truth array into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "07653ad0-7316-4db2-99da-d094fd9d474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/storage/mxg1065/MultiClassGNN/totalTestingLabelsRandom.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"totalTestingLabelsRandom\", data = randomTestingLabelsTotal)\n",
    "#Saves the randomized truth array to an hdf5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3a8691ad-21df-49eb-ae58-f9d807cb1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arranging testing edges with testing indices\n",
    "totalTestingEdgesRandom = []\n",
    "for i in range(len(testingLabelsTotal)):\n",
    "    totalTestingEdgesRandom.append(neighbor_pairs_unique_sorted[randomTestingIndicesTotal[i]])\n",
    "#Creates a testing edges array by using the randomized testing indices on the sorted neighbor pairs array.\n",
    "\n",
    "totalTestingEdgesRandom = np.array(totalTestingEdgesRandom)\n",
    "#Convert the testing edges array into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "466079ed-3b46-425d-98f5-943c1f5dc069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 1250242, 2)\n",
      "(30, 1250242, 2)\n"
     ]
    }
   ],
   "source": [
    "# Arranging testing edges with testing indices\n",
    "uniDirEdgeIndicesTest = []\n",
    "for i in range(len(testingLabelsTotal)):\n",
    "    uniDirEdgeIndicesTest.append(neighbor_pairs_unique_sorted)\n",
    "\n",
    "uniDirEdgeIndicesTest = np.array(uniDirEdgeIndicesTest)\n",
    "print(uniDirEdgeIndicesTest.shape)\n",
    "\n",
    "\n",
    "totalTestingEdgesRandom = []\n",
    "for i in range(len(testingLabelsTotal)):\n",
    "    totalTestingEdgesRandom.append(neighbor_pairs_unique_sorted)\n",
    "    # totalTestingEdgesRandom.append(neighbor_pairs_unique[total_testing_indices_rand[i]])\n",
    "#Creates a testing edges array by using the randomized testing indices on the sorted neighbor pairs array.\n",
    "\n",
    "totalTestingEdgesRandom = np.array(totalTestingEdgesRandom)\n",
    "#Convert the testing edges array into a numpy array.\n",
    "\n",
    "print(totalTestingEdgesRandom.shape)\n",
    "#70 events, each with 54,000 pair numbers, each with two cell indices that are neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e3751487-ddd5-49d8-98e0-3139f1d5cb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2500484)\n",
      "(30, 2500484)\n",
      "(30, 1250242)\n",
      "(30, 1250242)\n"
     ]
    }
   ],
   "source": [
    "test_edge_source_BD = createArray(uniDirEdgeIndicesTest, 30, True, True)\n",
    "test_edge_source_BD = np.array(test_edge_source_BD)\n",
    "#Converts the test_edge_source_BD array into a numpy array.\n",
    "\n",
    "test_edge_dest_BD = createArray(uniDirEdgeIndicesTest, 30, False, True)\n",
    "test_edge_dest_BD = np.array(test_edge_dest_BD)\n",
    "#Converts the test_edge_dest_BD array into a numpy array.\n",
    "\n",
    "\n",
    "test_edge_source_noBD = createArray(totalTestingEdgesRandom, 30, True, False)\n",
    "test_edge_source_noBD = np.array(test_edge_source_noBD)\n",
    "test_edge_dest_noBD = createArray(totalTestingEdgesRandom, 30, False, False)\n",
    "test_edge_dest_noBD = np.array(test_edge_dest_noBD)\n",
    "\n",
    "print(test_edge_source_BD.shape) #The bidirectionality doubles the number of pairs counted from 30,000 to 60,000\n",
    "#Prints the shape of the test_edge_source_BD array.\n",
    "print(test_edge_dest_BD.shape)\n",
    "#Prints the shape of the test_edge_dest_BD array.\n",
    "print(test_edge_source_noBD.shape)\n",
    "#Prints the shape of the test_edge_source_noBD array.\n",
    "print(test_edge_dest_noBD.shape)\n",
    "#Prints the shape of the test_edge_dest_noBD array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0405ed97-3a67-4d95-affe-309b119853a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/storage/mxg1065/MultiClassGNN/test_edge_source_BD_30evs.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"test_edge_source_BD\", data = test_edge_source_BD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/test_edge_dest_BD_30evs.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"test_edge_dest_BD\", data = test_edge_dest_BD)\n",
    "    \n",
    "    \n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/test_edge_source_noBD_30evs.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"test_edge_source_noBD\", data = test_edge_source_noBD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/test_edge_dest_noBD_30evs.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"test_edge_dest_noBD\", data = test_edge_dest_noBD)\n",
    "#Saves the BD and noBD arrays into hdf5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9010a67e-d7da-43b3-9671-b7fcedeba170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testingEdgesTrue = []\n",
    "testingEdgesBkgLone = []\n",
    "testingEdgesBkgClusterLone = []\n",
    "testingEdgesBkgLoneCluster = []\n",
    "testingEdgesBkgClusterCluster = []\n",
    "\n",
    "for i in range(30):\n",
    "    testingEdgesTrue.append(neighbor_pairs_unique_sorted[testingSetTrueIndices[i]])\n",
    "    testingEdgesBkgLone.append(neighbor_pairs_unique_sorted[testingSetBkgLoneLoneIndices[i]])\n",
    "    testingEdgesBkgClusterLone.append(neighbor_pairs_unique_sorted[testingSetBkgClusterLoneIndices[i]])\n",
    "    testingEdgesBkgLoneCluster.append(neighbor_pairs_unique_sorted[testingSetBkgLoneClusterIndices[i]])\n",
    "    testingEdgesBkgClusterCluster.append(neighbor_pairs_unique_sorted[testingSetBkgClusterClusterIndices[i]])\n",
    "#Create a test edges arrays by putting the sampled length indices into the sorted neighbor arrays.\n",
    "\n",
    "testingEdgesTrue = np.array(testingEdgesTrue)\n",
    "testingEdgesBkgLone = np.array(testingEdgesBkgLone)\n",
    "testingEdgesBkgClusterLone = np.array(testingEdgesBkgClusterLone)\n",
    "testingEdgesBkgLoneCluster = np.array(testingEdgesBkgLoneCluster)\n",
    "testingEdgesBkgClusterCluster = np.array(testingEdgesBkgClusterCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d9c583ab-f9da-4f5e-93cb-c943d3ff6b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_edge_source_true_BD = createArray(testingEdgesTrue, 30, True, True)\n",
    "\n",
    "test_edge_dest_true_BD = createArray(testingEdgesTrue, 30, False, True)\n",
    "\n",
    "test_edge_source_bkg_lone_BD = createArray(testingEdgesBkgLone, 30, True, True)\n",
    "\n",
    "test_edge_dest_bkg_lone_BD = createArray(testingEdgesBkgLone, 30, False, True)\n",
    "\n",
    "test_edge_source_bkg_cluster_lone_BD = createArray(testingEdgesBkgClusterLone, 30, True, True)\n",
    "\n",
    "test_edge_dest_bkg_cluster_lone_BD = createArray(testingEdgesBkgClusterLone, 30, False, True)\n",
    "\n",
    "test_edge_source_bkg_lone_cluster_BD = createArray(testingEdgesBkgLoneCluster, 30, True, True)\n",
    "\n",
    "test_edge_dest_bkg_lone_cluster_BD = createArray(testingEdgesBkgLoneCluster, 30, False, True)\n",
    "\n",
    "test_edge_source_bkg_cluster_cluster_BD = createArray(testingEdgesBkgClusterCluster, 30, True, True)\n",
    "\n",
    "test_edge_dest_bkg_cluster_cluster_BD = createArray(testingEdgesBkgClusterCluster, 30, False, True)\n",
    "\n",
    "test_edge_source_true_noBD = createArray(testingEdgesTrue, 30, True, False)\n",
    "\n",
    "test_edge_dest_true_noBD = createArray(testingEdgesTrue, 30, False, False)\n",
    "\n",
    "test_edge_source_bkg_lone_noBD = createArray(testingEdgesBkgLone, 30, True, False)\n",
    "\n",
    "test_edge_dest_bkg_lone_noBD = createArray(testingEdgesBkgLone, 30, False, False)\n",
    "\n",
    "test_edge_source_bkg_cluster_lone_noBD = createArray(testingEdgesBkgClusterLone, 30, True, False)\n",
    "\n",
    "test_edge_dest_bkg_cluster_lone_noBD = createArray(testingEdgesBkgClusterLone, 30, False, False)\n",
    "\n",
    "test_edge_source_bkg_lone_cluster_noBD = createArray(testingEdgesBkgLoneCluster, 30, True, False)\n",
    "\n",
    "test_edge_dest_bkg_lone_cluster_noBD = createArray(testingEdgesBkgLoneCluster, 30, False, False)\n",
    "\n",
    "test_edge_source_bkg_cluster_cluster_noBD = createArray(testingEdgesBkgClusterCluster, 30, True, False)\n",
    "\n",
    "test_edge_dest_bkg_cluster_cluster_noBD = createArray(testingEdgesBkgClusterCluster, 30, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8d9adaa9-28c5-457b-a82e-1e25ac738286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_source_true_BD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_source_true_BD\", data = test_edge_source_true_BD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_true_BD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_dest_true_BD\", data = test_edge_dest_true_BD)\n",
    "    \n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_source_true_noBD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_source_true_noBD\", data = test_edge_source_true_noBD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_true_noBD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_dest_true_noBD\", data = test_edge_dest_true_noBD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_lone_BD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_source_bkg_lone_BD\", data = test_edge_source_bkg_lone_BD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_lone_BD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_dest_bkg_lone_BD\", data = test_edge_dest_bkg_lone_BD)\n",
    "    \n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_lone_noBD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_source_bkg_lone_noBD\", data = test_edge_source_bkg_lone_noBD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_lone_noBD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_dest_bkg_lone_noBD\", data = test_edge_dest_bkg_lone_noBD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_cluster_lone_BD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_source_bkg_cluster_lone_BD\", data = test_edge_source_bkg_cluster_lone_BD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_cluster_lone_BD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_dest_bkg_cluster_lone_BD\", data = test_edge_dest_bkg_cluster_lone_BD)\n",
    "    \n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_cluster_lone_noBD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_source_bkg_cluster_lone_noBD\", data = test_edge_source_bkg_cluster_lone_noBD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_cluster_lone_noBD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_dest_bkg_cluster_lone_noBD\", data = test_edge_dest_bkg_cluster_lone_noBD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_lone_cluster_BD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_source_bkg_lone_cluster_BD\", data = test_edge_source_bkg_lone_cluster_BD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_lone_cluster_BD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_dest_bkg_lone_cluster_BD\", data = test_edge_dest_bkg_lone_cluster_BD)\n",
    "    \n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_lone_cluster_noBD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_source_bkg_lone_cluster_noBD\", data = test_edge_source_bkg_lone_cluster_noBD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_lone_cluster_noBD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_dest_bkg_lone_cluster_noBD\", data = test_edge_dest_bkg_lone_cluster_noBD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_cluster_cluster_BD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_source_bkg_cluster_cluster_BD\", data = test_edge_source_bkg_cluster_cluster_BD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_cluster_cluster_BD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_dest_bkg_cluster_cluster_BD\", data = test_edge_dest_bkg_cluster_cluster_BD)\n",
    "    \n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_cluster_cluster_noBD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_source_bkg_cluster_cluster_noBD\", data = test_edge_source_bkg_cluster_cluster_noBD)\n",
    "\n",
    "with h5py.File('/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_cluster_cluster_noBD.hdf5', 'w') as f: \n",
    "    dset = f.create_dataset(\"multi_test_edge_dest_bkg_cluster_cluster_noBD\", data = test_edge_dest_bkg_cluster_cluster_noBD)\n",
    "#Saves these BD and noBD arrays into hdf5 files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
