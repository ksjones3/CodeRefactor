{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from sklearn.preprocessing import MinMaxScaler, label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import uproot\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = uproot.open('/storage/mxg1065/MyxAODAnalysis_super3D.outputs.root')\n",
    "tree = file['analysis;1']\n",
    "branches = tree.arrays()\n",
    "\n",
    "# 100 events and 187652 cells\n",
    "# Arrays containing information about the energy, noise, snr, \n",
    "cell_e = np.array(branches['cell_e'])\n",
    "cell_noise = np.array(branches['cell_noiseSigma'])\n",
    "cell_snr = np.array(branches['cell_SNR'])\n",
    "cell_eta = np.array(branches['cell_eta'])\n",
    "cell_phi = np.array(branches['cell_phi'])\n",
    "\n",
    "# Represents the index of the cluster that each cell corresponds to. If the index\n",
    "# is 0, that means that the given cell does not belong to a cluster.\n",
    "cell_to_cluster_index = np.array(branches['cell_cluster_index'])\n",
    "\n",
    "# For each entry, contains the IDs of cells neighboring a given cell\n",
    "neighbor = branches['neighbor']\n",
    "\n",
    "num_of_events = len(cell_e) # 100 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02640459 0.24943821 0.09700817 0.23466232 0.5085498 ]\n",
      " [0.02567646 0.24627675 0.09700815 0.23466876 0.52418756]\n",
      " [0.02508186 0.24369499 0.09700817 0.23467347 0.5398244 ]\n",
      " ...\n",
      " [0.02632877 0.24791998 0.03177016 0.62017125 0.4921177 ]\n",
      " [0.02705116 0.2511391  0.07512318 0.6461531  0.4921177 ]\n",
      " [0.02638626 0.24820389 0.04149057 0.6731673  0.4921177 ]]\n",
      "(187652, 5)\n"
     ]
    }
   ],
   "source": [
    "# We use the data arrays to crete a data dictionary, where each entry corresponds\n",
    "# to the data of a given event; we scale this data.\n",
    "data = {}\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    data[f'data_{i}'] = np.concatenate((np.expand_dims(cell_snr[i], axis=1),\n",
    "                                        np.expand_dims(cell_e[i], axis=1),\n",
    "                                        np.expand_dims(cell_noise[i], axis=1),\n",
    "                                        np.expand_dims(cell_eta[i], axis=1),\n",
    "                                        np.expand_dims(cell_phi[i], axis=1)), axis=1)\n",
    "    \n",
    "# We combine the data into one array and apply the MinMaxScaler\n",
    "combined_data = np.vstack([data[key] for key in data])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_combined_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "# The scaled data is split to have the save structure as the original data dict\n",
    "scaled_data = {}\n",
    "start_idx = 0\n",
    "for i in range(num_of_events):\n",
    "    end_idx = start_idx + data[f\"data_{i}\"].shape[0]\n",
    "    scaled_data[f\"data_{i}\"] = scaled_combined_data[start_idx:end_idx]\n",
    "    start_idx = end_idx\n",
    "\n",
    "print(scaled_data[\"data_0\"])\n",
    "print(scaled_data[\"data_0\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any broken cells within the list of cells (These cells have\n",
    "# no noise, which doesn't make sense)\n",
    "\n",
    "broken_cells = []\n",
    "for i in range(num_of_events):\n",
    "    cells = np.argwhere(cell_noise[i]==0).flatten()\n",
    "    broken_cells = np.squeeze(cells)\n",
    "\n",
    "neighbor = neighbor[0]\n",
    "\n",
    "neighbor_pairs_list = []\n",
    "num_of_cells = len(neighbor) # 187652 cells\n",
    "\n",
    "for i in range(num_of_cells):\n",
    "    if i in broken_cells:\n",
    "        continue\n",
    "    for j in neighbor[i]:\n",
    "        if j in broken_cells:\n",
    "            continue\n",
    "        neighbor_pairs_list.append((i, int(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250242, 2)\n"
     ]
    }
   ],
   "source": [
    "# These functions remove permutation variants\n",
    "def canonical_form(t):\n",
    "    return tuple(sorted(t))\n",
    "\n",
    "def remove_permutation_variants(tuple_list):\n",
    "    unique_tuples = set(canonical_form(t) for t in tuple_list)\n",
    "    return [tuple(sorted(t)) for t in unique_tuples]\n",
    "\n",
    "neighbor_pairs_list = np.array(remove_permutation_variants(neighbor_pairs_list))\n",
    "print(neighbor_pairs_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1250242)\n",
      "[[0 0 0 ... 0 0 2]\n",
      " [3 3 0 ... 0 0 2]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 2 0 0]]\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Initialize array for labels\n",
    "labels_for_neighbor_pairs = np.zeros((num_of_events, len(neighbor_pairs_list)), dtype=int)\n",
    "\n",
    "# Extracting the individual cells within a cell pair\n",
    "cell_0 = cell_to_cluster_index[:, neighbor_pairs_list[:, 0]]\n",
    "cell_1 = cell_to_cluster_index[:, neighbor_pairs_list[:, 1]]\n",
    "\n",
    "# Computing labels using vectorized operations\n",
    "same_cluster = cell_0 == cell_1 \n",
    "both_nonzero = (cell_0 != 0) & (cell_1 != 0)\n",
    "\n",
    "labels_for_neighbor_pairs[same_cluster & (cell_0 == 0)] = 0 # Lone-Lone (0)\n",
    "labels_for_neighbor_pairs[same_cluster & (cell_0 != 0)] = 1 # True-True (1)\n",
    "labels_for_neighbor_pairs[~same_cluster & both_nonzero] = 4 # Cluster-Cluster (4)\n",
    "labels_for_neighbor_pairs[~same_cluster & (cell_0 == 0) & (cell_1 != 0)] = 3 # Lone-Cluster (3)\n",
    "labels_for_neighbor_pairs[~same_cluster & (cell_0 != 0) & (cell_1 == 0)] = 2 # Cluster-Lone (2)\n",
    "\n",
    "print(labels_for_neighbor_pairs.shape)\n",
    "print(labels_for_neighbor_pairs)\n",
    "print(np.unique(labels_for_neighbor_pairs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassBatchGenerator(IterableDataset):\n",
    "    def __init__(self, x, neighbor_pairs, labels_for_neighbor_pairs, batch_size, class_counts):\n",
    "        \"\"\"\n",
    "        Custom IterableDataset for multi-class batch generation.\n",
    "\n",
    "        Arguments:\n",
    "        x: (np.ndarray) Feature matrix from dict of shape data[\"data_i\"].shape = (num_of_cells, num_features).\n",
    "        neighbor_pairs: (np.ndarray) Array of cell pair indices, shape (num_pairs, 2).\n",
    "        labels_for_neighbor_pairs: (np.ndarray) Array of labels per pair, shape (num_of_events, num_pairs).\n",
    "        batch_size: (int) Total number of samples per batch.\n",
    "        class_counts: (dict) Number of samples per class per batch, {class_label: num_samples_per_batch}.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.neighbor_pairs = neighbor_pairs\n",
    "        self.labels_for_neighbor_pairs = labels_for_neighbor_pairs\n",
    "        self.batch_size = batch_size\n",
    "        self.class_counts = class_counts\n",
    "\n",
    "        # Store indices per class for each event\n",
    "        self.indices_per_event = self._compute_event_class_indices()\n",
    "\n",
    "    def _compute_event_class_indices(self):\n",
    "        \"\"\"Precompute indices of pairs for each event and class.\"\"\"\n",
    "        event_class_indices = {}\n",
    "        num_events = self.labels_for_neighbor_pairs.shape[0]\n",
    "\n",
    "        for event_id in range(num_events):\n",
    "            event_class_indices[event_id] = {}\n",
    "            for cls in np.unique(self.labels_for_neighbor_pairs[event_id]):\n",
    "                mask = self.labels_for_neighbor_pairs[event_id] == cls\n",
    "                event_class_indices[event_id][cls] = np.where(mask)[0]\n",
    "\n",
    "        return event_class_indices\n",
    "\n",
    "    def _batch_generator(self):\n",
    "        \"\"\"Generator function that iterates over events and selects samples per class.\"\"\"\n",
    "        num_events = self.labels_for_neighbor_pairs.shape[0]\n",
    "\n",
    "        for event_id in range(num_events):  # Iterate over events first\n",
    "            selected_pairs = []\n",
    "            selected_labels = []\n",
    "\n",
    "            for cls, n_samples in self.class_counts.items():  # Iterate over classes\n",
    "                if cls not in self.indices_per_event[event_id]:\n",
    "                    continue  # Skip if no pairs for this class\n",
    "\n",
    "                pair_indices = self.indices_per_event[event_id][cls]\n",
    "                if len(pair_indices) < n_samples:\n",
    "                    selected_idx = pair_indices  # Take all available samples\n",
    "                else:\n",
    "                    selected_idx = np.random.choice(pair_indices, size=n_samples, replace=False)  # Sample without replacement\n",
    "\n",
    "                # Gather the selected pairs and corresponding labels\n",
    "                # selected_pairs.append(self.neighbor_pairs[event_id][selected_idx])\n",
    "                selected_pairs.append(self.neighbor_pairs[selected_idx])\n",
    "                selected_labels.append(np.full(len(selected_idx), cls))  # Assign class labels\n",
    "\n",
    "            # If no samples were selected, skip this event\n",
    "            if len(selected_pairs) == 0:\n",
    "                continue\n",
    "\n",
    "            # Concatenate results\n",
    "            selected_pairs = np.concatenate(selected_pairs, axis=0)\n",
    "            selected_labels = np.concatenate(selected_labels, axis=0)\n",
    "\n",
    "            # Shuffle within the event before yielding\n",
    "            perm = np.random.permutation(len(selected_labels))\n",
    "            selected_pairs = selected_pairs[perm]\n",
    "            selected_labels = selected_labels[perm]\n",
    "\n",
    "            # Yield x for this event along with (x1, x2) pairs and labels\n",
    "            yield self.x[f\"data_{event_id}\"], selected_pairs, selected_labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batch_generator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing some checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_combined_data.shape: (18765200, 5)\n",
      "neighbor_pairs_list.shape: (1250242, 2)\n",
      "labels_for_neighbor_pairs.shape: (100, 1250242)\n",
      "x_batch shape: (187652, 5)\n",
      "neighbor_pairs_batch shape: (163000, 2)\n",
      "labels_batch shape: (163000,)\n",
      "Class distribution in batch: Counter({2: 40000, 0: 40000, 1: 40000, 3: 40000, 4: 3000})\n",
      "\n",
      "Checking Batch 1:\n",
      "Total mismatches in Batch 1: 4684 / 163000 | 0.029%\n",
      "\n",
      "Checking Batch 2:\n",
      "Total mismatches in Batch 2: 4608 / 163000 | 0.028%\n",
      "\n",
      "Checking Batch 3:\n",
      "Total mismatches in Batch 3: 4632 / 163000 | 0.028%\n"
     ]
    }
   ],
   "source": [
    "print(f\"scaled_combined_data.shape: {scaled_combined_data.shape}\")  # Expecting (total_data, num_features)\n",
    "print(f\"neighbor_pairs_list.shape: {neighbor_pairs_list.shape}\")  # Expecting (num_pairs, 2)\n",
    "print(f\"labels_for_neighbor_pairs.shape: {labels_for_neighbor_pairs.shape}\")  # Expecting (num_of_events, num_pairs)\n",
    "\n",
    "# Define the parameters for instantiating the generator\n",
    "batch_size = 20\n",
    "class_counts = {0: 40000, 1: 40000, 2: 40000, 3: 40000, 4: 3000}\n",
    "num_features = scaled_combined_data.shape[1]\n",
    "\n",
    "# Create the generator instance\n",
    "batch_generator = MultiClassBatchGenerator(\n",
    "    x=scaled_data,  # Pass dictionary instead of concatenated array\n",
    "    neighbor_pairs=neighbor_pairs_list, \n",
    "    labels_for_neighbor_pairs=labels_for_neighbor_pairs, \n",
    "    batch_size=batch_size, \n",
    "    class_counts=class_counts\n",
    ")\n",
    "\n",
    "# Explicitely get an iterator from the dataset\n",
    "batch_iterator = iter(batch_generator)\n",
    "\n",
    "# Retrieve the first batch\n",
    "batch = next(batch_iterator)\n",
    "\n",
    "# Unpack the batch according to the generator's output\n",
    "x_batch, neighbor_pairs_batch, labels_batch = batch  # Only three outputs\n",
    "\n",
    "print(f\"x_batch shape: {x_batch.shape}\")  \n",
    "print(f\"neighbor_pairs_batch shape: {neighbor_pairs_batch.shape}\")  \n",
    "print(f\"labels_batch shape: {labels_batch.shape}\")\n",
    "\n",
    "# Display class distribution in the batch\n",
    "label_counts = Counter(labels_batch.tolist())\n",
    "print(\"Class distribution in batch:\", label_counts)\n",
    "\n",
    "num_batches = 3\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    try:\n",
    "        x_batch, neighbor_pairs_batch, labels_batch = next(batch_iterator)\n",
    "    except StopIteration:\n",
    "        print(\"No more batches available.\")\n",
    "        break\n",
    "\n",
    "    mismatch_count = 0\n",
    "\n",
    "    print(f\"\\nChecking Batch {batch_num + 1}:\")\n",
    "\n",
    "    # Ensure shape is (num_pairs, 2)\n",
    "    neighbor_pairs_batch = neighbor_pairs_batch.reshape(-1, 2)\n",
    "\n",
    "    for idx, (pair, assigned_label) in enumerate(zip(neighbor_pairs_batch, labels_batch)):\n",
    "        i, j = pair\n",
    "        true_labels = labels_for_neighbor_pairs[:, i]  # Ensure event filtering\n",
    "\n",
    "        if assigned_label not in true_labels:\n",
    "            mismatch_count += 1\n",
    "\n",
    "    print(f\"Total mismatches in Batch {batch_num + 1}: {mismatch_count} / {len(labels_batch)} | {mismatch_count/len(labels_batch):.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`scaled_combined_data.shape: (18765200, 5)`**  \n",
    "   - This is the **feature matrix** containing data for all cells across events.  \n",
    "   - **18765200** represents the total number of cells.  \n",
    "   - **5** represents the number of features per cell.  \n",
    "\n",
    "2. **`neighbor_pairs_list.shape: (1250242, 2)`**  \n",
    "   - This represents the **neighbor (edge) pairs** of cells.  \n",
    "   - **1250242** is the total number of pairs (edges) in the dataset.  \n",
    "   - **2** indicates that each pair consists of two cell indices.\n",
    "\n",
    "3. **`labels_for_neighbor_pairs.shape: (100, 1250242)`**  \n",
    "   - This stores the **labels for cell pairs across events**.  \n",
    "   - **100** represents the number of events.  \n",
    "   - **1250242** represents the number of cell pairs in each event.  \n",
    "\n",
    "4. **`x_batch.shape: (163000, 5)`**  \n",
    "   - This is the **feature matrix for the current batch** of selected pairs.  \n",
    "   - **163000** is the number of selected cell pairs in the batch.  \n",
    "   - **5** is the number of features per cell.\n",
    "\n",
    "5. **`neighbor_pairs_batch.shape: (163000, 2)`**  \n",
    "   - This is the **subset of `neighbor_pairs_list`** used in the batch.  \n",
    "   - **163000** is the number of selected pairs in the batch.  \n",
    "   - **2** represents the two cell indices per pair.\n",
    "\n",
    "6. **`edge_index_batch.shape: (2, 1250242)`**  \n",
    "   - This represents the **edge index tensor** for the full dataset.  \n",
    "   - **2** represents the row-wise format:  \n",
    "     - Row 1: Source nodes  \n",
    "     - Row 2: Target nodes  \n",
    "   - **1250242** represents the total number of edges (pairs of cells).  \n",
    "\n",
    "7. **`edge_index_out_batch.shape: (2, 163000)`**  \n",
    "   - This is the **subset of `edge_index_batch`** corresponding to the batch.  \n",
    "   - **2** follows the same format as `edge_index_batch`.  \n",
    "   - **163000** represents the number of edges in the batch.  \n",
    "\n",
    "8. **`labels_batch.shape: (163000,)`**  \n",
    "   - This is the **batch of labels** corresponding to the selected edges.  \n",
    "   - **163000** ensures each selected edge has one assigned label.\n",
    "\n",
    "---\n",
    "\n",
    "### **Overall Structure:**\n",
    "- **Full Dataset:**  \n",
    "  - `scaled_combined_data`: Stores all cell features.  \n",
    "  - `neighbor_pairs_list`: Stores all neighbor relationships.  \n",
    "  - `labels_for_neighbor_pairs`: Stores labels for all edges across events.  \n",
    "\n",
    "- **Batch Data:**  \n",
    "  - `x_batch`: Features of selected cells.  \n",
    "  - `neighbor_pairs_batch`: Selected pairs of cells in the batch.  \n",
    "  - `edge_index_out_batch`: Graph representation of selected pairs.  \n",
    "  - `labels_batch`: Ground truth labels for the selected pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model incorporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassBatchGenerator(IterableDataset):\n",
    "    def __init__(self, x, neighbor_pairs, labels_for_neighbor_pairs, batch_size, class_counts):\n",
    "        self.x = x\n",
    "        self.neighbor_pairs = neighbor_pairs\n",
    "        self.labels_for_neighbor_pairs = labels_for_neighbor_pairs\n",
    "        self.batch_size = batch_size\n",
    "        self.class_counts = class_counts\n",
    "        self.indices_per_event = self._compute_event_class_indices()\n",
    "\n",
    "    def _compute_event_class_indices(self):\n",
    "        event_class_indices = {}\n",
    "        num_events = self.labels_for_neighbor_pairs.shape[0]\n",
    "        for event_id in range(num_events):\n",
    "            event_class_indices[event_id] = {}\n",
    "            for cls in np.unique(self.labels_for_neighbor_pairs[event_id]):\n",
    "                mask = self.labels_for_neighbor_pairs[event_id] == cls\n",
    "                event_class_indices[event_id][cls] = np.where(mask)[0]\n",
    "        return event_class_indices\n",
    "\n",
    "    def _batch_generator(self):\n",
    "        num_events = self.labels_for_neighbor_pairs.shape[0]\n",
    "        for event_id in range(num_events):\n",
    "            selected_pairs = []\n",
    "            selected_labels = []\n",
    "\n",
    "            for cls, n_samples in self.class_counts.items():\n",
    "                if cls not in self.indices_per_event[event_id]:\n",
    "                    continue\n",
    "\n",
    "                pair_indices = self.indices_per_event[event_id][cls]\n",
    "                if len(pair_indices) < n_samples:\n",
    "                    selected_idx = pair_indices\n",
    "                else:\n",
    "                    selected_idx = np.random.choice(pair_indices, size=n_samples, replace=False)\n",
    "\n",
    "                selected_pairs.append(self.neighbor_pairs[selected_idx])\n",
    "                selected_labels.append(np.full(len(selected_idx), cls))\n",
    "\n",
    "            if len(selected_pairs) == 0:\n",
    "                continue\n",
    "\n",
    "            selected_pairs = np.concatenate(selected_pairs, axis=0)\n",
    "            selected_labels = np.concatenate(selected_labels, axis=0)\n",
    "\n",
    "            perm = np.random.permutation(len(selected_labels))\n",
    "            selected_pairs = selected_pairs[perm]\n",
    "            selected_labels = selected_labels[perm]\n",
    "\n",
    "            edge_index_out = selected_pairs.T  # Transpose to match PyTorch Geometric format\n",
    "            \n",
    "            # **Debugging: Print the shapes before yielding**\n",
    "            print(f\"x[event_id] shape: {self.x[event_id].shape}\")  # Should be (num_nodes, num_features)\n",
    "            print(f\"selected_pairs shape: {selected_pairs.shape}\")  # Should be (num_edges, 2)\n",
    "            print(f\"edge_index_out shape: {edge_index_out.shape}\")  # Should be (2, num_edges)\n",
    "            print(f\"selected_labels shape: {selected_labels.shape}\")  # Should be (num_edges,)\n",
    "\n",
    "            yield self.x[f\"data_{event_id}\"], selected_pairs, edge_index_out, selected_labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batch_generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEdgeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=6, layer_weights=True, debug=False):\n",
    "        super(MultiEdgeClassifier, self).__init__()\n",
    "        self.debug = debug\n",
    "        self.layer_weights_enabled = layer_weights  # Store setting\n",
    "\n",
    "        # Node embedding layer\n",
    "        self.node_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Initialize convolution and batch norm layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.layer_weights = nn.ParameterList() if layer_weights else None  # Only create if enabled\n",
    "\n",
    "        # First layer\n",
    "        self.convs.append(GCNConv(hidden_dim, 128))\n",
    "        self.bns.append(BatchNorm1d(128))\n",
    "        if layer_weights:\n",
    "            self.layer_weights.append(nn.Parameter(torch.tensor(1.0, requires_grad=True)))\n",
    "\n",
    "        # Additional layers\n",
    "        for i in range(1, num_layers):\n",
    "            in_channels = 128 if i == 1 else 64\n",
    "            out_channels = 64\n",
    "            self.convs.append(GCNConv(in_channels, out_channels))\n",
    "            self.bns.append(BatchNorm1d(out_channels))\n",
    "            if layer_weights:\n",
    "                self.layer_weights.append(nn.Parameter(torch.tensor(1.0, requires_grad=True)))\n",
    "\n",
    "        # Edge classification layer (now output_dim is passed as a parameter)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "\n",
    "    def debug_print(self, message):\n",
    "        if self.debug:\n",
    "            print(message)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_index_out):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        if isinstance(edge_index, np.ndarray):\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.float32)\n",
    "        edge_index = edge_index.T\n",
    "        if isinstance(edge_index_out, np.ndarray):\n",
    "            edge_index_out = torch.tensor(edge_index_out, dtype=torch.float32)\n",
    "        self.debug_print(f\"Input x shape: {x.shape}\")\n",
    "        self.debug_print(f\"Input edge_index shape: {edge_index.shape}\")\n",
    "        self.debug_print(f\"Input edge_index_out shape: {edge_index_out.shape}\")\n",
    "\n",
    "        # Node embedding\n",
    "        x = self.node_embedding(x)\n",
    "        self.debug_print(f\"Node embedding output shape: {x.shape}\")\n",
    "\n",
    "        if x.dim() == 3 and x.size(0) == 1:  # Check and remove batch dimension\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "        # Loop through convolution layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            self.debug_print(f\"After GCNConv {i+1}: {x.shape}\")\n",
    "            if x.dim() == 3 and x.size(0) == 1:\n",
    "                x = x.squeeze(0)\n",
    "            x = self.bns[i](x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "            # Apply layer weight if enabled\n",
    "            if self.layer_weights_enabled:\n",
    "                x = x * self.layer_weights[i]\n",
    "                self.debug_print(f\"After Layer Weight {i+1}: {x.shape}\")\n",
    "\n",
    "        # Edge representations\n",
    "        edge_rep = torch.cat([x[edge_index_out[0]], x[edge_index_out[1]]], dim=1)\n",
    "        self.debug_print(f\"Edge representation shape: {edge_rep.shape}\")\n",
    "\n",
    "        # Return Logits (size depends on output_dim)\n",
    "        edge_scores = self.fc(edge_rep)\n",
    "        return edge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_for_train_and_test(logits, labels, criterion=nn.CrossEntropyLoss()):\n",
    "    return criterion(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        x, edge_index, edge_index_out, labels = data  # Make sure you unpack edge_index_out here\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, edge_index, edge_index_out)  # Pass edge_index_out along with edge_index\n",
    "        loss = loss_for_train_and_test(logits, labels, criterion)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predicted = logits.argmax(dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(train_loader), accuracy\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            x, edge_index, labels = data\n",
    "            logits = model(x, edge_index)\n",
    "            loss = loss_for_train_and_test(logits, labels, criterion)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, train_loader, test_loader, epochs=10, batch_size=20, class_counts=None, learning_rate=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss_per_epoch = []\n",
    "    scores = []\n",
    "    truth_labels = []\n",
    "\n",
    "    avg_loss_training_true_class = []\n",
    "    logits_training_true_class = []\n",
    "    avg_loss_testing_true_class = []\n",
    "    logits_testing_true_class = []\n",
    "    avg_loss_training_bkg_classes = []\n",
    "    logits_training_bkg_classes = []\n",
    "    avg_loss_testing_bkg_classes = []\n",
    "    logits_testing_bkg_classes = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_accuracy = train_model(model, train_loader, optimizer, criterion)\n",
    "        test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "\n",
    "        loss_per_epoch.append({'train': train_loss, 'test': test_loss})\n",
    "        scores.append({'train': train_accuracy, 'test': test_accuracy})\n",
    "\n",
    "        # Track per-class losses and logits (assuming classes are numbered 0 to 4)\n",
    "        avg_loss_training_true_class.append(train_loss)  # Example, adapt based on your data structure\n",
    "        logits_training_true_class.append(None)  # Placeholder, replace with actual logits for true classes\n",
    "        avg_loss_testing_true_class.append(test_loss)  # Example, adapt based on your data structure\n",
    "        logits_testing_true_class.append(None)  # Placeholder, replace with actual logits for testing\n",
    "\n",
    "        # Assuming background class labels exist from the previous configuration\n",
    "        avg_loss_training_bkg_classes.append(None)  # Placeholder\n",
    "        logits_training_bkg_classes.append(None)  # Placeholder\n",
    "        avg_loss_testing_bkg_classes.append(None)  # Placeholder\n",
    "        logits_testing_bkg_classes.append(None)  # Placeholder\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'loss_per_epoch': loss_per_epoch,\n",
    "        'scores': scores,\n",
    "        'truth_labels': truth_labels,\n",
    "        'avg_loss_training_true_class': avg_loss_training_true_class,\n",
    "        'logits_training_true_class': logits_training_true_class,\n",
    "        'avg_loss_testing_true_class': avg_loss_testing_true_class,\n",
    "        'logits_testing_true_class': logits_testing_true_class,\n",
    "        'avg_loss_training_bkg_classes': avg_loss_training_bkg_classes,\n",
    "        'logits_training_bkg_classes': logits_training_bkg_classes,\n",
    "        'avg_loss_testing_bkg_classes': avg_loss_testing_bkg_classes,\n",
    "        'logits_testing_bkg_classes': logits_testing_bkg_classes\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function to handle the data\n",
    "def collate_fn(data):\n",
    "    # Assuming each `data` element is a tuple (x, edge_index, edge_index_out, labels)\n",
    "    x = [d.x for d in data]\n",
    "    edge_index = [d.edge_index for d in data]\n",
    "    edge_index_out = [d.edge_index_out for d in data]  # Ensure this is included\n",
    "    labels = [d.y for d in data]\n",
    "    \n",
    "    # Return a tuple of 4 elements\n",
    "    return torch.stack(x, dim=0), torch.stack(edge_index, dim=0), torch.stack(edge_index_out, dim=0), torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[event_id] shape: (5,)\n",
      "selected_pairs shape: (163000, 2)\n",
      "edge_index_out shape: (2, 163000)\n",
      "selected_labels shape: (163000,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m MultiEdgeClassifier(input_dim\u001b[38;5;241m=\u001b[39mnum_features, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39mnum_classes, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Print or save metrics\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics)\n",
      "Cell \u001b[0;32mIn[50], line 18\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(model, train_loader, test_loader, epochs, batch_size, class_counts, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m logits_testing_bkg_classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 18\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m test_model(model, test_loader, criterion)\n\u001b[1;32m     21\u001b[0m     loss_per_epoch\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: test_loss})\n",
      "Cell \u001b[0;32mIn[49], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m      4\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m     x, edge_index, edge_index_out, labels \u001b[38;5;241m=\u001b[39m data  \u001b[38;5;66;03m# Make sure you unpack edge_index_out here\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[46], line 57\u001b[0m, in \u001b[0;36mMultiClassBatchGenerator._batch_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index_out shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00medge_index_out\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Should be (2, num_edges)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselected_labels shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_labels\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Should be (num_edges,)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mevent_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, selected_pairs, edge_index_out, selected_labels\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Assume data has been loaded into `scaled_combined_data`, `neighbor_pairs_list`, `labels_for_neighbor_pairs`\n",
    "batch_size = 20\n",
    "class_counts = {0: 40000, 1: 40000, 2: 40000, 3: 40000, 4: 3000}\n",
    "\n",
    "# Initialize the generator and model\n",
    "batch_generator = MultiClassBatchGenerator(\n",
    "    x=scaled_combined_data, \n",
    "    neighbor_pairs=neighbor_pairs_list, \n",
    "    labels_for_neighbor_pairs=labels_for_neighbor_pairs, \n",
    "    batch_size=batch_size, \n",
    "    class_counts=class_counts\n",
    ")\n",
    "\n",
    "train_loader = batch_generator\n",
    "\n",
    "# Model definition\n",
    "num_features = scaled_combined_data.shape[1]\n",
    "num_classes = 5\n",
    "model = MultiEdgeClassifier(input_dim=num_features, hidden_dim=128, output_dim=num_classes, debug=True)\n",
    "\n",
    "# Run the model\n",
    "metrics = run_model(model, train_loader, train_loader, epochs=10, batch_size=batch_size)\n",
    "\n",
    "# Print or save metrics\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
