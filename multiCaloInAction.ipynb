{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4560d7",
   "metadata": {},
   "source": [
    "## Required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9210fe68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc170e78-748a-4b33-bffd-0645f194cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 4\n"
     ]
    }
   ],
   "source": [
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available GPUs: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943dd03e",
   "metadata": {},
   "source": [
    "## Required data generated by GNNonCalo_Scaling_DataPreparation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "786d7a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the cell features and the training hdf5 files\n",
    "hf_multi_cellFeaturesScaled_neighbor = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_cellFeaturesScaled_train_70evs.hdf5\", 'r')\n",
    "hf_multi_train_edge_source_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/train_edge_source_BD_70evs.hdf5\", 'r')\n",
    "hf_multi_train_edge_dest_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/train_edge_dest_BD_70evs.hdf5\", 'r')\n",
    "hf_multi_train_edge_source_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/train_edge_source_noBD_70evs.hdf5\", 'r')\n",
    "hf_multi_train_edge_dest_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/train_edge_dest_noBD_70evs.hdf5\", 'r')\n",
    "hf_multi_truth_label_train_neighbor= h5py.File(\"/storage/mxg1065/neighborLabels100Events.hdf5\", 'r')\n",
    "# hf_multi_truth_label_train_neighbor= h5py.File('/storage/mxg1065/MultiClassGNN/totalTrainingLabelsRandom.hdf5', 'r')\n",
    "\n",
    "\n",
    "# Pull the data as arrays\n",
    "multi_cellFeaturesScaled = hf_multi_cellFeaturesScaled_neighbor.get(\"multi_cellFeatures_trainS\")[:]\n",
    "multi_train_edge_source_BD = hf_multi_train_edge_source_BD.get(\"train_edge_source_BD\")[:]\n",
    "multi_train_edge_dest_BD = hf_multi_train_edge_dest_BD.get(\"train_edge_dest_BD\")[:]\n",
    "multi_train_edge_source_noBD = hf_multi_train_edge_source_noBD.get(\"train_edge_source_noBD\")[:]\n",
    "multi_train_edge_dest_noBD = hf_multi_train_edge_dest_noBD.get(\"train_edge_dest_noBD\")[:]\n",
    "multi_truth_label_train = hf_multi_truth_label_train_neighbor.get(\"neighborLabels100Events\")[:]\n",
    "# multi_truth_label_train = hf_multi_truth_label_train_neighbor.get(\"totalTrainingLabelsRandom\")[:]\n",
    "\n",
    "# Close the files\n",
    "hf_multi_cellFeaturesScaled_neighbor.close()\n",
    "hf_multi_train_edge_source_BD.close()\n",
    "hf_multi_train_edge_dest_BD.close()\n",
    "hf_multi_train_edge_source_noBD.close()\n",
    "hf_multi_train_edge_dest_noBD.close()\n",
    "hf_multi_truth_label_train_neighbor.close()\n",
    "\n",
    "# Opening and closing the True hdf5 files\n",
    "hf_multi_test_edge_source_true_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_true_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_true_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_true_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_source_true_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_true_noBD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_true_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_true_noBD.hdf5\", \"r\")\n",
    "\n",
    "multi_test_edge_source_true_BD = hf_multi_test_edge_source_true_BD.get(\"multi_test_edge_source_true_BD\")[:]\n",
    "multi_test_edge_dest_true_BD = hf_multi_test_edge_dest_true_BD.get(\"multi_test_edge_dest_true_BD\")[:]\n",
    "multi_test_edge_source_true_noBD = hf_multi_test_edge_source_true_noBD.get(\"multi_test_edge_source_true_noBD\")[:]\n",
    "multi_test_edge_dest_true_noBD = hf_multi_test_edge_dest_true_noBD.get(\"multi_test_edge_dest_true_noBD\")[:]\n",
    "\n",
    "hf_multi_test_edge_source_true_BD.close()\n",
    "hf_multi_test_edge_dest_true_BD.close()\n",
    "hf_multi_test_edge_source_true_noBD.close()\n",
    "hf_multi_test_edge_dest_true_noBD.close()\n",
    "\n",
    "# Opening and closing the Lone-Lone hdf5 files\n",
    "hf_multi_test_edge_source_bkg_lone_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_lone_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_lone_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_lone_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_source_bkg_lone_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_lone_noBD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_lone_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_lone_noBD.hdf5\", \"r\")\n",
    "\n",
    "multi_test_edge_source_bkg_lone_BD = hf_multi_test_edge_source_bkg_lone_BD.get(\"multi_test_edge_source_bkg_lone_BD\")[:]\n",
    "multi_test_edge_dest_bkg_lone_BD = hf_multi_test_edge_dest_bkg_lone_BD.get(\"multi_test_edge_dest_bkg_lone_BD\")[:]\n",
    "multi_test_edge_source_bkg_lone_noBD = hf_multi_test_edge_source_bkg_lone_noBD.get(\"multi_test_edge_source_bkg_lone_noBD\")[:]\n",
    "multi_test_edge_dest_bkg_lone_noBD = hf_multi_test_edge_dest_bkg_lone_noBD.get(\"multi_test_edge_dest_bkg_lone_noBD\")[:]\n",
    "\n",
    "hf_multi_test_edge_source_bkg_lone_BD.close()\n",
    "hf_multi_test_edge_dest_bkg_lone_BD.close()\n",
    "hf_multi_test_edge_source_bkg_lone_noBD.close()\n",
    "hf_multi_test_edge_dest_bkg_lone_noBD.close()\n",
    "\n",
    "# Opening and closing the Lone-Cluster hdf5 files\n",
    "hf_multi_test_edge_source_bkg_lone_cluster_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_lone_cluster_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_lone_cluster_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_lone_cluster_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_source_bkg_lone_cluster_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_lone_cluster_noBD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_lone_cluster_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_lone_cluster_noBD.hdf5\", \"r\")\n",
    "\n",
    "multi_test_edge_source_bkg_lone_cluster_BD = hf_multi_test_edge_source_bkg_lone_cluster_BD.get(\"multi_test_edge_source_bkg_lone_cluster_BD\")[:]\n",
    "multi_test_edge_dest_bkg_lone_cluster_BD = hf_multi_test_edge_dest_bkg_lone_cluster_BD.get(\"multi_test_edge_dest_bkg_lone_cluster_BD\")[:]\n",
    "multi_test_edge_source_bkg_lone_cluster_noBD = hf_multi_test_edge_source_bkg_lone_cluster_noBD.get(\"multi_test_edge_source_bkg_lone_cluster_noBD\")[:]\n",
    "multi_test_edge_dest_bkg_lone_cluster_noBD = hf_multi_test_edge_dest_bkg_lone_cluster_noBD.get(\"multi_test_edge_dest_bkg_lone_cluster_noBD\")[:]\n",
    "\n",
    "hf_multi_test_edge_source_bkg_lone_cluster_BD.close()\n",
    "hf_multi_test_edge_dest_bkg_lone_cluster_BD.close()\n",
    "hf_multi_test_edge_source_bkg_lone_cluster_noBD.close()\n",
    "hf_multi_test_edge_dest_bkg_lone_cluster_noBD.close()\n",
    "\n",
    "# Opening and closing the Cluster-Lone hdf5 files\n",
    "hf_multi_test_edge_source_bkg_cluster_lone_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_cluster_lone_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_cluster_lone_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_cluster_lone_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_source_bkg_cluster_lone_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_cluster_lone_noBD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_cluster_lone_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_cluster_lone_noBD.hdf5\", \"r\")\n",
    "\n",
    "multi_test_edge_source_bkg_cluster_lone_BD = hf_multi_test_edge_source_bkg_cluster_lone_BD.get(\"multi_test_edge_source_bkg_cluster_lone_BD\")[:]\n",
    "multi_test_edge_dest_bkg_cluster_lone_BD = hf_multi_test_edge_dest_bkg_cluster_lone_BD.get(\"multi_test_edge_dest_bkg_cluster_lone_BD\")[:]\n",
    "multi_test_edge_source_bkg_cluster_lone_noBD = hf_multi_test_edge_source_bkg_cluster_lone_noBD.get(\"multi_test_edge_source_bkg_cluster_lone_noBD\")[:]\n",
    "multi_test_edge_dest_bkg_cluster_lone_noBD = hf_multi_test_edge_dest_bkg_cluster_lone_noBD.get(\"multi_test_edge_dest_bkg_cluster_lone_noBD\")[:]\n",
    "\n",
    "hf_multi_test_edge_source_bkg_cluster_lone_BD.close()\n",
    "hf_multi_test_edge_dest_bkg_cluster_lone_BD.close()\n",
    "hf_multi_test_edge_source_bkg_cluster_lone_noBD.close()\n",
    "hf_multi_test_edge_dest_bkg_cluster_lone_noBD.close()\n",
    "\n",
    "# Opening and closing the Cluster-CLuster hdf5 files\n",
    "hf_multi_test_edge_source_bkg_cluster_cluster_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_cluster_cluster_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_cluster_cluster_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_cluster_cluster_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_source_bkg_cluster_cluster_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_cluster_cluster_noBD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_cluster_cluster_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_cluster_cluster_noBD.hdf5\", \"r\")\n",
    "\n",
    "multi_test_edge_source_bkg_cluster_cluster_BD = hf_multi_test_edge_source_bkg_cluster_cluster_BD.get(\"multi_test_edge_source_bkg_cluster_cluster_BD\")[:]\n",
    "multi_test_edge_dest_bkg_cluster_cluster_BD = hf_multi_test_edge_dest_bkg_cluster_cluster_BD.get(\"multi_test_edge_dest_bkg_cluster_cluster_BD\")[:]\n",
    "multi_test_edge_source_bkg_cluster_cluster_noBD = hf_multi_test_edge_source_bkg_cluster_cluster_noBD.get(\"multi_test_edge_source_bkg_cluster_cluster_noBD\")[:]\n",
    "multi_test_edge_dest_bkg_cluster_cluster_noBD = hf_multi_test_edge_dest_bkg_cluster_cluster_noBD.get(\"multi_test_edge_dest_bkg_cluster_cluster_noBD\")[:]\n",
    "\n",
    "hf_multi_test_edge_source_bkg_cluster_cluster_BD.close()\n",
    "hf_multi_test_edge_dest_bkg_cluster_cluster_BD.close()\n",
    "hf_multi_test_edge_source_bkg_cluster_cluster_noBD.close()\n",
    "hf_multi_test_edge_dest_bkg_cluster_cluster_noBD.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16af783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 187652, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_cellFeaturesScaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85fc1d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 187652, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the scaled cell features into a torch tensor\n",
    "x = torch.tensor(multi_cellFeaturesScaled, dtype=torch.float)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee295ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 2500484)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_train_edge_source_BD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85959348-4b40-494a-bf86-38d5a58dfc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 66000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_test_edge_source_true_BD.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97c155",
   "metadata": {},
   "source": [
    "## Preparing the Training Set and Test Set of Edges for GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35380490-473a-4c13-bccd-15725a042a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEdgeIndexTensor(source, dest):\n",
    "    edgeIndex = torch.tensor([source, dest], dtype=torch.long)\n",
    "    return edgeIndex.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "850133e9-7e26-430d-86ad-4d7679b1e260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2273706/2545984526.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  edgeIndex = torch.tensor([source, dest], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Training set (Bi-directional and Uni-directional)\n",
    "trainingEdgeIndexBD = createEdgeIndexTensor(multi_train_edge_source_BD, multi_train_edge_dest_BD)\n",
    "trainingEdgeIndexNoBD = createEdgeIndexTensor(multi_train_edge_source_noBD, multi_train_edge_dest_noBD)\n",
    "\n",
    "# Dictionary for storing edge indices for different edge types\n",
    "edgeIndexData = {\n",
    "    \"ttBD\": (multi_test_edge_source_true_BD, multi_test_edge_dest_true_BD),\n",
    "    \"ttNoBD\": (multi_test_edge_source_true_noBD, multi_test_edge_dest_true_noBD),\n",
    "    \"llBD\": (multi_test_edge_source_bkg_lone_BD, multi_test_edge_dest_bkg_lone_BD),\n",
    "    \"llNoBD\": (multi_test_edge_source_bkg_lone_noBD, multi_test_edge_dest_bkg_lone_noBD),\n",
    "    \"lcBD\": (multi_test_edge_source_bkg_lone_cluster_BD, multi_test_edge_dest_bkg_lone_cluster_BD),\n",
    "    \"lcNoBD\": (multi_test_edge_source_bkg_lone_cluster_noBD, multi_test_edge_dest_bkg_lone_cluster_noBD),\n",
    "    \"clBD\": (multi_test_edge_source_bkg_cluster_lone_BD, multi_test_edge_dest_bkg_cluster_lone_BD),\n",
    "    \"clNoBD\": (multi_test_edge_source_bkg_cluster_lone_noBD, multi_test_edge_dest_bkg_cluster_lone_noBD),\n",
    "    \"ccBD\": (multi_test_edge_source_bkg_cluster_cluster_BD, multi_test_edge_dest_bkg_cluster_cluster_BD),\n",
    "    \"ccNoBD\": (multi_test_edge_source_bkg_cluster_cluster_noBD, multi_test_edge_dest_bkg_cluster_cluster_noBD),\n",
    "}\n",
    "\n",
    "# Use list comprehension to create and permute tensors for all edge types\n",
    "edgeIndexTensors = {key: createEdgeIndexTensor(sources, dests) for key, (sources, dests) in edgeIndexData.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d97a5fe-7c06-45c7-af63-901861e35347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 2, 2500484])\n",
      "torch.Size([70, 2, 1250242])\n"
     ]
    }
   ],
   "source": [
    "print(trainingEdgeIndexBD.shape)\n",
    "print(trainingEdgeIndexNoBD.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9360902",
   "metadata": {},
   "source": [
    "## Preparing label (true/Fake) tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e031179d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take the array that represents the target labels and add\n",
    "# a dimension in the \"1\" index position to make the array\n",
    "# three-dimensional, with the first dimension representing\n",
    "# the length of the training set\n",
    "trainingTruthLabels = np.expand_dims(multi_truth_label_train, axis=1)\n",
    "# Expands the dimensions of multi_truth_label_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3740e08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1, 1250242)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingTruthLabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e33d82-ae87-47fd-8dd7-85696729b809",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 1250242])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the target labels into a torch tensor\n",
    "y_train = torch.tensor(trainingTruthLabels)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29e70f",
   "metadata": {},
   "source": [
    "## Data customization specific to pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26374bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a class that inherents from the torch.utils.data.Dataset class\n",
    "# The pytorch class is abstract, meaning we need to define certain methods\n",
    "# like __len__() and __getitem__()\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    # Class constructor that takes in data list and\n",
    "    # stores it as an instance, making it avaliable\n",
    "    # to other methods in the class\n",
    "    def __init__(self, dataList):\n",
    "        self.dataList = dataList\n",
    "    \n",
    "    # Method return length of data set\n",
    "    def __len__(self):\n",
    "        return len(self.dataList)\n",
    "\n",
    "    # Method returns data point at index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataList[idx]\n",
    "\n",
    "# Used to handle batch loading, shuffling, and parallel loading during \n",
    "# training and testing in the ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a35702f4-3fe0-4358-9e9c-652b3a768e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with information regarding a homogenous graph (a graph\n",
    "# where all nodes represent instances of the same type [cells in the \n",
    "# detector] and all edges represent relations of the same type [connections\n",
    "# between cells])\n",
    "def createDataList(edgeIndexBD, edgeIndexNoBD, x):\n",
    "    dataList = []\n",
    "    for i in range(len(edgeIndexBD)):\n",
    "        # Create a node feature matrix out of the scaled cell features\n",
    "        # torch tensor\n",
    "        x_mat = x[i]\n",
    "        # Create a graph connectivity matrix out of the torch tensor that\n",
    "        # contained information of the bi-directional training edge sources\n",
    "        # and destinations\n",
    "        edge_index = edgeIndexBD[i]\n",
    "        edge_index, _ = add_self_loops(edge_index)\n",
    "        # Create the data object describing a homogeneous graph. x_mat is \n",
    "        # the node feature matrix, edge_index is the graph connectivity \n",
    "        # matrix, y_train are the target labels \n",
    "        data = Data(x=x_mat, edge_index=edge_index, edge_index_out=edgeIndexNoBD[i], y=y_train[i])\n",
    "        # Converts a homogeneous or heterogeneous graph to an undirected\n",
    "        # graph (a graph whose edges does not have direction)\n",
    "        data = ToUndirected()(data)\n",
    "        dataList.append(data)\n",
    "    return dataList\n",
    "\n",
    "# Create collate function which extracts the features (x),\n",
    "# graph connectivity (edge_index BD, edge_index_out noBD),\n",
    "# and truth labels (y) to be used in combining samples into\n",
    "# batches\n",
    "def collateData(dataList, is_training=False):\n",
    "    if is_training:\n",
    "        return (\n",
    "            [data.x for data in dataList],\n",
    "            [data.edge_index for data in dataList],\n",
    "            [data.edge_index_out for data in dataList],\n",
    "            [data.y for data in dataList]\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            [data.x for data in dataList],\n",
    "            [data.edge_index for data in dataList],\n",
    "            [data.edge_index_out for data in dataList]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83a1ae95-5a35-4840-9449-1ca92c10357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data lists for all edge types\n",
    "# Create data lists for all categories\n",
    "\n",
    "dataListTraining = createDataList(trainingEdgeIndexBD, trainingEdgeIndexNoBD, x) # Training Edges\n",
    "dataListTT = createDataList(edgeIndexTensors['ttBD'], edgeIndexTensors['ttNoBD'], x) # True-True Edges\n",
    "dataListLL = createDataList(edgeIndexTensors['llBD'], edgeIndexTensors['llNoBD'], x) # Lone-lone Edges\n",
    "dataListLC = createDataList(edgeIndexTensors['lcBD'], edgeIndexTensors['lcNoBD'], x) # Lone-Cluster Edges\n",
    "dataListCL = createDataList(edgeIndexTensors['clBD'], edgeIndexTensors['clNoBD'], x) # Cluster-Lone Edges\n",
    "dataListCC = createDataList(edgeIndexTensors['ccBD'], edgeIndexTensors['ccNoBD'], x) # Cluster-Cluster Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "156b080f-f4e5-4c3b-835c-e50716fff483",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 5\n",
    "dataSets = {}\n",
    "dataLoaders = {}\n",
    "dataListMapping = {\n",
    "    \"train\": dataListTraining,  # Training Edges\n",
    "    \"tt\": dataListTT,           # True-True Edges\n",
    "    \"ll\": dataListLL,           # Lone-Lone Edges\n",
    "    \"lc\": dataListLC,           # Lone-Cluster Edges\n",
    "    \"cl\": dataListCL,           # Cluster-Lone Edges\n",
    "    \"cc\": dataListCC            # Cluster-Cluster Edges\n",
    "}\n",
    "\n",
    "for key, data_list in dataListMapping.items():\n",
    "    dataSets[key] = CustomDataset(data_list)\n",
    "    # For 'train', pass is_training=True, otherwise False\n",
    "    if key == \"train\":\n",
    "        dataLoaders[key] = torch.utils.data.DataLoader(\n",
    "            dataSets[key], \n",
    "            batch_size=batchSize, \n",
    "            collate_fn=lambda batch: collateData(batch, is_training=True)  # Force is_training=True for train\n",
    "        )\n",
    "    else:\n",
    "        dataLoaders[key] = torch.utils.data.DataLoader(\n",
    "            dataSets[key], \n",
    "            batch_size=batchSize, \n",
    "            collate_fn=lambda batch: collateData(batch, is_training=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6609882-66f2-4e39-bb22-2b42e2931b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the total background dataset\n",
    "dataListTotalBkg = dataListLL + dataListLC + dataListCL + dataListCC\n",
    "\n",
    "# Collate function for total background\n",
    "def collateTotalBkg(dataListTotalBkg):\n",
    "    batch_x = [data.x for data in dataListTotalBkg]\n",
    "    batch_edge_index = [data.edge_index for data in dataListTotalBkg]\n",
    "    batch_edge_index_out = [data.edge_index_out for data in dataListTotalBkg]\n",
    "    return batch_x, batch_edge_index, batch_edge_index_out\n",
    "\n",
    "# Create the total background DataLoader\n",
    "customDatasetTotalBkg = CustomDataset(dataListTotalBkg)\n",
    "dataLoaderTotalBkg = torch.utils.data.DataLoader(customDatasetTotalBkg, batch_size=batchSize, collate_fn=collateTotalBkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe9ba9-543e-4734-a89f-50846ce7e73c",
   "metadata": {},
   "source": [
    "## Multi-Edge Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "591196e5-1675-4cc6-a2b9-23bbe5a8a95e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiEdgeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(MultiEdgeClassifier, self).__init__()\n",
    "\n",
    "        # Node embedding layer\n",
    "        self.node_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Initialize lists to hold convolutional layers and batch normalization layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        # Add the first graph convolutional layer\n",
    "        self.convs.append(GCNConv(hidden_dim, 128))\n",
    "        self.bns.append(BatchNorm1d(128))\n",
    "\n",
    "        # Add additional layers based on the parameter 'num_layers'\n",
    "        for i in range(1, num_layers):\n",
    "            in_channels = 128 if i == 1 else 64  # First layer has 128 channels, rest have 64\n",
    "            out_channels = 64\n",
    "            self.convs.append(GCNConv(in_channels, out_channels))\n",
    "            self.bns.append(BatchNorm1d(out_channels))\n",
    "        \n",
    "        # Edge classification layer\n",
    "        self.fc = nn.Linear(128, output_dim)  # Output logits\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_index_out):\n",
    "        edge_index = edge_index\n",
    "        x = self.node_embedding(x)\n",
    "\n",
    "        # Loop through the convolutional layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        # Edge representations\n",
    "        edge_index_to_compare = edge_index_out\n",
    "        edge_rep = torch.cat([x[edge_index_to_compare[0]], x[edge_index_to_compare[1]]], dim=1)  # Check the dim=1 part\n",
    "\n",
    "        # Return logits (no softmax, CrossEntropyLoss will handle it)\n",
    "        edge_scores = self.fc(edge_rep)\n",
    "        return edge_scores\n",
    "\n",
    "# Usage example:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate the model with dynamic layers\n",
    "input_dim = 8\n",
    "hidden_dim = 256\n",
    "output_dim = 5  # Multiclass classification\n",
    "num_layers = 5  # Example: you can change this to any number of layers\n",
    "model = MultiEdgeClassifier(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Handles softmax internally\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c6e5e53-0154-4d22-b4a5-9a65a1012fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, device, data_loader, optimizer, criterion):\n",
    "    # Sets the model into training mode\n",
    "    model.train()\n",
    "    # Sends model to GPU if available, otherwise uses the CPU\n",
    "    model.to(device)\n",
    "    # Initializes the total loss per epoch list\n",
    "    totalLossPerEpoch = []\n",
    "\n",
    "    # Loops iterates over batches of data from the data loader\n",
    "    for batch_x, batch_edge_index, batch_edge_index_out, batch_y in data_loader:\n",
    "        # Sends the input features, the edge indices, and target\n",
    "        # labels to the GPU if available, otherwise the CPU\n",
    "        batch_x = torch.stack(batch_x).to(device)\n",
    "        batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "        batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "\n",
    "        # Convert target labels to LongTensor (torch.int64)\n",
    "        batch_y = [y.long().to(device) for y in batch_y]\n",
    "\n",
    "        # Clears the gradients of the model parameters to ensure\n",
    "        # they are not accumulated across batches\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initializes the loss per batch list\n",
    "        loss_per_batch = []\n",
    "\n",
    "        # Model processes each graph in the batch one by one\n",
    "        for i in range(len(batch_edge_index)):\n",
    "            # Pass the features and the edge indices into the model and store\n",
    "            # the output (logits)\n",
    "            _output = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "\n",
    "            # Ensure that model outputs (logits) are of type float32\n",
    "            _output = _output.float()\n",
    "\n",
    "            # Calculate the difference between the model output and the targets\n",
    "            # via the provided criterion (loss function)\n",
    "            # Note: Criterion expects logits and target class labels\n",
    "            loss = criterion(_output.squeeze(), batch_y[i].squeeze())\n",
    "\n",
    "            # This difference is stored in the loss_per_batch list\n",
    "            loss_per_batch.append(loss)\n",
    "\n",
    "        # The average loss across all subgraphs within the batch is calculated and stored\n",
    "        total_loss_per_batch = sum(loss_per_batch) / len(loss_per_batch)\n",
    "        totalLossPerEpoch.append(total_loss_per_batch)\n",
    "\n",
    "        # Computes the loss gradients with respect to the model parameters\n",
    "        total_loss_per_batch.backward()\n",
    "\n",
    "        # Updates the model parameters using the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "    # After processing all the batches, the average loss across all\n",
    "    # batches is calculated and returned\n",
    "    total_loss_per_epoch = sum(totalLossPerEpoch) / len(totalLossPerEpoch)\n",
    "    return total_loss_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc3aebe8-9de1-4311-9b3e-ce47e89901ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_bkg_dict = {\n",
    "    0: dataLoaders['ll'],  # For label 0\n",
    "    2: dataLoaders['cl'],  # For label 2\n",
    "    3: dataLoaders['lc'],  # For label 3\n",
    "    4: dataLoaders['cc']   # For label 4\n",
    "}\n",
    "\n",
    "def testModel(model, device, data_loader_true, data_loader_bkg_dict):\n",
    "    all_scores = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        # Loop over true edges (positive class)\n",
    "        for batch_x, batch_edge_index, batch_edge_index_out in data_loader_true:\n",
    "            batch_x = torch.stack(batch_x).to(device)\n",
    "            batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "            batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                \n",
    "                # Assign the true label for true edges (positive class is 1 for true edges)\n",
    "                true_label = torch.ones(test_edge_scores.size(0), dtype=torch.long, device=device)\n",
    "                \n",
    "                # Append scores and true labels for this batch\n",
    "                all_scores.append(test_edge_scores)\n",
    "                true_labels.append(true_label)\n",
    "        \n",
    "        # Loop over background edges (negative class)\n",
    "        for background_type, data_loader_bkg in data_loader_bkg_dict.items():\n",
    "            background_label = background_type  # This is your label for the current background type\n",
    "\n",
    "            for batch_x, batch_edge_index, batch_edge_index_out in data_loader_bkg:\n",
    "                batch_x = torch.stack(batch_x).to(device)\n",
    "                batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "                batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "\n",
    "                for i in range(len(batch_edge_index)):\n",
    "                    test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                    \n",
    "                    # Assign the true label for the current background type\n",
    "                    true_label = torch.full((test_edge_scores.size(0),), background_label, dtype=torch.long, device=device)\n",
    "                    \n",
    "                    # Append scores and true labels for this batch\n",
    "                    all_scores.append(test_edge_scores)\n",
    "                    true_labels.append(true_label)\n",
    "\n",
    "    # Concatenate all scores and labels from different batches\n",
    "    all_scores = torch.cat(all_scores, dim=0).cpu().numpy()\n",
    "    true_labels = torch.cat(true_labels, dim=0).cpu().numpy()\n",
    "\n",
    "    return all_scores, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33a72fa3-a458-45fc-bed7-f687176729d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossForTrainingAndTesting(model, loader, loss_fn, optimizer, training, device):\n",
    "    if training:\n",
    "        model.train()  # Set model to training mode\n",
    "    else:\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    if training:\n",
    "        # Training loop: loader provides 4 elements\n",
    "        for batch_x, batch_edge_index, batch_edge_index_out, batch_y in loader:\n",
    "            batch_x = torch.stack(batch_x).to(device)\n",
    "            batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "            batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "            batch_y = torch.cat(batch_y).to(device)  # True labels for the current batch\n",
    "            \n",
    "            for i in range(len(batch_edge_index)):\n",
    "                # Forward pass\n",
    "                test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                \n",
    "                # Compute loss using the provided true labels (`batch_y`)\n",
    "                loss = loss_fn(test_edge_scores, batch_y[i])\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    else:\n",
    "        # Evaluation loop: loader provides only 3 elements\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_edge_index, batch_edge_index_out in loader:\n",
    "                batch_x = torch.stack(batch_x).to(device)\n",
    "                batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "                batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "                \n",
    "                for i in range(len(batch_edge_index)):\n",
    "                    # Forward pass\n",
    "                    test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                    all_scores.append(test_edge_scores)\n",
    "                    num_batches += 1\n",
    "    \n",
    "    # Compute the average loss during training (loss is not calculated during evaluation)\n",
    "    average_loss = total_loss / num_batches if training else None\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f08d7-c2ce-4bc8-b208-4efeba64ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data-saving function\n",
    "def saveDataH5(fileName, dataDict, compressionType='gzip'):\n",
    "    with h5py.File(fileName, 'w') as file:\n",
    "        for key, data in dataDict.items():\n",
    "            file.create_dataset(key, data=data, compression=compressionType)\n",
    "\n",
    "num_epochs = 500\n",
    "lossPerEpoch = []\n",
    "scores = []\n",
    "truth_labels = []\n",
    "avgLoss_TrueTrain = []\n",
    "avgLoss_TrueTest = []\n",
    "avgLoss_BkgTrain = []\n",
    "avgLoss_BkgTest = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    total_loss_per_epoch = trainModel(model, device, dataLoaders['train'], optimizer, criterion)\n",
    "    lossPerEpoch.append(total_loss_per_epoch.cpu().detach().numpy())  # Ensure tensor is detached for saving\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Test the model\n",
    "    epoch_scores, epoch_true_labels = testModel(model, device, dataLoaders['tt'], data_loader_bkg_dict)\n",
    "    \n",
    "    # Compute the average loss for true and background edges\n",
    "    avgLossTrueTrain = lossForTrainingAndTesting(model, dataLoaders['train'], criterion, optimizer, True, device)\n",
    "    avgLossTrueTest = lossForTrainingAndTesting(model, dataLoaders['tt'], criterion, optimizer, False, device)\n",
    "    avgLossBkgTrain = lossForTrainingAndTesting(model, dataLoaders['train'], criterion, optimizer, True, device)\n",
    "    avgLossBkgTest = lossForTrainingAndTesting(model, dataLoaderTotalBkg, criterion, optimizer, False, device)\n",
    "\n",
    "    # Store results\n",
    "    scores.append(epoch_scores)\n",
    "    truth_labels.append(epoch_true_labels)\n",
    "    avgLoss_TrueTrain.append(avgLossTrueTrain)\n",
    "    avgLoss_TrueTest.append(avgLossTrueTest)\n",
    "    avgLoss_BkgTrain.append(avgLossBkgTrain)\n",
    "    avgLoss_BkgTest.append(avgLossBkgTest)\n",
    "\n",
    "    # Print the loss for the current epoch\n",
    "    print(f\"Epoch: {epoch+1} | Total Loss Per Epoch: {total_loss_per_epoch.item():.4f}\")\n",
    "    \n",
    "    # Save the model and data every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = f\"/storage/mxg1065/MultiClassGNN/modelCheckpoints/model_{num_layers}_layers_epoch_{epoch+1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Model checkpoint saved for epoch {epoch+1}\")\n",
    "        \n",
    "        # Save current training metrics\n",
    "        data_path = f\"/storage/mxg1065/MultiClassGNN/data_with_{num_layers}_layers_epoch_{epoch+1}.h5\"\n",
    "        saveDataH5(\n",
    "            data_path,\n",
    "            {\n",
    "                \"lossData\": np.array(lossPerEpoch, dtype=np.float32),\n",
    "                \"scores\": np.array(scores, dtype=np.float32),\n",
    "                \"truth_labels\": np.array(truth_labels, dtype=np.int32),\n",
    "                \"avgLoss_TrueTrain\": np.array(avgLoss_TrueTrain, dtype=np.float32),\n",
    "                \"avgLoss_TrueTest\": np.array(avgLoss_TrueTest, dtype=np.float32),\n",
    "                \"avgLoss_BkgTrain\": np.array(avgLoss_BkgTrain, dtype=np.float32),\n",
    "                \"avgLoss_BkgTest\": np.array(avgLoss_BkgTest, dtype=np.float32)\n",
    "            },\n",
    "            compressionType='gzip'\n",
    "        )\n",
    "        print(f\"Training data saved for epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c46d78-350e-49a2-858d-502b0bf9e3b1",
   "metadata": {},
   "source": [
    "### Running the code from a starting model epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a0fec-1c91-419a-b3c6-cd99249365be",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 300\n",
    "num_epochs = 500\n",
    "\n",
    "\n",
    "# Load the model checkpoint from starting epoch\n",
    "checkpoint_path = \"SOME PATH\"\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.to(device)\n",
    "\n",
    "# Resume training\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # Train the model\n",
    "    total_loss_per_epoch = trainModel(model, device, dataLoaders['train'], optimizer, criterion)\n",
    "    lossPerEpoch.append(total_loss_per_epoch)  # Append new losses to existing list\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Test the model\n",
    "    epoch_scores, epoch_true_labels = testModel(model, device, dataLoaders['tt'], data_loader_bkg_dict)\n",
    "    \n",
    "    # Compute the average loss for true and background edges\n",
    "    avgLossTrueTrain = lossForTrainingAndTesting(model, dataLoaders['train'], criterion, optimizer, True, device)\n",
    "    avgLossTrueTest = lossForTrainingAndTesting(model, dataLoaders['tt'], criterion, optimizer, False, device)\n",
    "    avgLossBkgTrain = lossForTrainingAndTesting(model, dataLoaders['train'], criterion, optimizer, True, device)\n",
    "    avgLossBkgTest = lossForTrainingAndTesting(model, dataLoaderTotalBkg, criterion, optimizer, False, device)\n",
    "\n",
    "    # Store results\n",
    "    scores.append(epoch_scores)\n",
    "    truth_labels.append(epoch_true_labels)\n",
    "    avgLoss_TrueTrain.append(avgLossTrueTrain)\n",
    "    avgLoss_TrueTest.append(avgLossTrueTest)\n",
    "    avgLoss_BkgTrain.append(avgLossBkgTrain)\n",
    "    avgLoss_BkgTest.append(avgLossBkgTest)\n",
    "\n",
    "    # Print the loss for the current epoch\n",
    "    print(f\"Epoch: {epoch+1} | Total Loss Per Epoch: {total_loss_per_epoch.item():.4f}\")\n",
    "\n",
    "    # As an example, if you start at 300 epochs, this code below shows how to\n",
    "    # save the model at 400 and 500 epochs\n",
    "    if (epoch + 1) in [400, 500]:\n",
    "        checkpoint_path = f\"/storage/mxg1065/MultiClassGNN/modelCheckpoints/model_{num_layers}_layers_epoch_{epoch+1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Model checkpoint saved for epoch {epoch+1}\")\n",
    "\n",
    "# Moves the loss back to the CPU and converts it to a numpy array\n",
    "lossPerEpoch = [tensor.cpu() for tensor in lossPerEpoch]\n",
    "lossPerEpoch = [tensor.detach().numpy() for tensor in lossPerEpoch]\n",
    "\n",
    "# Convert lists to numpy arrays for saving\n",
    "lossPerEpoch = np.array(lossPerEpoch, dtype=np.float32)\n",
    "scores = np.array(scores, dtype=np.float32)\n",
    "truth_labels = np.array(truth_labels, dtype=np.int32)\n",
    "avgLoss_TrueTrain = np.array(avgLoss_TrueTrain, dtype=np.float32)\n",
    "avgLoss_TrueTest = np.array(avgLoss_TrueTest, dtype=np.float32)\n",
    "avgLoss_BkgTrain = np.array(avgLoss_BkgTrain, dtype=np.float32)\n",
    "avgLoss_BkgTest = np.array(avgLoss_BkgTest, dtype=np.float32)\n",
    "\n",
    "# Saving the data\n",
    "saveDataH5(\"/storage/mxg1065/MultiClassGNN/data_five_layers.h5\", \n",
    "           {\"lossData\": lossPerEpoch,\n",
    "            \"scores\": scores,\n",
    "            'truth_labels': truth_labels,\n",
    "            \"avgLoss_TrueTrain\": avgLoss_TrueTrain,\n",
    "            \"avgLoss_TrueTest\": avgLoss_TrueTest,\n",
    "            \"avgLoss_BkgTrain\": avgLoss_BkgTrain,\n",
    "            \"avgLoss_BkgTest\": avgLoss_BkgTest},\n",
    "           'gzip')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
