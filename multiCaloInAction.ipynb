{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4560d7",
   "metadata": {},
   "source": [
    "## Required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9210fe68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943dd03e",
   "metadata": {},
   "source": [
    "## Required data generated by GNNonCalo_Scaling_DataPreparation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786d7a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the cell features and the training hdf5 files\n",
    "hf_multi_cellFeaturesScaled_neighbor = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_cellFeaturesScaled_train_70evs.hdf5\", 'r')\n",
    "hf_multi_train_edge_source_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/train_edge_source_BD_70evs.hdf5\", 'r')\n",
    "hf_multi_train_edge_dest_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/train_edge_dest_BD_70evs.hdf5\", 'r')\n",
    "hf_multi_train_edge_source_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/train_edge_source_noBD_70evs.hdf5\", 'r')\n",
    "hf_multi_train_edge_dest_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/train_edge_dest_noBD_70evs.hdf5\", 'r')\n",
    "hf_multi_truth_label_train_neighbor= h5py.File(\"/storage/mxg1065/neighborLabels100Events.hdf5\", 'r')\n",
    "\n",
    "# Pull the data as arrays\n",
    "multi_cellFeaturesScaled = hf_multi_cellFeaturesScaled_neighbor.get(\"multi_cellFeatures_trainS\")[:]\n",
    "multi_train_edge_source_BD = hf_multi_train_edge_source_BD.get(\"train_edge_source_BD\")[:]\n",
    "multi_train_edge_dest_BD = hf_multi_train_edge_dest_BD.get(\"train_edge_dest_BD\")[:]\n",
    "multi_train_edge_source_noBD = hf_multi_train_edge_source_noBD.get(\"train_edge_source_noBD\")[:]\n",
    "multi_train_edge_dest_noBD = hf_multi_train_edge_dest_noBD.get(\"train_edge_dest_noBD\")[:]\n",
    "multi_truth_label_train = hf_multi_truth_label_train_neighbor.get(\"neighborLabels100Events\")[:]\n",
    "\n",
    "# Close the files\n",
    "hf_multi_cellFeaturesScaled_neighbor.close()\n",
    "hf_multi_train_edge_source_BD.close()\n",
    "hf_multi_train_edge_dest_BD.close()\n",
    "hf_multi_train_edge_source_noBD.close()\n",
    "hf_multi_train_edge_dest_noBD.close()\n",
    "hf_multi_truth_label_train_neighbor.close()\n",
    "\n",
    "# Opening and closing the True hdf5 files\n",
    "hf_multi_test_edge_source_true_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_true_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_true_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_true_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_source_true_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_true_noBD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_true_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_true_noBD.hdf5\", \"r\")\n",
    "\n",
    "multi_test_edge_source_true_BD = hf_multi_test_edge_source_true_BD.get(\"multi_test_edge_source_true_BD\")[:]\n",
    "multi_test_edge_dest_true_BD = hf_multi_test_edge_dest_true_BD.get(\"multi_test_edge_dest_true_BD\")[:]\n",
    "multi_test_edge_source_true_noBD = hf_multi_test_edge_source_true_noBD.get(\"multi_test_edge_source_true_noBD\")[:]\n",
    "multi_test_edge_dest_true_noBD = hf_multi_test_edge_dest_true_noBD.get(\"multi_test_edge_dest_true_noBD\")[:]\n",
    "\n",
    "hf_multi_test_edge_source_true_BD.close()\n",
    "hf_multi_test_edge_dest_true_BD.close()\n",
    "hf_multi_test_edge_source_true_noBD.close()\n",
    "hf_multi_test_edge_dest_true_noBD.close()\n",
    "\n",
    "# Opening and closing the Lone-Lone hdf5 files\n",
    "hf_multi_test_edge_source_bkg_lone_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_lone_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_lone_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_lone_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_source_bkg_lone_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_lone_noBD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_lone_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_lone_noBD.hdf5\", \"r\")\n",
    "\n",
    "multi_test_edge_source_bkg_lone_BD = hf_multi_test_edge_source_bkg_lone_BD.get(\"multi_test_edge_source_bkg_lone_BD\")[:]\n",
    "multi_test_edge_dest_bkg_lone_BD = hf_multi_test_edge_dest_bkg_lone_BD.get(\"multi_test_edge_dest_bkg_lone_BD\")[:]\n",
    "multi_test_edge_source_bkg_lone_noBD = hf_multi_test_edge_source_bkg_lone_noBD.get(\"multi_test_edge_source_bkg_lone_noBD\")[:]\n",
    "multi_test_edge_dest_bkg_lone_noBD = hf_multi_test_edge_dest_bkg_lone_noBD.get(\"multi_test_edge_dest_bkg_lone_noBD\")[:]\n",
    "\n",
    "hf_multi_test_edge_source_bkg_lone_BD.close()\n",
    "hf_multi_test_edge_dest_bkg_lone_BD.close()\n",
    "hf_multi_test_edge_source_bkg_lone_noBD.close()\n",
    "hf_multi_test_edge_dest_bkg_lone_noBD.close()\n",
    "\n",
    "# Opening and closing the Lone-Cluster hdf5 files\n",
    "hf_multi_test_edge_source_bkg_lone_cluster_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_lone_cluster_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_lone_cluster_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_lone_cluster_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_source_bkg_lone_cluster_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_lone_cluster_noBD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_lone_cluster_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_lone_cluster_noBD.hdf5\", \"r\")\n",
    "\n",
    "multi_test_edge_source_bkg_lone_cluster_BD = hf_multi_test_edge_source_bkg_lone_cluster_BD.get(\"multi_test_edge_source_bkg_lone_cluster_BD\")[:]\n",
    "multi_test_edge_dest_bkg_lone_cluster_BD = hf_multi_test_edge_dest_bkg_lone_cluster_BD.get(\"multi_test_edge_dest_bkg_lone_cluster_BD\")[:]\n",
    "multi_test_edge_source_bkg_lone_cluster_noBD = hf_multi_test_edge_source_bkg_lone_cluster_noBD.get(\"multi_test_edge_source_bkg_lone_cluster_noBD\")[:]\n",
    "multi_test_edge_dest_bkg_lone_cluster_noBD = hf_multi_test_edge_dest_bkg_lone_cluster_noBD.get(\"multi_test_edge_dest_bkg_lone_cluster_noBD\")[:]\n",
    "\n",
    "hf_multi_test_edge_source_bkg_lone_cluster_BD.close()\n",
    "hf_multi_test_edge_dest_bkg_lone_cluster_BD.close()\n",
    "hf_multi_test_edge_source_bkg_lone_cluster_noBD.close()\n",
    "hf_multi_test_edge_dest_bkg_lone_cluster_noBD.close()\n",
    "\n",
    "# Opening and closing the Cluster-Lone hdf5 files\n",
    "hf_multi_test_edge_source_bkg_cluster_lone_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_cluster_lone_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_cluster_lone_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_cluster_lone_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_source_bkg_cluster_lone_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_cluster_lone_noBD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_cluster_lone_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_cluster_lone_noBD.hdf5\", \"r\")\n",
    "\n",
    "multi_test_edge_source_bkg_cluster_lone_BD = hf_multi_test_edge_source_bkg_cluster_lone_BD.get(\"multi_test_edge_source_bkg_cluster_lone_BD\")[:]\n",
    "multi_test_edge_dest_bkg_cluster_lone_BD = hf_multi_test_edge_dest_bkg_cluster_lone_BD.get(\"multi_test_edge_dest_bkg_cluster_lone_BD\")[:]\n",
    "multi_test_edge_source_bkg_cluster_lone_noBD = hf_multi_test_edge_source_bkg_cluster_lone_noBD.get(\"multi_test_edge_source_bkg_cluster_lone_noBD\")[:]\n",
    "multi_test_edge_dest_bkg_cluster_lone_noBD = hf_multi_test_edge_dest_bkg_cluster_lone_noBD.get(\"multi_test_edge_dest_bkg_cluster_lone_noBD\")[:]\n",
    "\n",
    "hf_multi_test_edge_source_bkg_cluster_lone_BD.close()\n",
    "hf_multi_test_edge_dest_bkg_cluster_lone_BD.close()\n",
    "hf_multi_test_edge_source_bkg_cluster_lone_noBD.close()\n",
    "hf_multi_test_edge_dest_bkg_cluster_lone_noBD.close()\n",
    "\n",
    "# Opening and closing the Cluster-CLuster hdf5 files\n",
    "hf_multi_test_edge_source_bkg_cluster_cluster_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_cluster_cluster_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_cluster_cluster_BD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_cluster_cluster_BD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_source_bkg_cluster_cluster_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_source_bkg_cluster_cluster_noBD.hdf5\", \"r\")\n",
    "hf_multi_test_edge_dest_bkg_cluster_cluster_noBD = h5py.File(\"/storage/mxg1065/MultiClassGNN/multi_test_edge_dest_bkg_cluster_cluster_noBD.hdf5\", \"r\")\n",
    "\n",
    "multi_test_edge_source_bkg_cluster_cluster_BD = hf_multi_test_edge_source_bkg_cluster_cluster_BD.get(\"multi_test_edge_source_bkg_cluster_cluster_BD\")[:]\n",
    "multi_test_edge_dest_bkg_cluster_cluster_BD = hf_multi_test_edge_dest_bkg_cluster_cluster_BD.get(\"multi_test_edge_dest_bkg_cluster_cluster_BD\")[:]\n",
    "multi_test_edge_source_bkg_cluster_cluster_noBD = hf_multi_test_edge_source_bkg_cluster_cluster_noBD.get(\"multi_test_edge_source_bkg_cluster_cluster_noBD\")[:]\n",
    "multi_test_edge_dest_bkg_cluster_cluster_noBD = hf_multi_test_edge_dest_bkg_cluster_cluster_noBD.get(\"multi_test_edge_dest_bkg_cluster_cluster_noBD\")[:]\n",
    "\n",
    "hf_multi_test_edge_source_bkg_cluster_cluster_BD.close()\n",
    "hf_multi_test_edge_dest_bkg_cluster_cluster_BD.close()\n",
    "hf_multi_test_edge_source_bkg_cluster_cluster_noBD.close()\n",
    "hf_multi_test_edge_dest_bkg_cluster_cluster_noBD.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16af783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 187652, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_cellFeaturesScaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85fc1d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 187652, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the scaled cell features into a torch tensor\n",
    "x = torch.tensor(multi_cellFeaturesScaled, dtype=torch.float)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee295ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 2500484)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_train_edge_source_BD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85959348-4b40-494a-bf86-38d5a58dfc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 66000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_test_edge_source_true_BD.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97c155",
   "metadata": {},
   "source": [
    "## Preparing the Training Set and Test Set of Edges for GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35380490-473a-4c13-bccd-15725a042a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEdgeIndexTensor(source, dest):\n",
    "    edgeIndex = torch.tensor([source, dest], dtype=torch.long)\n",
    "    return edgeIndex.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "850133e9-7e26-430d-86ad-4d7679b1e260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2064440/2545984526.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  edgeIndex = torch.tensor([source, dest], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Training set (Bi-directional and Uni-directional)\n",
    "trainingEdgeIndexBD = createEdgeIndexTensor(multi_train_edge_source_BD, multi_train_edge_dest_BD)\n",
    "trainingEdgeIndexNoBD = createEdgeIndexTensor(multi_train_edge_source_noBD, multi_train_edge_dest_noBD)\n",
    "\n",
    "# Dictionary for storing edge indices for different edge types\n",
    "edgeIndexData = {\n",
    "    \"ttBD\": (multi_test_edge_source_true_BD, multi_test_edge_dest_true_BD),\n",
    "    \"ttNoBD\": (multi_test_edge_source_true_noBD, multi_test_edge_dest_true_noBD),\n",
    "    \"llBD\": (multi_test_edge_source_bkg_lone_BD, multi_test_edge_dest_bkg_lone_BD),\n",
    "    \"llNoBD\": (multi_test_edge_source_bkg_lone_noBD, multi_test_edge_dest_bkg_lone_noBD),\n",
    "    \"lcBD\": (multi_test_edge_source_bkg_lone_cluster_BD, multi_test_edge_dest_bkg_lone_cluster_BD),\n",
    "    \"lcNoBD\": (multi_test_edge_source_bkg_lone_cluster_noBD, multi_test_edge_dest_bkg_lone_cluster_noBD),\n",
    "    \"clBD\": (multi_test_edge_source_bkg_cluster_lone_BD, multi_test_edge_dest_bkg_cluster_lone_BD),\n",
    "    \"clNoBD\": (multi_test_edge_source_bkg_cluster_lone_noBD, multi_test_edge_dest_bkg_cluster_lone_noBD),\n",
    "    \"ccBD\": (multi_test_edge_source_bkg_cluster_cluster_BD, multi_test_edge_dest_bkg_cluster_cluster_BD),\n",
    "    \"ccNoBD\": (multi_test_edge_source_bkg_cluster_cluster_noBD, multi_test_edge_dest_bkg_cluster_cluster_noBD),\n",
    "}\n",
    "\n",
    "# Use list comprehension to create and permute tensors for all edge types\n",
    "edgeIndexTensors = {key: createEdgeIndexTensor(sources, dests) for key, (sources, dests) in edgeIndexData.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d97a5fe-7c06-45c7-af63-901861e35347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 2, 2500484])\n",
      "torch.Size([70, 2, 1250242])\n"
     ]
    }
   ],
   "source": [
    "print(trainingEdgeIndexBD.shape)\n",
    "print(trainingEdgeIndexNoBD.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9360902",
   "metadata": {},
   "source": [
    "## Preparing label (true/Fake) tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e031179d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take the array that represents the target labels and add\n",
    "# a dimension in the \"1\" index position to make the array\n",
    "# three-dimensional, with the first dimension representing\n",
    "# the length of the training set\n",
    "trainingTruthLabels = np.expand_dims(multi_truth_label_train, axis=1)\n",
    "# Expands the dimensions of multi_truth_label_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3740e08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1, 1250242)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingTruthLabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9e33d82-ae87-47fd-8dd7-85696729b809",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 1250242])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the target labels into a torch tensor\n",
    "y_train = torch.tensor(trainingTruthLabels)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa1c4127-f1fa-45e5-96a1-f7fcd29da821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 3, 2, 0]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29e70f",
   "metadata": {},
   "source": [
    "## Data customization specific to pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26374bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a class that inherents from the torch.utils.data.Dataset class\n",
    "# The pytorch class is abstract, meaning we need to define certain methods\n",
    "# like __len__() and __getitem__()\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    # Class constructor that takes in data list and\n",
    "    # stores it as an instance, making it avaliable\n",
    "    # to other methods in the class\n",
    "    def __init__(self, dataList):\n",
    "        self.dataList = dataList\n",
    "    \n",
    "    # Method return length of data set\n",
    "    def __len__(self):\n",
    "        return len(self.dataList)\n",
    "\n",
    "    # Method returns data point at index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataList[idx]\n",
    "\n",
    "# Used to handle batch loading, shuffling, and parallel loading during \n",
    "# training and testing in the ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a35702f4-3fe0-4358-9e9c-652b3a768e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with information regarding a homogenous graph (a graph\n",
    "# where all nodes represent instances of the same type [cells in the \n",
    "# detector] and all edges represent relations of the same type [connections\n",
    "# between cells])\n",
    "def createDataList(edgeIndexBD, edgeIndexNoBD, x):\n",
    "    dataList = []\n",
    "    for i in range(len(edgeIndexBD)):\n",
    "        # Create a node feature matrix out of the scaled cell features\n",
    "        # torch tensor\n",
    "        x_mat = x[i]\n",
    "        # Create a graph connectivity matrix out of the torch tensor that\n",
    "        # contained information of the bi-directional training edge sources\n",
    "        # and destinations\n",
    "        edge_index = edgeIndexBD[i]\n",
    "        edge_index, _ = add_self_loops(edge_index)\n",
    "        # Create the data object describing a homogeneous graph. x_mat is \n",
    "        # the node feature matrix, edge_index is the graph connectivity \n",
    "        # matrix, y_train are the target labels \n",
    "        data = Data(x=x_mat, edge_index=edge_index, edge_index_out=edgeIndexNoBD[i], y=y_train[i])\n",
    "        # Converts a homogeneous or heterogeneous graph to an undirected\n",
    "        # graph (a graph whose edges does not have direction)\n",
    "        data = ToUndirected()(data)\n",
    "        dataList.append(data)\n",
    "    return dataList\n",
    "\n",
    "# Create collate function which extracts the features (x),\n",
    "# graph connectivity (edge_index BD, edge_index_out noBD),\n",
    "# and truth labels (y) to be used in combining samples into\n",
    "# batches\n",
    "def collateData(dataList, is_training=False):\n",
    "    if is_training:\n",
    "        return (\n",
    "            [data.x for data in dataList],\n",
    "            [data.edge_index for data in dataList],\n",
    "            [data.edge_index_out for data in dataList],\n",
    "            [data.y for data in dataList]\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            [data.x for data in dataList],\n",
    "            [data.edge_index for data in dataList],\n",
    "            [data.edge_index_out for data in dataList]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83a1ae95-5a35-4840-9449-1ca92c10357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data lists for all edge types\n",
    "# Create data lists for all categories\n",
    "\n",
    "dataListTraining = createDataList(trainingEdgeIndexBD, trainingEdgeIndexNoBD, x) # Training Edges\n",
    "dataListTT = createDataList(edgeIndexTensors['ttBD'], edgeIndexTensors['ttNoBD'], x) # True-True Edges\n",
    "dataListLL = createDataList(edgeIndexTensors['llBD'], edgeIndexTensors['llNoBD'], x) # Lone-lone Edges\n",
    "dataListLC = createDataList(edgeIndexTensors['lcBD'], edgeIndexTensors['lcNoBD'], x) # Lone-Cluster Edges\n",
    "dataListCL = createDataList(edgeIndexTensors['clBD'], edgeIndexTensors['clNoBD'], x) # Cluster-Lone Edges\n",
    "dataListCC = createDataList(edgeIndexTensors['ccBD'], edgeIndexTensors['ccNoBD'], x) # Cluster-Cluster Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "156b080f-f4e5-4c3b-835c-e50716fff483",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 5\n",
    "dataSets = {}\n",
    "dataLoaders = {}\n",
    "dataListMapping = {\n",
    "    \"train\": dataListTraining,  # Training Edges\n",
    "    \"tt\": dataListTT,           # True-True Edges\n",
    "    \"ll\": dataListLL,           # Lone-Lone Edges\n",
    "    \"lc\": dataListLC,           # Lone-Cluster Edges\n",
    "    \"cl\": dataListCL,           # Cluster-Lone Edges\n",
    "    \"cc\": dataListCC            # Cluster-Cluster Edges\n",
    "}\n",
    "\n",
    "for key, data_list in dataListMapping.items():\n",
    "    dataSets[key] = CustomDataset(data_list)\n",
    "    # For 'train', pass is_training=True, otherwise False\n",
    "    if key == \"train\":\n",
    "        dataLoaders[key] = torch.utils.data.DataLoader(\n",
    "            dataSets[key], \n",
    "            batch_size=batchSize, \n",
    "            collate_fn=lambda batch: collateData(batch, is_training=True)  # Force is_training=True for train\n",
    "        )\n",
    "    else:\n",
    "        dataLoaders[key] = torch.utils.data.DataLoader(\n",
    "            dataSets[key], \n",
    "            batch_size=batchSize, \n",
    "            collate_fn=lambda batch: collateData(batch, is_training=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6609882-66f2-4e39-bb22-2b42e2931b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the total background dataset\n",
    "dataListTotalBkg = dataListLL + dataListLC + dataListCL + dataListCC\n",
    "\n",
    "# Collate function for total background\n",
    "def collateTotalBkg(dataListTotalBkg):\n",
    "    batch_x = [data.x for data in dataListTotalBkg]\n",
    "    batch_edge_index = [data.edge_index for data in dataListTotalBkg]\n",
    "    batch_edge_index_out = [data.edge_index_out for data in dataListTotalBkg]\n",
    "    return batch_x, batch_edge_index, batch_edge_index_out\n",
    "\n",
    "# Create the total background DataLoader\n",
    "customDatasetTotalBkg = CustomDataset(dataListTotalBkg)\n",
    "dataLoaderTotalBkg = torch.utils.data.DataLoader(customDatasetTotalBkg, batch_size=batchSize, collate_fn=collateTotalBkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe9ba9-543e-4734-a89f-50846ce7e73c",
   "metadata": {},
   "source": [
    "## Multi-Edge Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "591196e5-1675-4cc6-a2b9-23bbe5a8a95e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiEdgeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultiEdgeClassifier, self).__init__()\n",
    "\n",
    "        # Node embedding layer\n",
    "        self.node_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(hidden_dim, 128)\n",
    "        self.bn1 = BatchNorm1d(128)\n",
    "        \n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "        self.bn2 = BatchNorm1d(64)\n",
    "        \n",
    "        # Edge classification layer\n",
    "        self.fc = nn.Linear(128 , output_dim) # Output logits\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_index_out):\n",
    "        edge_index = edge_index\n",
    "        x = self.node_embedding(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Edge representations\n",
    "        edge_index_to_compare = edge_index_out\n",
    "        edge_rep = torch.cat([x[edge_index_to_compare[0]], x[edge_index_to_compare[1]]], dim=1) #Check the dim=1 part\n",
    "        # Return logits (no softmax since we will use nn.CrossEntropyLoss() as out loss function, which internally\n",
    "        # handles softmaxing\n",
    "        edge_scores = self.fc(edge_rep)\n",
    "        return edge_scores\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Instantiate the model\n",
    "input_dim = 8\n",
    "hidden_dim = 256\n",
    "output_dim = 5  # Multivariate classification\n",
    "model = MultiEdgeClassifier(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # When using this loss, the target y should be class\n",
    "# labels, not one-hot encoded. Furthermore, this loss handles softmax internally\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c6e5e53-0154-4d22-b4a5-9a65a1012fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, device, data_loader, optimizer, criterion):\n",
    "    # Sets the model into training mode\n",
    "    model.train()\n",
    "    # Sends model to GPU if available, otherwise uses the CPU\n",
    "    model.to(device)\n",
    "    # Initializes the total loss per epoch list\n",
    "    totalLossPerEpoch = []\n",
    "\n",
    "    # Loops iterates over batches of data from the data loader\n",
    "    for batch_x, batch_edge_index, batch_edge_index_out, batch_y in data_loader:\n",
    "        # Sends the input features, the edge indices, and target\n",
    "        # labels to the GPU if available, otherwise the CPU\n",
    "        batch_x = torch.stack(batch_x).to(device)\n",
    "        batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "        batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "\n",
    "        # Convert target labels to LongTensor (torch.int64)\n",
    "        batch_y = [y.long().to(device) for y in batch_y]\n",
    "\n",
    "        # Clears the gradients of the model parameters to ensure\n",
    "        # they are not accumulated across batches\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initializes the loss per batch list\n",
    "        loss_per_batch = []\n",
    "\n",
    "        # Model processes each graph in the batch one by one\n",
    "        for i in range(len(batch_edge_index)):\n",
    "            # Pass the features and the edge indices into the model and store\n",
    "            # the output (logits)\n",
    "            _output = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "\n",
    "            # Ensure that model outputs (logits) are of type float32\n",
    "            _output = _output.float()\n",
    "\n",
    "            # Calculate the difference between the model output and the targets\n",
    "            # via the provided criterion (loss function)\n",
    "            # Note: Criterion expects logits and target class labels\n",
    "            loss = criterion(_output.squeeze(), batch_y[i].squeeze())\n",
    "\n",
    "            # This difference is stored in the loss_per_batch list\n",
    "            loss_per_batch.append(loss)\n",
    "\n",
    "        # The average loss across all subgraphs within the batch is calculated and stored\n",
    "        total_loss_per_batch = sum(loss_per_batch) / len(loss_per_batch)\n",
    "        totalLossPerEpoch.append(total_loss_per_batch)\n",
    "\n",
    "        # Computes the loss gradients with respect to the model parameters\n",
    "        total_loss_per_batch.backward()\n",
    "\n",
    "        # Updates the model parameters using the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "    # After processing all the batches, the average loss across all\n",
    "    # batches is calculated and returned\n",
    "    total_loss_per_epoch = sum(totalLossPerEpoch) / len(totalLossPerEpoch)\n",
    "    return total_loss_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4119837b-4490-42bd-ab3d-7546b406d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, device, data_loader_true, data_loader_bkg):\n",
    "    all_scores = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        # Loop over true edges (positive class)\n",
    "        for batch_x, batch_edge_index, batch_edge_index_out in data_loader_true:\n",
    "            batch_x = torch.stack(batch_x).to(device)\n",
    "            batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "            batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                \n",
    "                # No need to squeeze, as the output shape should be [num_edges, num_classes]\n",
    "                \n",
    "                # Assign the true label for true edges (positive class is 1 for true edges)\n",
    "                true_label = torch.ones(test_edge_scores.size(0), dtype=torch.long, device=device)\n",
    "                \n",
    "                # Append scores and true labels for this batch\n",
    "                all_scores.append(test_edge_scores)\n",
    "                true_labels.append(true_label)\n",
    "        \n",
    "        # Loop over background edges (negative class)\n",
    "        for batch_x, batch_edge_index, batch_edge_index_out in data_loader_bkg:\n",
    "            batch_x = torch.stack(batch_x).to(device)\n",
    "            batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "            batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                \n",
    "                # Assign the true label for background edges (negative class is 0 for background edges)\n",
    "                true_label = torch.zeros(test_edge_scores.size(0), dtype=torch.long, device=device)\n",
    "                \n",
    "                # Append scores and true labels for this batch\n",
    "                all_scores.append(test_edge_scores)\n",
    "                true_labels.append(true_label)\n",
    "\n",
    "    # Concatenate all scores and labels from different batches\n",
    "    all_scores = torch.cat(all_scores, dim=0).cpu().numpy()\n",
    "    true_labels = torch.cat(true_labels, dim=0).cpu().numpy()\n",
    "\n",
    "    return all_scores, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cdd2d11-5278-4b13-8820-9ea0de37fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function tests a GNN model on both true edges (positive class)\n",
    "# # and background edges (negative class), collecting the predicted scores\n",
    "# # and true labels and returning the average loss over the testing set.\n",
    "# def testModel(model, device, data_loader_true, data_loader_bkg):\n",
    "#     all_scores = []\n",
    "#     true_labels = []\n",
    "#     # total_loss = 0.0  # To accumulate the total loss\n",
    "#     # num_samples = 0   # To accumulate the number of samples\n",
    "\n",
    "#     # The .no_grad() method turns off the calculations of gradients during\n",
    "#     # the forward pass. Such calculations should be done during training,\n",
    "#     # not testing.\n",
    "#     with torch.no_grad():\n",
    "#         # Sets the model into evaluation mode\n",
    "#         model.eval()\n",
    "#         # Sends model to GPU if avaliable, otherwise uses the CPU\n",
    "#         model.to(device)\n",
    "        \n",
    "#         # Basically the same as the section in the trainModel function, but\n",
    "#         # with a different data loader used and the true and bkg label scores\n",
    "#         # appended to individual lists instead and with the loss computation\n",
    "#         # also occuring.\n",
    "        \n",
    "#         # Loop over true edges\n",
    "#         for batch_x, batch_edge_index, batch_edge_index_out in data_loader_true:\n",
    "#             batch_x = torch.stack(batch_x).to(device)\n",
    "#             batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "#             batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "\n",
    "#             for i in range(len(batch_edge_index)):\n",
    "#                 test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "#                 # Squeeze to remove extra dimensions to match the shape of y_true\n",
    "#                 test_edge_scores = test_edge_scores.squeeze() \n",
    "\n",
    "#                 # # Calculate loss for this batch\n",
    "#                 true_label = torch.ones(test_edge_scores.size(0), device=device)\n",
    "#                 # loss = loss_fn(test_edge_scores, true_label)\n",
    "                \n",
    "#                 # total_loss += loss.item() * test_edge_scores.size(0)  # Multiply by number of samples\n",
    "#                 # num_samples += test_edge_scores.size(0)  # Add the number of samples in this batch\n",
    "                \n",
    "#                 all_scores.append(test_edge_scores)\n",
    "#                 true_labels.append(true_label)\n",
    "\n",
    "#         # Loop over background edges\n",
    "#         for batch_x, batch_edge_index, batch_edge_index_out in data_loader_bkg:\n",
    "#             batch_x = torch.stack(batch_x).to(device)\n",
    "#             batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "#             batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "\n",
    "#             for i in range(len(batch_edge_index)):\n",
    "#                 test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "#                 test_edge_scores = test_edge_scores.squeeze()  # Squeeze to match the shape of y_true\n",
    "\n",
    "#                 # Calculate loss for this batch\n",
    "#                 true_label = torch.zeros(test_edge_scores.size(0), device=device)\n",
    "#                 # loss = loss_fn(test_edge_scores, true_label)\n",
    "                \n",
    "#                 # total_loss += loss.item() * test_edge_scores.size(0)\n",
    "#                 # num_samples += test_edge_scores.size(0)\n",
    "                \n",
    "#                 all_scores.append(test_edge_scores)\n",
    "#                 true_labels.append(true_label)\n",
    "\n",
    "#     # After looping through all the batches, compute average loss\n",
    "#     # avg_loss = total_loss / num_samples\n",
    "#     all_scores = torch.cat(all_scores, dim=0).cpu().numpy()\n",
    "#     true_labels = torch.cat(true_labels, dim=0).cpu().numpy()\n",
    "\n",
    "#     return all_scores, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33a72fa3-a458-45fc-bed7-f687176729d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossForTrainingAndTesting(model, loader, loss_fn, optimizer, training, device):\n",
    "    if training:\n",
    "        model.train()  # Set model to training mode\n",
    "    else:\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    if training:\n",
    "        # Training loop: loader provides 4 elements\n",
    "        for batch_x, batch_edge_index, batch_edge_index_out, batch_y in loader:\n",
    "            batch_x = torch.stack(batch_x).to(device)\n",
    "            batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "            batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "            batch_y = torch.cat(batch_y).to(device)  # True labels for the current batch\n",
    "            \n",
    "            for i in range(len(batch_edge_index)):\n",
    "                # Forward pass\n",
    "                test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                \n",
    "                # No need to squeeze, shape should be [num_edges, num_classes]\n",
    "                \n",
    "                # Compute loss using the provided true labels (`batch_y`)\n",
    "                loss = loss_fn(test_edge_scores, batch_y[i])\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    else:\n",
    "        # Evaluation loop: loader provides only 3 elements\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_edge_index, batch_edge_index_out in loader:\n",
    "                batch_x = torch.stack(batch_x).to(device)\n",
    "                batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "                batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "                \n",
    "                for i in range(len(batch_edge_index)):\n",
    "                    # Forward pass\n",
    "                    test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                    \n",
    "                    # Since there are no true labels in evaluation mode, you don't calculate loss here\n",
    "                    # but you can still append scores and labels for further analysis (e.g., metrics)\n",
    "                    all_scores.append(test_edge_scores)\n",
    "                    # Assuming we want true labels for further metrics computation\n",
    "                    # Assigning `torch.ones` or `torch.zeros` for true/bkg edges can be done here if needed\n",
    "                    \n",
    "                    num_batches += 1\n",
    "    \n",
    "    # Compute the average loss during training (loss is not calculated during evaluation)\n",
    "    average_loss = total_loss / num_batches if training else None\n",
    "    \n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f359a87e-6ede-4d61-81c7-919b3ccbc47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE BINARY CASE\n",
    "\n",
    "# def lossForTrainingAndTesting(model, loader, loss_fn, optimizer, truth_label, training):\n",
    "#     if training:\n",
    "#         model.train()  # Set model to training mode\n",
    "#     else:\n",
    "#         model.eval()  # Set model to evaluation mode\n",
    "#     total_loss = 0.0\n",
    "#     num_batches = 0\n",
    "#     all_scores = []\n",
    "#     all_labels = []\n",
    "#     if training:\n",
    "#         for batch_x, batch_edge_index, batch_edge_index_out in loader:\n",
    "#             batch_x = torch.stack(batch_x).to(device)\n",
    "#             batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "#             batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]            \n",
    "#             # For each graph in the batch, a forward pass is done, test edge scores\n",
    "#             # are predicted and squeezed to remove extra dimensions, ensuring the\n",
    "#             # scores match the shape of the true labels\n",
    "#             for i in range(len(batch_edge_index)):\n",
    "#                 # Forward pass\n",
    "#                 test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "#                 test_edge_scores = test_edge_scores.squeeze() \n",
    "#                 # Define true labels for the loss computation\n",
    "#                 y_true = torch.ones(test_edge_scores.size(0),dtype=torch.long,device=device) if truth_label else torch.zeros(test_edge_scores.size(0),dtype=torch.long,device=device)\n",
    "#                 # Compute loss using the provided loss function. The total loss\n",
    "#                 # is incremented by the loss for the current batch and the number\n",
    "#                 # of batches is incremented to track how many of the batches \n",
    "#                 # have been processed.\n",
    "#                 total_loss += loss_fn(test_edge_scores, y_true).item()\n",
    "#                 loss = loss_fn(test_edge_scores, y_true)\n",
    "#                 num_batches += 1      \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     else:\n",
    "#         with torch.no_grad():\n",
    "#             # Same as before\n",
    "#             for batch_x, batch_edge_index, batch_edge_index_out in loader:\n",
    "#                 batch_x = torch.stack(batch_x).to(device)\n",
    "#                 batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "#                 batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]            \n",
    "#                 # For each graph in the batch, a forward pass is done, test edge scores\n",
    "#                 # are predicted and squeezed to remove extra dimensions, ensuring the\n",
    "#                 # scores match the shape of the true labels\n",
    "#                 for i in range(len(batch_edge_index)):\n",
    "#                     # Forward pass\n",
    "#                     test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "#                     test_edge_scores = test_edge_scores.squeeze() \n",
    "#                     # Define true labels for the loss computation\n",
    "#                     y_true = torch.ones(test_edge_scores.size(0), device=device) if truth_label else torch.zeros(test_edge_scores.size(0), device=device)\n",
    "#                     # Compute loss using the provided loss function. The total loss\n",
    "#                     # is incremented by the loss for the current batch and the number\n",
    "#                     # of batches is incremented to track how many of the batches \n",
    "#                     # have been processed.\n",
    "#                     total_loss += loss_fn(test_edge_scores, y_true).item()\n",
    "#                     num_batches += 1\n",
    "#     average_loss = total_loss / num_batches\n",
    "#     return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a00975-4699-41ca-ab99-610828c1b24c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Total Loss Per Epoch: 0.7040\n",
      "Epoch: 2 | Total Loss Per Epoch: 0.7016\n",
      "Epoch: 3 | Total Loss Per Epoch: 0.7012\n",
      "Epoch: 4 | Total Loss Per Epoch: 0.7006\n",
      "Epoch: 5 | Total Loss Per Epoch: 0.7004\n",
      "Epoch: 6 | Total Loss Per Epoch: 0.7000\n",
      "Epoch: 7 | Total Loss Per Epoch: 0.6968\n",
      "Epoch: 8 | Total Loss Per Epoch: 0.6946\n",
      "Epoch: 9 | Total Loss Per Epoch: 0.6938\n",
      "Epoch: 10 | Total Loss Per Epoch: 0.6872\n",
      "Epoch: 11 | Total Loss Per Epoch: 0.6813\n",
      "Epoch: 12 | Total Loss Per Epoch: 0.6741\n",
      "Epoch: 13 | Total Loss Per Epoch: 0.6691\n",
      "Epoch: 14 | Total Loss Per Epoch: 0.6643\n",
      "Epoch: 15 | Total Loss Per Epoch: 0.6506\n",
      "Epoch: 16 | Total Loss Per Epoch: 0.6451\n",
      "Epoch: 17 | Total Loss Per Epoch: 0.6422\n",
      "Epoch: 18 | Total Loss Per Epoch: 0.6368\n",
      "Epoch: 19 | Total Loss Per Epoch: 0.6491\n",
      "Epoch: 20 | Total Loss Per Epoch: 0.6471\n",
      "Epoch: 21 | Total Loss Per Epoch: 0.6274\n",
      "Epoch: 22 | Total Loss Per Epoch: 0.6216\n",
      "Epoch: 23 | Total Loss Per Epoch: 0.6188\n",
      "Epoch: 24 | Total Loss Per Epoch: 0.6153\n",
      "Epoch: 25 | Total Loss Per Epoch: 0.6027\n",
      "Epoch: 26 | Total Loss Per Epoch: 0.6397\n",
      "Epoch: 27 | Total Loss Per Epoch: 0.6158\n",
      "Epoch: 28 | Total Loss Per Epoch: 0.6017\n",
      "Epoch: 29 | Total Loss Per Epoch: 0.6773\n",
      "Epoch: 30 | Total Loss Per Epoch: 0.5945\n",
      "Epoch: 31 | Total Loss Per Epoch: 0.5893\n",
      "Epoch: 32 | Total Loss Per Epoch: 0.5963\n",
      "Epoch: 33 | Total Loss Per Epoch: 0.5929\n",
      "Epoch: 34 | Total Loss Per Epoch: 0.5809\n",
      "Epoch: 35 | Total Loss Per Epoch: 0.5829\n",
      "Epoch: 36 | Total Loss Per Epoch: 0.5750\n",
      "Epoch: 37 | Total Loss Per Epoch: 0.5832\n",
      "Epoch: 38 | Total Loss Per Epoch: 0.5991\n",
      "Epoch: 39 | Total Loss Per Epoch: 0.5711\n",
      "Epoch: 40 | Total Loss Per Epoch: 0.5688\n",
      "Epoch: 41 | Total Loss Per Epoch: 0.5902\n",
      "Epoch: 42 | Total Loss Per Epoch: 0.5859\n",
      "Epoch: 43 | Total Loss Per Epoch: 0.5724\n",
      "Epoch: 44 | Total Loss Per Epoch: 0.5732\n",
      "Epoch: 45 | Total Loss Per Epoch: 0.5725\n",
      "Epoch: 46 | Total Loss Per Epoch: 0.5662\n",
      "Epoch: 47 | Total Loss Per Epoch: 0.5654\n",
      "Epoch: 48 | Total Loss Per Epoch: 0.5714\n",
      "Epoch: 49 | Total Loss Per Epoch: 0.5679\n",
      "Epoch: 50 | Total Loss Per Epoch: 0.5795\n",
      "Epoch: 51 | Total Loss Per Epoch: 0.5627\n",
      "Epoch: 52 | Total Loss Per Epoch: 0.5642\n",
      "Epoch: 53 | Total Loss Per Epoch: 0.5612\n",
      "Epoch: 54 | Total Loss Per Epoch: 0.5609\n",
      "Epoch: 55 | Total Loss Per Epoch: 0.5675\n",
      "Epoch: 56 | Total Loss Per Epoch: 0.5583\n",
      "Epoch: 57 | Total Loss Per Epoch: 0.5554\n",
      "Epoch: 58 | Total Loss Per Epoch: 0.5560\n",
      "Epoch: 59 | Total Loss Per Epoch: 0.5614\n",
      "Epoch: 60 | Total Loss Per Epoch: 0.5597\n",
      "Epoch: 61 | Total Loss Per Epoch: 0.5554\n",
      "Epoch: 62 | Total Loss Per Epoch: 0.5537\n",
      "Epoch: 63 | Total Loss Per Epoch: 0.5629\n",
      "Epoch: 64 | Total Loss Per Epoch: 0.5537\n",
      "Epoch: 65 | Total Loss Per Epoch: 0.5565\n",
      "Epoch: 66 | Total Loss Per Epoch: 0.5559\n",
      "Epoch: 67 | Total Loss Per Epoch: 0.5547\n",
      "Epoch: 68 | Total Loss Per Epoch: 0.5562\n",
      "Epoch: 69 | Total Loss Per Epoch: 0.5499\n",
      "Epoch: 70 | Total Loss Per Epoch: 0.5544\n",
      "Epoch: 71 | Total Loss Per Epoch: 0.5515\n",
      "Epoch: 72 | Total Loss Per Epoch: 0.5476\n",
      "Epoch: 73 | Total Loss Per Epoch: 0.5564\n",
      "Epoch: 74 | Total Loss Per Epoch: 0.5499\n",
      "Epoch: 75 | Total Loss Per Epoch: 0.5533\n",
      "Epoch: 76 | Total Loss Per Epoch: 0.5508\n",
      "Epoch: 77 | Total Loss Per Epoch: 0.5479\n",
      "Epoch: 78 | Total Loss Per Epoch: 0.5494\n",
      "Epoch: 79 | Total Loss Per Epoch: 0.5504\n",
      "Epoch: 80 | Total Loss Per Epoch: 0.5486\n",
      "Epoch: 81 | Total Loss Per Epoch: 0.5459\n",
      "Epoch: 82 | Total Loss Per Epoch: 0.5440\n",
      "Epoch: 83 | Total Loss Per Epoch: 0.5482\n",
      "Epoch: 84 | Total Loss Per Epoch: 0.5449\n",
      "Epoch: 85 | Total Loss Per Epoch: 0.5513\n",
      "Epoch: 86 | Total Loss Per Epoch: 0.5467\n",
      "Epoch: 87 | Total Loss Per Epoch: 0.5400\n",
      "Epoch: 88 | Total Loss Per Epoch: 0.5409\n",
      "Epoch: 89 | Total Loss Per Epoch: 0.5481\n",
      "Epoch: 90 | Total Loss Per Epoch: 0.5481\n",
      "Epoch: 91 | Total Loss Per Epoch: 0.5463\n",
      "Epoch: 92 | Total Loss Per Epoch: 0.5437\n",
      "Epoch: 93 | Total Loss Per Epoch: 0.5469\n",
      "Epoch: 94 | Total Loss Per Epoch: 0.5373\n",
      "Epoch: 95 | Total Loss Per Epoch: 0.5412\n",
      "Epoch: 96 | Total Loss Per Epoch: 0.5399\n",
      "Epoch: 97 | Total Loss Per Epoch: 0.5381\n",
      "Epoch: 98 | Total Loss Per Epoch: 0.5448\n",
      "Epoch: 99 | Total Loss Per Epoch: 0.5373\n",
      "Epoch: 100 | Total Loss Per Epoch: 0.5384\n",
      "Epoch: 101 | Total Loss Per Epoch: 0.5375\n",
      "Epoch: 102 | Total Loss Per Epoch: 0.5409\n",
      "Epoch: 103 | Total Loss Per Epoch: 0.5361\n",
      "Epoch: 104 | Total Loss Per Epoch: 0.5410\n",
      "Epoch: 105 | Total Loss Per Epoch: 0.5337\n",
      "Epoch: 106 | Total Loss Per Epoch: 0.5323\n",
      "Epoch: 107 | Total Loss Per Epoch: 0.5360\n",
      "Epoch: 108 | Total Loss Per Epoch: 0.5409\n",
      "Epoch: 109 | Total Loss Per Epoch: 0.5361\n",
      "Epoch: 110 | Total Loss Per Epoch: 0.5394\n",
      "Epoch: 111 | Total Loss Per Epoch: 0.5319\n",
      "Epoch: 112 | Total Loss Per Epoch: 0.5322\n",
      "Epoch: 113 | Total Loss Per Epoch: 0.5332\n",
      "Epoch: 114 | Total Loss Per Epoch: 0.5306\n",
      "Epoch: 115 | Total Loss Per Epoch: 0.5313\n",
      "Epoch: 116 | Total Loss Per Epoch: 0.5279\n",
      "Epoch: 117 | Total Loss Per Epoch: 0.5334\n",
      "Epoch: 118 | Total Loss Per Epoch: 0.5363\n",
      "Epoch: 119 | Total Loss Per Epoch: 0.5327\n",
      "Epoch: 120 | Total Loss Per Epoch: 0.5401\n",
      "Epoch: 121 | Total Loss Per Epoch: 0.5338\n",
      "Epoch: 122 | Total Loss Per Epoch: 0.5281\n",
      "Epoch: 123 | Total Loss Per Epoch: 0.5264\n",
      "Epoch: 124 | Total Loss Per Epoch: 0.5271\n",
      "Epoch: 125 | Total Loss Per Epoch: 0.5261\n",
      "Epoch: 126 | Total Loss Per Epoch: 0.5262\n",
      "Epoch: 127 | Total Loss Per Epoch: 0.5287\n",
      "Epoch: 128 | Total Loss Per Epoch: 0.5297\n",
      "Epoch: 129 | Total Loss Per Epoch: 0.5355\n",
      "Epoch: 130 | Total Loss Per Epoch: 0.5299\n",
      "Epoch: 131 | Total Loss Per Epoch: 0.5247\n",
      "Epoch: 132 | Total Loss Per Epoch: 0.5255\n",
      "Epoch: 133 | Total Loss Per Epoch: 0.5276\n",
      "Epoch: 134 | Total Loss Per Epoch: 0.5262\n",
      "Epoch: 135 | Total Loss Per Epoch: 0.5243\n",
      "Epoch: 136 | Total Loss Per Epoch: 0.5241\n",
      "Epoch: 137 | Total Loss Per Epoch: 0.5243\n",
      "Epoch: 138 | Total Loss Per Epoch: 0.5233\n",
      "Epoch: 139 | Total Loss Per Epoch: 0.5231\n",
      "Epoch: 140 | Total Loss Per Epoch: 0.5219\n",
      "Epoch: 141 | Total Loss Per Epoch: 0.5218\n",
      "Epoch: 142 | Total Loss Per Epoch: 0.5240\n",
      "Epoch: 143 | Total Loss Per Epoch: 0.5218\n",
      "Epoch: 144 | Total Loss Per Epoch: 0.5215\n",
      "Epoch: 145 | Total Loss Per Epoch: 0.5236\n",
      "Epoch: 146 | Total Loss Per Epoch: 0.5201\n",
      "Epoch: 147 | Total Loss Per Epoch: 0.5190\n",
      "Epoch: 148 | Total Loss Per Epoch: 0.5190\n",
      "Epoch: 149 | Total Loss Per Epoch: 0.5184\n",
      "Epoch: 150 | Total Loss Per Epoch: 0.5196\n",
      "Epoch: 151 | Total Loss Per Epoch: 0.5210\n",
      "Epoch: 152 | Total Loss Per Epoch: 0.5181\n",
      "Epoch: 153 | Total Loss Per Epoch: 0.5188\n",
      "Epoch: 154 | Total Loss Per Epoch: 0.5189\n",
      "Epoch: 155 | Total Loss Per Epoch: 0.5222\n",
      "Epoch: 156 | Total Loss Per Epoch: 0.5200\n",
      "Epoch: 157 | Total Loss Per Epoch: 0.5175\n",
      "Epoch: 158 | Total Loss Per Epoch: 0.5179\n",
      "Epoch: 159 | Total Loss Per Epoch: 0.5192\n",
      "Epoch: 160 | Total Loss Per Epoch: 0.5245\n",
      "Epoch: 161 | Total Loss Per Epoch: 0.5175\n",
      "Epoch: 162 | Total Loss Per Epoch: 0.5175\n",
      "Epoch: 163 | Total Loss Per Epoch: 0.5150\n",
      "Epoch: 164 | Total Loss Per Epoch: 0.5157\n",
      "Epoch: 165 | Total Loss Per Epoch: 0.5153\n",
      "Epoch: 166 | Total Loss Per Epoch: 0.5169\n",
      "Epoch: 167 | Total Loss Per Epoch: 0.5142\n",
      "Epoch: 168 | Total Loss Per Epoch: 0.5161\n",
      "Epoch: 169 | Total Loss Per Epoch: 0.5148\n",
      "Epoch: 170 | Total Loss Per Epoch: 0.5143\n",
      "Epoch: 171 | Total Loss Per Epoch: 0.5150\n",
      "Epoch: 172 | Total Loss Per Epoch: 0.5137\n",
      "Epoch: 173 | Total Loss Per Epoch: 0.5144\n",
      "Epoch: 174 | Total Loss Per Epoch: 0.5141\n",
      "Epoch: 175 | Total Loss Per Epoch: 0.5135\n",
      "Epoch: 176 | Total Loss Per Epoch: 0.5136\n",
      "Epoch: 177 | Total Loss Per Epoch: 0.5190\n",
      "Epoch: 178 | Total Loss Per Epoch: 0.5164\n",
      "Epoch: 179 | Total Loss Per Epoch: 0.5206\n",
      "Epoch: 180 | Total Loss Per Epoch: 0.5175\n",
      "Epoch: 181 | Total Loss Per Epoch: 0.5161\n",
      "Epoch: 182 | Total Loss Per Epoch: 0.5148\n",
      "Epoch: 183 | Total Loss Per Epoch: 0.5149\n",
      "Epoch: 184 | Total Loss Per Epoch: 0.5148\n",
      "Epoch: 185 | Total Loss Per Epoch: 0.5140\n",
      "Epoch: 186 | Total Loss Per Epoch: 0.5149\n",
      "Epoch: 187 | Total Loss Per Epoch: 0.5152\n",
      "Epoch: 188 | Total Loss Per Epoch: 0.5134\n",
      "Epoch: 189 | Total Loss Per Epoch: 0.5126\n",
      "Epoch: 190 | Total Loss Per Epoch: 0.5132\n",
      "Epoch: 191 | Total Loss Per Epoch: 0.5130\n",
      "Epoch: 192 | Total Loss Per Epoch: 0.5131\n",
      "Epoch: 193 | Total Loss Per Epoch: 0.5126\n",
      "Epoch: 194 | Total Loss Per Epoch: 0.5122\n",
      "Epoch: 195 | Total Loss Per Epoch: 0.5117\n",
      "Epoch: 196 | Total Loss Per Epoch: 0.5122\n",
      "Epoch: 197 | Total Loss Per Epoch: 0.5114\n",
      "Epoch: 198 | Total Loss Per Epoch: 0.5113\n",
      "Epoch: 199 | Total Loss Per Epoch: 0.5113\n",
      "Epoch: 200 | Total Loss Per Epoch: 0.5117\n",
      "Epoch: 201 | Total Loss Per Epoch: 0.5113\n",
      "Epoch: 202 | Total Loss Per Epoch: 0.5115\n",
      "Epoch: 203 | Total Loss Per Epoch: 0.5115\n",
      "Epoch: 204 | Total Loss Per Epoch: 0.5116\n",
      "Epoch: 205 | Total Loss Per Epoch: 0.5113\n",
      "Epoch: 206 | Total Loss Per Epoch: 0.5119\n",
      "Epoch: 207 | Total Loss Per Epoch: 0.5113\n",
      "Epoch: 208 | Total Loss Per Epoch: 0.5115\n",
      "Epoch: 209 | Total Loss Per Epoch: 0.5114\n",
      "Epoch: 210 | Total Loss Per Epoch: 0.5113\n",
      "Epoch: 211 | Total Loss Per Epoch: 0.5115\n",
      "Epoch: 212 | Total Loss Per Epoch: 0.5131\n",
      "Epoch: 213 | Total Loss Per Epoch: 0.5127\n",
      "Epoch: 214 | Total Loss Per Epoch: 0.5118\n",
      "Epoch: 215 | Total Loss Per Epoch: 0.5123\n",
      "Epoch: 216 | Total Loss Per Epoch: 0.5122\n",
      "Epoch: 217 | Total Loss Per Epoch: 0.5120\n",
      "Epoch: 218 | Total Loss Per Epoch: 0.5117\n",
      "Epoch: 219 | Total Loss Per Epoch: 0.5112\n",
      "Epoch: 220 | Total Loss Per Epoch: 0.5110\n",
      "Epoch: 221 | Total Loss Per Epoch: 0.5105\n",
      "Epoch: 222 | Total Loss Per Epoch: 0.5106\n",
      "Epoch: 223 | Total Loss Per Epoch: 0.5101\n",
      "Epoch: 224 | Total Loss Per Epoch: 0.5100\n",
      "Epoch: 225 | Total Loss Per Epoch: 0.5097\n",
      "Epoch: 226 | Total Loss Per Epoch: 0.5097\n",
      "Epoch: 227 | Total Loss Per Epoch: 0.5096\n",
      "Epoch: 228 | Total Loss Per Epoch: 0.5092\n",
      "Epoch: 229 | Total Loss Per Epoch: 0.5090\n",
      "Epoch: 230 | Total Loss Per Epoch: 0.5089\n",
      "Epoch: 231 | Total Loss Per Epoch: 0.5088\n",
      "Epoch: 232 | Total Loss Per Epoch: 0.5086\n",
      "Epoch: 233 | Total Loss Per Epoch: 0.5085\n",
      "Epoch: 234 | Total Loss Per Epoch: 0.5083\n",
      "Epoch: 235 | Total Loss Per Epoch: 0.5082\n",
      "Epoch: 236 | Total Loss Per Epoch: 0.5082\n",
      "Epoch: 237 | Total Loss Per Epoch: 0.5079\n",
      "Epoch: 238 | Total Loss Per Epoch: 0.5078\n",
      "Epoch: 239 | Total Loss Per Epoch: 0.5077\n",
      "Epoch: 240 | Total Loss Per Epoch: 0.5077\n",
      "Epoch: 241 | Total Loss Per Epoch: 0.5076\n",
      "Epoch: 242 | Total Loss Per Epoch: 0.5075\n",
      "Epoch: 243 | Total Loss Per Epoch: 0.5076\n",
      "Epoch: 244 | Total Loss Per Epoch: 0.5076\n",
      "Epoch: 245 | Total Loss Per Epoch: 0.5074\n",
      "Epoch: 246 | Total Loss Per Epoch: 0.5075\n",
      "Epoch: 247 | Total Loss Per Epoch: 0.5072\n",
      "Epoch: 248 | Total Loss Per Epoch: 0.5070\n",
      "Epoch: 249 | Total Loss Per Epoch: 0.5068\n",
      "Epoch: 250 | Total Loss Per Epoch: 0.5065\n",
      "Epoch: 251 | Total Loss Per Epoch: 0.5065\n",
      "Epoch: 252 | Total Loss Per Epoch: 0.5063\n",
      "Epoch: 253 | Total Loss Per Epoch: 0.5062\n",
      "Epoch: 254 | Total Loss Per Epoch: 0.5061\n",
      "Epoch: 255 | Total Loss Per Epoch: 0.5059\n",
      "Epoch: 256 | Total Loss Per Epoch: 0.5058\n",
      "Epoch: 257 | Total Loss Per Epoch: 0.5057\n",
      "Epoch: 258 | Total Loss Per Epoch: 0.5056\n",
      "Epoch: 259 | Total Loss Per Epoch: 0.5055\n",
      "Epoch: 260 | Total Loss Per Epoch: 0.5054\n",
      "Epoch: 261 | Total Loss Per Epoch: 0.5052\n",
      "Epoch: 262 | Total Loss Per Epoch: 0.5052\n",
      "Epoch: 263 | Total Loss Per Epoch: 0.5050\n",
      "Epoch: 264 | Total Loss Per Epoch: 0.5050\n",
      "Epoch: 265 | Total Loss Per Epoch: 0.5048\n",
      "Epoch: 266 | Total Loss Per Epoch: 0.5048\n",
      "Epoch: 267 | Total Loss Per Epoch: 0.5047\n",
      "Epoch: 268 | Total Loss Per Epoch: 0.5046\n",
      "Epoch: 269 | Total Loss Per Epoch: 0.5046\n",
      "Epoch: 270 | Total Loss Per Epoch: 0.5045\n",
      "Epoch: 271 | Total Loss Per Epoch: 0.5045\n",
      "Epoch: 272 | Total Loss Per Epoch: 0.5044\n",
      "Epoch: 273 | Total Loss Per Epoch: 0.5043\n",
      "Epoch: 274 | Total Loss Per Epoch: 0.5043\n",
      "Epoch: 275 | Total Loss Per Epoch: 0.5042\n",
      "Epoch: 276 | Total Loss Per Epoch: 0.5042\n",
      "Epoch: 277 | Total Loss Per Epoch: 0.5041\n",
      "Epoch: 278 | Total Loss Per Epoch: 0.5041\n",
      "Epoch: 279 | Total Loss Per Epoch: 0.5040\n",
      "Epoch: 280 | Total Loss Per Epoch: 0.5040\n",
      "Epoch: 281 | Total Loss Per Epoch: 0.5039\n",
      "Epoch: 282 | Total Loss Per Epoch: 0.5039\n",
      "Epoch: 283 | Total Loss Per Epoch: 0.5038\n",
      "Epoch: 284 | Total Loss Per Epoch: 0.5038\n",
      "Epoch: 285 | Total Loss Per Epoch: 0.5038\n",
      "Epoch: 286 | Total Loss Per Epoch: 0.5037\n",
      "Epoch: 287 | Total Loss Per Epoch: 0.5037\n",
      "Epoch: 288 | Total Loss Per Epoch: 0.5037\n",
      "Epoch: 289 | Total Loss Per Epoch: 0.5036\n",
      "Epoch: 290 | Total Loss Per Epoch: 0.5036\n",
      "Epoch: 291 | Total Loss Per Epoch: 0.5036\n",
      "Epoch: 292 | Total Loss Per Epoch: 0.5036\n",
      "Epoch: 293 | Total Loss Per Epoch: 0.5035\n",
      "Epoch: 294 | Total Loss Per Epoch: 0.5035\n",
      "Epoch: 295 | Total Loss Per Epoch: 0.5034\n",
      "Epoch: 296 | Total Loss Per Epoch: 0.5034\n",
      "Epoch: 297 | Total Loss Per Epoch: 0.5034\n",
      "Epoch: 298 | Total Loss Per Epoch: 0.5034\n",
      "Epoch: 299 | Total Loss Per Epoch: 0.5033\n",
      "Epoch: 300 | Total Loss Per Epoch: 0.5034\n",
      "Epoch: 301 | Total Loss Per Epoch: 0.5034\n",
      "Epoch: 302 | Total Loss Per Epoch: 0.5034\n",
      "Epoch: 303 | Total Loss Per Epoch: 0.5033\n",
      "Epoch: 304 | Total Loss Per Epoch: 0.5032\n"
     ]
    }
   ],
   "source": [
    "# Here we train the GNN for a certain number of epochs. We compute \n",
    "# the loss at each epoch, and test the model periodically using\n",
    "# separate datasets of true edges and background edges.\n",
    "\n",
    "num_epochs = 500\n",
    "lossPerEpoch = []\n",
    "scores = []\n",
    "truth_labels = []\n",
    "avgLoss_TrueTrain = []\n",
    "avgLoss_TrueTest = []\n",
    "avgLoss_BkgTrain = []\n",
    "avgLoss_BkgTest = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Model trained using the train function\n",
    "    total_loss_per_epoch = trainModel(model, device, dataLoaders['train'], optimizer, criterion)\n",
    "    # Total loss per epoch stored in the lossPerEpoch list and\n",
    "    # the current epoch's loss is printed to track progress\n",
    "    lossPerEpoch.append(total_loss_per_epoch)\n",
    "    \n",
    "    # Update the learning rate at the end of each epoch according\n",
    "    # to the predefined schedule\n",
    "    scheduler.step()\n",
    "    # The model is trained on both true and bkg edges, returning\n",
    "    # epoch scores (the predicted scores for both edge types) and\n",
    "    # epoch truth labels (1 for true, 0 for bkg).\n",
    "    epoch_scores, epoch_truth_labels = testModel(model = model,\n",
    "                                                 device = device,\n",
    "                                                 data_loader_true = dataLoaders['tt'],\n",
    "                                                 data_loader_bkg = dataLoaderTotalBkg)\n",
    "    \n",
    "    # Compute the average loss for true edges at the current epoch\n",
    "    avgLossTrueTrain = lossForTrainingAndTesting(model = model,\n",
    "                                                 loader = dataLoaders['train'],\n",
    "                                                 loss_fn = criterion,\n",
    "                                                 optimizer = optimizer,\n",
    "                                                 training = True,\n",
    "                                                 device=device)\n",
    "    avgLossTrueTest = lossForTrainingAndTesting(model = model,\n",
    "                                                loader = dataLoaders['tt'],\n",
    "                                                loss_fn = criterion,\n",
    "                                                optimizer = optimizer,\n",
    "                                                training = False,\n",
    "                                                device=device)\n",
    "    \n",
    "    # Compute the average loss for bkg edges at the current epoch\n",
    "    avgLossBkgTrain = lossForTrainingAndTesting(model = model,\n",
    "                                                loader = dataLoaders['train'],\n",
    "                                                loss_fn = criterion,\n",
    "                                                optimizer = optimizer,\n",
    "                                                training = True,\n",
    "                                                device=device)\n",
    "    avgLossBkgTest= lossForTrainingAndTesting(model = model,\n",
    "                                              loader = dataLoaderTotalBkg,\n",
    "                                              loss_fn = criterion,\n",
    "                                              optimizer = optimizer,\n",
    "                                              training = False,\n",
    "                                              device=device)    \n",
    "    \n",
    "    # The predicted scores, truth labels, average loss for true edges\n",
    "    # and average loss for bkg edges are stored to be used for later \n",
    "    # analysis (Like in the creation of the ROC curve)\n",
    "    scores.append(epoch_scores)\n",
    "    truth_labels.append(epoch_truth_labels)\n",
    "    avgLoss_TrueTrain.append(avgLossTrueTrain)\n",
    "    avgLoss_TrueTest.append(avgLossTrueTest)\n",
    "    avgLoss_BkgTrain.append(avgLossBkgTrain)\n",
    "    avgLoss_BkgTest.append(avgLossBkgTest)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1} | Total Loss Per Epoch: {total_loss_per_epoch.item():.4f}\")\n",
    "#Trains the GNN for 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea20f63-97a5-4617-8940-5b82ce01f15c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Moves the loss back to the cpu and changes it into a numpy array\n",
    "lossPerEpoch = [tensor.cpu() for tensor in lossPerEpoch]\n",
    "lossPerEpoch = [tensor.detach().numpy() for tensor in lossPerEpoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8466e10-6d53-4b51-9543-cd0d81f3b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the trained model to an external file.\n",
    "path = \"/storage/mxg1065/MultiClassGNN/Multi_GNNCalo_cluster.pth\"\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e803612-ba3c-41e0-bd7b-6e808c3299c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that saves the data obtained after training the model\n",
    "# in the form of a compressed dictionary.\n",
    "def saveDataH5(fileName, dataDict, compressionType):\n",
    "    with h5py.File(fileName, 'w') as file:\n",
    "        for key, data in dataDict.items():\n",
    "            file.create_dataset(key, data=data, compression = compressionType)\n",
    "\n",
    "saveDataH5(\"/storage/mxg1065/MultiClassGNN/data.h5\", \n",
    "           {\"lossData\": lossPerEpoch,\n",
    "            \"scores\": scores,\n",
    "            \"truth_labels\": truth_labels,\n",
    "            \"avgLoss_TrueTrain\": avgLoss_TrueTrain,\n",
    "            \"avgLoss_TrueTest\":avgLoss_TrueTest,\n",
    "            \"avgLoss_BkgTrain\":avgLoss_BkgTrain,\n",
    "            \"avgLoss_BkgTest\":avgLoss_BkgTest},\n",
    "           'gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
