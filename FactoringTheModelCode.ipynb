{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "random.seed(42)  \n",
    "\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns  \n",
    "\n",
    "from collections import Counter  \n",
    "\n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F  \n",
    "import torch.optim as optim  \n",
    "\n",
    "from torch.nn import BatchNorm1d  \n",
    "from torch_geometric.data import Data  \n",
    "from torch_geometric.nn import GCNConv  \n",
    "from torch_geometric.transforms import ToUndirected  \n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix  \n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "\n",
    "import uproot  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['analysis;1']\n"
     ]
    }
   ],
   "source": [
    "file = uproot.open('/home/mxg1065/MyxAODAnalysis_super3D.outputs.root')\n",
    "print(file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RunNumber', 'EventNumber', 'cell_eta', 'cell_phi', 'cell_x', 'cell_y', 'cell_z', 'cell_subCalo', 'cell_sampling', 'cell_size', 'cell_hashID', 'neighbor', 'seedCell_id', 'cell_e', 'cell_noiseSigma', 'cell_SNR', 'cell_time', 'cell_weight', 'cell_truth', 'cell_truth_indices', 'cell_shared_indices', 'cell_cluster_index', 'cluster_to_cell_indices', 'cluster_to_cell_weights', 'cell_to_cluster_e', 'cell_to_cluster_eta', 'cell_to_cluster_phi', 'cluster_eta', 'cluster_phi', 'cluster_e', 'cellsNo_cluster', 'clustersNo_event', 'jetEnergyWtdTimeAve', 'jetEta', 'jetPhi', 'jetE', 'jetPt', 'jetNumberPerEvent', 'cellIndices_per_jet']\n"
     ]
    }
   ],
   "source": [
    "tree = file['analysis;1']\n",
    "branches = tree.arrays()\n",
    "print(tree.keys()) # Variables per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 events and 187652 cells\n",
    "# Arrays containing information about the energy, noise, snr, \n",
    "cell_e = np.array(branches['cell_e'])\n",
    "cell_noise = np.array(branches['cell_noiseSigma'])\n",
    "cell_snr = np.array(branches['cell_SNR'])\n",
    "cell_eta = np.array(branches['cell_eta'])\n",
    "cell_phi = np.array(branches['cell_phi'])\n",
    "\n",
    "# Represents the index of the cluster that each cell corresponds to. If the index\n",
    "# is 0, that means that the given cell does not belong to a cluster.\n",
    "cell_to_cluster_index = np.array(branches['cell_cluster_index'])\n",
    "\n",
    "# For each entry, contains the IDs of cells neighboring a given cell\n",
    "neighbor = branches['neighbor']\n",
    "\n",
    "num_of_events = len(cell_e) # 100 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02640459 0.24943821 0.09700817 0.23466232 0.5085498 ]\n",
      " [0.02567646 0.24627675 0.09700815 0.23466876 0.52418756]\n",
      " [0.02508186 0.24369499 0.09700817 0.23467347 0.5398244 ]\n",
      " ...\n",
      " [0.02632877 0.24791998 0.03177016 0.62017125 0.4921177 ]\n",
      " [0.02705116 0.2511391  0.07512318 0.6461531  0.4921177 ]\n",
      " [0.02638626 0.24820389 0.04149057 0.6731673  0.4921177 ]]\n",
      "(187652, 5)\n"
     ]
    }
   ],
   "source": [
    "# We use the data arrays to crete a data dictionary, where each entry corresponds\n",
    "# to the data of a given event; we scale this data.\n",
    "data = {}\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    data[f'data_{i}'] = np.concatenate((np.expand_dims(cell_snr[i], axis=1),\n",
    "                                        np.expand_dims(cell_e[i], axis=1),\n",
    "                                        np.expand_dims(cell_noise[i], axis=1),\n",
    "                                        np.expand_dims(cell_eta[i], axis=1),\n",
    "                                        np.expand_dims(cell_phi[i], axis=1)), axis=1)\n",
    "    \n",
    "# We combine the data into one array and apply the MinMaxScaler\n",
    "combined_data = np.vstack([data[key] for key in data])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_combined_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "# The scaled data is split to have the save structure as the original data dict\n",
    "scaled_data = {}\n",
    "start_idx = 0\n",
    "for i in range(num_of_events):\n",
    "    end_idx = start_idx + data[f\"data_{i}\"].shape[0]\n",
    "    scaled_data[f\"data_{i}\"] = scaled_combined_data[start_idx:end_idx]\n",
    "    start_idx = end_idx\n",
    "\n",
    "print(scaled_data[\"data_0\"])\n",
    "print(scaled_data[\"data_0\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepairing Neighbor Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186986 187352]\n"
     ]
    }
   ],
   "source": [
    "# The IDs of the broken cells (those with zero noise) are collected\n",
    "broken_cells = []\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    cells = np.argwhere(cell_noise[i]==0).flatten()\n",
    "    broken_cells = np.squeeze(cells)\n",
    "\n",
    "print(broken_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the values associated with neighbor[0] and neighbor[1] are all equal\n",
    "# we will just work with neighbor[0] to simplify our calculations\n",
    "neighbor = neighbor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We loop through the neighbor awkward array and remove the IDs associated\n",
    "# with the broken cells.  Loops through all cells in the neighbor list. If the loop \n",
    "# reaches the cell numbers 186986 or 187352, loop skips over these inoperative cells. \n",
    "# The final list contains tuples (i,j) where i is the cell ID in question and the \n",
    "# js are the neighboring cell IDs\n",
    "neighbor_pairs_list = []\n",
    "num_of_cells = len(neighbor) # 187652 cells\n",
    "\n",
    "for i in range(num_of_cells):\n",
    "    if i in broken_cells:\n",
    "        continue\n",
    "    for j in neighbor[i]:\n",
    "        if j in broken_cells:\n",
    "            continue\n",
    "        neighbor_pairs_list.append((i, int(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully excluded broken cells.\n"
     ]
    }
   ],
   "source": [
    "# This code checks to see if the broken cells were removed\n",
    "found_broken_cells = []\n",
    "\n",
    "for pair in neighbor_pairs_list:\n",
    "    # Loop through each cell in pair\n",
    "    for cell in pair:\n",
    "        # If the cell is broken, appends to list\n",
    "        if cell in broken_cells:\n",
    "            found_broken_cells.append(cell)\n",
    "\n",
    "if found_broken_cells:\n",
    "    print(\"Error: Broken cells are still present in neighbor pairs.\")\n",
    "else:\n",
    "    print(\"Successfully excluded broken cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 90345 119588]\n",
      " [  4388  17680]\n",
      " [ 39760  39825]\n",
      " ...\n",
      " [159757 168717]\n",
      " [ 62911  78974]\n",
      " [135353 135609]]\n",
      "(1250242, 2)\n"
     ]
    }
   ],
   "source": [
    "# These functions remove permutation variants\n",
    "def canonical_form(t):\n",
    "    return tuple(sorted(t))\n",
    "\n",
    "def remove_permutation_variants(tuple_list):\n",
    "    unique_tuples = set(canonical_form(t) for t in tuple_list)\n",
    "    return [tuple(sorted(t)) for t in unique_tuples]\n",
    "\n",
    "neighbor_pairs_list = np.array(remove_permutation_variants(neighbor_pairs_list))\n",
    "print(neighbor_pairs_list)\n",
    "print(neighbor_pairs_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Labels for the Neighbor Pairs\n",
    "\n",
    "For a given pair of cells and the IDs of the clusters that they belong to (i, j), if\n",
    "1. i=j and both are nonzero, then both cells are part of the same cluster. \n",
    "    * We call these True-True pairs and label them with 1\n",
    "2. i=j and both are zero, then both cells are not part of any cluster. \n",
    "    * We call these Lone-Lone pairs and label them with 0\n",
    "3. i is nonzero and j=0, then cell i is part of a cluster while cell j is not. \n",
    "    * We call these Cluster-Lone pairs and label them with 2\n",
    "4. i=0 and j is nonzero, then cell i is not part of a cluste while cell j is. \n",
    "    * We call these Lone-Cluster pairs and label them with 3\n",
    "5. i is not the same as j and both are nonzero, then both cells are part of different clusters. \n",
    "    * We call these Cluster-Cluster pairs and label them with 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1250242)\n",
      "[[0 0 0 ... 0 0 2]\n",
      " [3 3 0 ... 0 0 2]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 2 0 0]]\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Initialize array for labels\n",
    "labels_for_neighbor_pairs = np.zeros((num_of_events, len(neighbor_pairs_list)), dtype=int)\n",
    "\n",
    "# Extracting the individual cells within a cell pair\n",
    "cell_0 = cell_to_cluster_index[:, neighbor_pairs_list[:, 0]]\n",
    "cell_1 = cell_to_cluster_index[:, neighbor_pairs_list[:, 1]]\n",
    "\n",
    "# Computing labels using vectorized operations\n",
    "same_cluster = cell_0 == cell_1 \n",
    "both_nonzero = (cell_0 != 0) & (cell_1 != 0)\n",
    "\n",
    "# Lone-Lone (0)\n",
    "labels_for_neighbor_pairs[same_cluster & (cell_0 == 0)] = 0\n",
    "\n",
    "# True-True (1)\n",
    "labels_for_neighbor_pairs[same_cluster & (cell_0 != 0)] = 1\n",
    "\n",
    "# Cluster-Cluster (4)\n",
    "labels_for_neighbor_pairs[~same_cluster & both_nonzero] = 4\n",
    "\n",
    "# Lone-Cluster (3)\n",
    "labels_for_neighbor_pairs[~same_cluster & (cell_0 == 0) & (cell_1 != 0)] = 3\n",
    "\n",
    "# Cluster-Lone (2)\n",
    "labels_for_neighbor_pairs[~same_cluster & (cell_0 != 0) & (cell_1 == 0)] = 2\n",
    "\n",
    "print(labels_for_neighbor_pairs.shape)\n",
    "print(labels_for_neighbor_pairs)\n",
    "print(np.unique(labels_for_neighbor_pairs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Multi-Class Batch Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassBatchGenerator(IterableDataset):\n",
    "    def __init__(self, x, neighbor_pairs, labels_for_neighbor_pairs, batch_size, class_counts):\n",
    "        '''\n",
    "        Class Purpose: \n",
    "        This class is a custom PyTorch IterableDataset designed to generate mini-batches of data \n",
    "        for multi-class classification tasks. It supports fixed class-based sampling per batch.\n",
    "\n",
    "        How it works:\n",
    "        1. Initializes with event data, labels, and the exact number of samples per class.\n",
    "        2. Precomputes permutations for each event, avoiding per-batch shuffling.\n",
    "        3. Selects samples based on precomputed permutations and yields batches.\n",
    "\n",
    "        Arguments:\n",
    "        x: (np.ndarray) Feature matrix of shape (num_of_events, num_features).\n",
    "        neighbor_pairs: (np.ndarray) Array of cell pair indices of shape (num_of_events, num_pairs, 2).\n",
    "        labels_for_neighbor_pairs: (np.ndarray) Array of labels for each pair in each event, shape (num_of_events, num_pairs).\n",
    "        batch_size: (int) The total number of samples per batch.\n",
    "        class_counts: (dict) Specifies the exact number of samples to include per class in each batch, e.g., {1: 15, 2: 10, 3: 5}\n",
    "        '''\n",
    "        self.x = x\n",
    "        self.neighbor_pairs = neighbor_pairs\n",
    "        self.labels_for_neighbor_pairs = labels_for_neighbor_pairs\n",
    "        self.batch_size = batch_size\n",
    "        self.class_counts = class_counts  # Dictionary {class_label: num_samples_per_batch}\n",
    "\n",
    "        # Store indices per class\n",
    "        self.indices = {cls: np.where(labels_for_neighbor_pairs == cls) for cls in np.unique(labels_for_neighbor_pairs)}\n",
    "\n",
    "        # Generate precomputed permutations for each event\n",
    "        self.permutations = self._precompute_permutations()\n",
    "\n",
    "    def _precompute_permutations(self):\n",
    "        \"\"\"Precompute a permutation of pair indices and corresponding labels for each event.\"\"\"\n",
    "        permutations = {}\n",
    "        \n",
    "        for cls, (event_ids, pair_ids) in self.indices.items():\n",
    "            permuted_indices = []\n",
    "            for event in np.unique(event_ids):\n",
    "                event_mask = event_ids == event\n",
    "                shuffled_indices = np.random.permutation(pair_ids[event_mask])  # Shuffle indices\n",
    "                \n",
    "                # Store tuples of (event_id, shuffled pair indices)\n",
    "                permuted_indices.append((event, shuffled_indices))\n",
    "            \n",
    "            permutations[cls] = permuted_indices\n",
    "        \n",
    "        return permutations\n",
    "\n",
    "    def _batch_generator(self):\n",
    "        \"\"\"Generates batches dynamically using precomputed permutations.\"\"\"\n",
    "        \n",
    "        while True:\n",
    "            batch_data = []\n",
    "            batch_labels = []\n",
    "\n",
    "            for cls, n_samples in self.class_counts.items():\n",
    "                if cls not in self.permutations or len(self.permutations[cls]) == 0:\n",
    "                    continue\n",
    "\n",
    "                selected_pairs = []\n",
    "                selected_events = []\n",
    "\n",
    "                for event, shuffled_pairs in self.permutations[cls]:\n",
    "                    if len(shuffled_pairs) < n_samples:\n",
    "                        continue  # Skip if not enough pairs\n",
    "                    \n",
    "                    selected_indices = shuffled_pairs[:n_samples]\n",
    "                    selected_pairs.extend(selected_indices)\n",
    "                    selected_events.extend([event] * n_samples)\n",
    "                    \n",
    "                    break  # Stop after collecting enough samples\n",
    "\n",
    "                if len(selected_events) == 0:\n",
    "                    continue  # Skip if no valid samples found\n",
    "\n",
    "                # Ensure features and labels are sampled correctly\n",
    "                batch_data.extend(self.x[selected_events])  \n",
    "                batch_labels.extend(self.labels_for_neighbor_pairs[selected_events, selected_pairs])  \n",
    "\n",
    "            # Convert to numpy arrays before yielding\n",
    "            batch_data = np.array(batch_data)\n",
    "            batch_labels = np.array(batch_labels)\n",
    "\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batch_generator())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_data(indices, labels):\n",
    "    \"\"\"Randomize indices and labels while keeping the same permutation.\"\"\"\n",
    "    for i in range(indices.shape[0]):  # Iterate over events\n",
    "        perm = np.random.permutation(indices.shape[1])\n",
    "        indices[i] = indices[i, perm]\n",
    "        labels[i] = labels[i, perm]\n",
    "    return indices, labels\n",
    "\n",
    "# List of how many of each class I want in a batch - fix this instead of class ratios\n",
    "# Notice, for every batch, you are shuffling, instead you should get a permtation for each event,\n",
    "# then you select n_i of each type of permutation, then you move the data out\n",
    "\n",
    "class MultiClassBatchGenerator(IterableDataset):\n",
    "    def __init__(self, x, neighbor_pairs, labels_for_neighbor_pairs, batch_size, class_sampling_ratio):\n",
    "        '''\n",
    "        Class Purpose: \n",
    "        This class is a custom PyTorch IterableDataset designed to generate mini-batches of data \n",
    "        for multi-class classification tasks. It supports class-specific sampling, where certain\n",
    "        classes can be more prevalent than others.\n",
    "\n",
    "        How it works:\n",
    "        1. Initializes with event data, labels, and class sampling ratios.\n",
    "        2. Shuffles and samples indices based on class distribution.\n",
    "        3. Yields batches of data and labels.\n",
    "\n",
    "        Arguments:\n",
    "        x: (np.ndarray) Feature matrix of shape (num_of_events, num_features).\n",
    "        neighbor_pairs: (np.ndarray) Array of cell pair indices of shape (num_of_events, num_pairs, 2).\n",
    "        labels_for_neighbor_pairs: (np.ndarray) Array of labels for each pair in each event, shape (num_of_events, num_pairs).\n",
    "        batch_size: (int) The total number of samples per batch.\n",
    "        class_sampling_ratio: (dict, optional) Dictionary defining the number of samples for each class per batch, e.g., {1: 15, 2: 10, 3: 5}\n",
    "        '''\n",
    "        self.x = x\n",
    "        self.neighbor_pairs = neighbor_pairs\n",
    "        self.labels_for_neighbor_pairs = labels_for_neighbor_pairs\n",
    "        self.batch_size = batch_size\n",
    "        self.class_sampling_ratio = class_sampling_ratio\n",
    "\n",
    "        # The indices for each class is stored in a dictionary\n",
    "        self.indices = {cls: np.where(labels_for_neighbor_pairs == cls) for cls in np.unique(labels_for_neighbor_pairs)}\n",
    "        \n",
    "    def _shuffle_indices(self):\n",
    "        \"\"\"Shuffles indices and labels together, maintaining the relationship between the two\"\"\"\n",
    "        for cls in self.indices:\n",
    "            event_ids, pair_ids = self.indices[cls]\n",
    "            shuffled_event_ids, shuffled_pair_ids = randomize_data(event_ids.reshape(1, -1), pair_ids.reshape(1, -1))\n",
    "            self.indices[cls] = (shuffled_event_ids.flatten(), shuffled_pair_ids.flatten())\n",
    "\n",
    "            # Shuffling over and over, instead of in memory, give set of indices for each class and event, up to the number you need\n",
    "            # don't pass everything. Returns (after permutation) the number that you need for the class.\n",
    "\n",
    "            # Give back list of permutations -> then making batch just get the number that you want for each class\n",
    "\n",
    "    def _batch_generator(self):\n",
    "        \"\"\"Generates batches dynamically according to the class_sampling_ratio\"\"\"\n",
    "        self._shuffle_indices()\n",
    "        \n",
    "        while True:\n",
    "            batch_data = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for cls, num_samples in self.class_sampling_ratio.items():\n",
    "                event_ids, pair_ids = self.indices[cls]\n",
    "\n",
    "                # Select the required number of samples and gather the corresponding data and labels to the batch\n",
    "                selected_indices = np.random.choice(len(pair_ids), num_samples, replace=False)\n",
    "                selected_events = event_ids[selected_indices]\n",
    "                selected_pairs = pair_ids[selected_indices]\n",
    "                \n",
    "                batch_data.extend(self.x[selected_events])\n",
    "                batch_labels.extend(self.labels_for_neighbor_pairs[selected_events, selected_pairs])\n",
    "\n",
    "            # Shuffle entire batch (cross-class shuffle)\n",
    "            perm = np.random.permutation(len(batch_data))\n",
    "            batch_data = np.array(batch_data)[perm]\n",
    "            batch_labels = np.array(batch_labels)[perm]\n",
    "            \n",
    "            if len(batch_data) >= self.batch_size:\n",
    "                yield batch_data, batch_labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batch_generator())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
