{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import random  \n",
    "import numpy as np  \n",
    "\n",
    "# Third-party libraries\n",
    "from torch.utils.data import IterableDataset\n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "import uproot  \n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['analysis;1']\n"
     ]
    }
   ],
   "source": [
    "file = uproot.open('/storage/mxg1065/MyxAODAnalysis_super3D.outputs.root')\n",
    "print(file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RunNumber', 'EventNumber', 'cell_eta', 'cell_phi', 'cell_x', 'cell_y', 'cell_z', 'cell_subCalo', 'cell_sampling', 'cell_size', 'cell_hashID', 'neighbor', 'seedCell_id', 'cell_e', 'cell_noiseSigma', 'cell_SNR', 'cell_time', 'cell_weight', 'cell_truth', 'cell_truth_indices', 'cell_shared_indices', 'cell_cluster_index', 'cluster_to_cell_indices', 'cluster_to_cell_weights', 'cell_to_cluster_e', 'cell_to_cluster_eta', 'cell_to_cluster_phi', 'cluster_eta', 'cluster_phi', 'cluster_e', 'cellsNo_cluster', 'clustersNo_event', 'jetEnergyWtdTimeAve', 'jetEta', 'jetPhi', 'jetE', 'jetPt', 'jetNumberPerEvent', 'cellIndices_per_jet']\n"
     ]
    }
   ],
   "source": [
    "tree = file['analysis;1']\n",
    "branches = tree.arrays()\n",
    "print(tree.keys()) # Variables per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 events and 187652 cells\n",
    "# Arrays containing information about the energy, noise, snr, \n",
    "cell_e = np.array(branches['cell_e'])\n",
    "cell_noise = np.array(branches['cell_noiseSigma'])\n",
    "cell_snr = np.array(branches['cell_SNR'])\n",
    "cell_eta = np.array(branches['cell_eta'])\n",
    "cell_phi = np.array(branches['cell_phi'])\n",
    "\n",
    "# Represents the index of the cluster that each cell corresponds to. If the index\n",
    "# is 0, that means that the given cell does not belong to a cluster.\n",
    "cell_to_cluster_index = np.array(branches['cell_cluster_index'])\n",
    "\n",
    "# For each entry, contains the IDs of cells neighboring a given cell\n",
    "neighbor = branches['neighbor']\n",
    "\n",
    "num_of_events = len(cell_e) # 100 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02640459 0.24943821 0.09700817 0.23466232 0.5085498 ]\n",
      " [0.02567646 0.24627675 0.09700815 0.23466876 0.52418756]\n",
      " [0.02508186 0.24369499 0.09700817 0.23467347 0.5398244 ]\n",
      " ...\n",
      " [0.02632877 0.24791998 0.03177016 0.62017125 0.4921177 ]\n",
      " [0.02705116 0.2511391  0.07512318 0.6461531  0.4921177 ]\n",
      " [0.02638626 0.24820389 0.04149057 0.6731673  0.4921177 ]]\n",
      "(187652, 5)\n"
     ]
    }
   ],
   "source": [
    "# We use the data arrays to crete a data dictionary, where each entry corresponds\n",
    "# to the data of a given event; we scale this data.\n",
    "data = {}\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    data[f'data_{i}'] = np.concatenate((np.expand_dims(cell_snr[i], axis=1),\n",
    "                                        np.expand_dims(cell_e[i], axis=1),\n",
    "                                        np.expand_dims(cell_noise[i], axis=1),\n",
    "                                        np.expand_dims(cell_eta[i], axis=1),\n",
    "                                        np.expand_dims(cell_phi[i], axis=1)), axis=1)\n",
    "    \n",
    "# We combine the data into one array and apply the MinMaxScaler\n",
    "combined_data = np.vstack([data[key] for key in data])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_combined_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "# The scaled data is split to have the save structure as the original data dict\n",
    "scaled_data = {}\n",
    "start_idx = 0\n",
    "for i in range(num_of_events):\n",
    "    end_idx = start_idx + data[f\"data_{i}\"].shape[0]\n",
    "    scaled_data[f\"data_{i}\"] = scaled_combined_data[start_idx:end_idx]\n",
    "    start_idx = end_idx\n",
    "\n",
    "print(scaled_data[\"data_0\"])\n",
    "print(scaled_data[\"data_0\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepairing Neighbor Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186986 187352]\n"
     ]
    }
   ],
   "source": [
    "# The IDs of the broken cells (those with zero noise) are collected\n",
    "broken_cells = []\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    cells = np.argwhere(cell_noise[i]==0).flatten()\n",
    "    broken_cells = np.squeeze(cells)\n",
    "\n",
    "print(broken_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the values associated with neighbor[0] and neighbor[1] are all equal\n",
    "# we will just work with neighbor[0] to simplify our calculations\n",
    "neighbor = neighbor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We loop through the neighbor awkward array and remove the IDs associated\n",
    "# with the broken cells.  Loops through all cells in the neighbor list. If the loop \n",
    "# reaches the cell numbers 186986 or 187352, loop skips over these inoperative cells. \n",
    "# The final list contains tuples (i,j) where i is the cell ID in question and the \n",
    "# js are the neighboring cell IDs\n",
    "neighbor_pairs_list = []\n",
    "num_of_cells = len(neighbor) # 187652 cells\n",
    "\n",
    "for i in range(num_of_cells):\n",
    "    if i in broken_cells:\n",
    "        continue\n",
    "    for j in neighbor[i]:\n",
    "        if j in broken_cells:\n",
    "            continue\n",
    "        neighbor_pairs_list.append((i, int(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully excluded broken cells.\n"
     ]
    }
   ],
   "source": [
    "# This code checks to see if the broken cells were removed\n",
    "found_broken_cells = []\n",
    "\n",
    "for pair in neighbor_pairs_list:\n",
    "    # Loop through each cell in pair\n",
    "    for cell in pair:\n",
    "        # If the cell is broken, appends to list\n",
    "        if cell in broken_cells:\n",
    "            found_broken_cells.append(cell)\n",
    "\n",
    "if found_broken_cells:\n",
    "    print(\"Error: Broken cells are still present in neighbor pairs.\")\n",
    "else:\n",
    "    print(\"Successfully excluded broken cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 90345 119588]\n",
      " [  4388  17680]\n",
      " [ 39760  39825]\n",
      " ...\n",
      " [159757 168717]\n",
      " [ 62911  78974]\n",
      " [135353 135609]]\n",
      "(1250242, 2)\n"
     ]
    }
   ],
   "source": [
    "# These functions remove permutation variants\n",
    "def canonical_form(t):\n",
    "    return tuple(sorted(t))\n",
    "\n",
    "def remove_permutation_variants(tuple_list):\n",
    "    unique_tuples = set(canonical_form(t) for t in tuple_list)\n",
    "    return [tuple(sorted(t)) for t in unique_tuples]\n",
    "\n",
    "neighbor_pairs_list = np.array(remove_permutation_variants(neighbor_pairs_list))\n",
    "print(neighbor_pairs_list)\n",
    "print(neighbor_pairs_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Labels for the Neighbor Pairs\n",
    "\n",
    "For a given pair of cells and the IDs of the clusters that they belong to (i, j), if\n",
    "1. i=j and both are nonzero, then both cells are part of the same cluster. \n",
    "    * We call these True-True pairs and label them with 1\n",
    "2. i=j and both are zero, then both cells are not part of any cluster. \n",
    "    * We call these Lone-Lone pairs and label them with 0\n",
    "3. i is nonzero and j=0, then cell i is part of a cluster while cell j is not. \n",
    "    * We call these Cluster-Lone pairs and label them with 2\n",
    "4. i=0 and j is nonzero, then cell i is not part of a cluste while cell j is. \n",
    "    * We call these Lone-Cluster pairs and label them with 3\n",
    "5. i is not the same as j and both are nonzero, then both cells are part of different clusters. \n",
    "    * We call these Cluster-Cluster pairs and label them with 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1250242)\n",
      "[[0 0 0 ... 0 0 2]\n",
      " [3 3 0 ... 0 0 2]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 2 0 0]]\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Initialize array for labels\n",
    "labels_for_neighbor_pairs = np.zeros((num_of_events, len(neighbor_pairs_list)), dtype=int)\n",
    "\n",
    "# Extracting the individual cells within a cell pair\n",
    "cell_0 = cell_to_cluster_index[:, neighbor_pairs_list[:, 0]]\n",
    "cell_1 = cell_to_cluster_index[:, neighbor_pairs_list[:, 1]]\n",
    "\n",
    "# Computing labels using vectorized operations\n",
    "same_cluster = cell_0 == cell_1 \n",
    "both_nonzero = (cell_0 != 0) & (cell_1 != 0)\n",
    "\n",
    "# Lone-Lone (0)\n",
    "labels_for_neighbor_pairs[same_cluster & (cell_0 == 0)] = 0\n",
    "\n",
    "# True-True (1)\n",
    "labels_for_neighbor_pairs[same_cluster & (cell_0 != 0)] = 1\n",
    "\n",
    "# Cluster-Cluster (4)\n",
    "labels_for_neighbor_pairs[~same_cluster & both_nonzero] = 4\n",
    "\n",
    "# Lone-Cluster (3)\n",
    "labels_for_neighbor_pairs[~same_cluster & (cell_0 == 0) & (cell_1 != 0)] = 3\n",
    "\n",
    "# Cluster-Lone (2)\n",
    "labels_for_neighbor_pairs[~same_cluster & (cell_0 != 0) & (cell_1 == 0)] = 2\n",
    "\n",
    "print(labels_for_neighbor_pairs.shape)\n",
    "print(labels_for_neighbor_pairs)\n",
    "print(np.unique(labels_for_neighbor_pairs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Multi-Class Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassBatchGenerator(IterableDataset):\n",
    "    def __init__(self, x, neighbor_pairs, labels_for_neighbor_pairs, batch_size, class_counts):\n",
    "        \"\"\"\n",
    "        Custom IterableDataset for multi-class batch generation.\n",
    "\n",
    "        Arguments:\n",
    "        x: (np.ndarray) Feature matrix of shape (num_of_events, num_features).\n",
    "        neighbor_pairs: (np.ndarray) Array of cell pair indices, shape (num_of_events, num_pairs, 2).\n",
    "        labels_for_neighbor_pairs: (np.ndarray) Array of labels per pair, shape (num_of_events, num_pairs).\n",
    "        batch_size: (int) Total number of samples per batch.\n",
    "        class_counts: (dict) Number of samples per class per batch, {class_label: num_samples_per_batch} e.g., {1: 15, 2: 10, 3: 5}.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.neighbor_pairs = neighbor_pairs\n",
    "        self.labels_for_neighbor_pairs = labels_for_neighbor_pairs\n",
    "        self.batch_size = batch_size\n",
    "        self.class_counts = class_counts\n",
    "\n",
    "        # Store indices per class for each event\n",
    "        self.indices_per_event = self._compute_event_class_indices()\n",
    "\n",
    "    def _compute_event_class_indices(self):\n",
    "        \"\"\"Precompute indices of pairs for each event and class.\"\"\"\n",
    "        event_class_indices = {}\n",
    "        num_events = self.labels_for_neighbor_pairs.shape[0]\n",
    "\n",
    "        for event_id in range(num_events):\n",
    "            event_class_indices[event_id] = {}\n",
    "            for cls in np.unique(self.labels_for_neighbor_pairs[event_id]):\n",
    "                mask = self.labels_for_neighbor_pairs[event_id] == cls\n",
    "                event_class_indices[event_id][cls] = np.where(mask)[0]\n",
    "\n",
    "        return event_class_indices\n",
    "    '''\n",
    "    If you make the above a dict of lists, you don't need to do the event_id loop\n",
    "    '''\n",
    "\n",
    "    def _batch_generator(self):\n",
    "        \"\"\"Generator function that iterates over events and selects samples per class.\"\"\"\n",
    "        num_events = self.labels_for_neighbor_pairs.shape[0]\n",
    "\n",
    "        for event_id in range(num_events):  # Iterate over events first\n",
    "            selected_pairs = []\n",
    "            selected_labels = []\n",
    "\n",
    "            for cls, n_samples in self.class_counts.items():  # Iterate over classes\n",
    "                if cls not in self.indices_per_event[event_id]:\n",
    "                    continue  # Skip if no pairs for this class\n",
    "\n",
    "                pair_indices = self.indices_per_event[event_id][cls]\n",
    "                if len(pair_indices) < n_samples:\n",
    "                    selected_idx = pair_indices  # Take all available samples\n",
    "                else:\n",
    "                    selected_idx = np.random.choice(pair_indices, size=n_samples, replace=False)  # Sample without replacement\n",
    "\n",
    "                # Gather the selected pairs and corresponding labels\n",
    "                selected_pairs.append(self.neighbor_pairs[event_id][selected_idx])\n",
    "                selected_labels.append(np.full(len(selected_idx), cls))  # Assign class labels\n",
    "\n",
    "            # If no samples were selected, skip this event\n",
    "            if len(selected_pairs) == 0:\n",
    "                continue\n",
    "\n",
    "            # Concatenate results\n",
    "            selected_pairs = np.concatenate(selected_pairs, axis=0)\n",
    "            selected_labels = np.concatenate(selected_labels, axis=0)\n",
    "\n",
    "            # Shuffle within the event before yielding\n",
    "            perm = np.random.permutation(len(selected_labels))\n",
    "            selected_pairs = selected_pairs[perm]\n",
    "            selected_labels = selected_labels[perm]\n",
    "\n",
    "            # Yield x for this event along with (x1, x2) pairs and labels\n",
    "            yield self.x[event_id], selected_pairs, selected_labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._batch_generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 428328 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m batch_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch_generator)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Retrieve the first batch\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Unpack the batch to inspect\u001b[39;00m\n\u001b[1;32m     20\u001b[0m x_batch, neighbor_pairs_batch, labels_batch \u001b[38;5;241m=\u001b[39m batch\n",
      "Cell \u001b[0;32mIn[13], line 57\u001b[0m, in \u001b[0;36mMultiClassBatchGenerator._batch_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m         selected_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(pair_indices, size\u001b[38;5;241m=\u001b[39mn_samples, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Sample without replacement\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Gather the selected pairs and corresponding labels\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     selected_pairs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbor_pairs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mevent_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_idx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     58\u001b[0m     selected_labels\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mfull(\u001b[38;5;28mlen\u001b[39m(selected_idx), \u001b[38;5;28mcls\u001b[39m))  \u001b[38;5;66;03m# Assign class labels\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# If no samples were selected, skip this event\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 428328 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "# Define the parameters for instantiating the generator\n",
    "batch_size = 1\n",
    "class_counts = {0: 40000, 1: 40000, 2: 40000, 3: 40000, 4: 3000}\n",
    "num_features = scaled_combined_data.shape[1]\n",
    "\n",
    "# Create the generator instance\n",
    "batch_generator = MultiClassBatchGenerator(\n",
    "    x=scaled_combined_data, \n",
    "    neighbor_pairs=neighbor_pairs_list, \n",
    "    labels_for_neighbor_pairs=labels_for_neighbor_pairs, \n",
    "    batch_size=batch_size, \n",
    "    class_counts=class_counts\n",
    ")\n",
    "\n",
    "# Explicitely get an iterator from the dataset\n",
    "batch_iterator = iter(batch_generator)\n",
    "# Retrieve the first batch\n",
    "batch = next(batch_iterator)\n",
    "\n",
    "# Unpack the batch to inspect\n",
    "x_batch, neighbor_pairs_batch, labels_batch = batch\n",
    "print(f\"x_batch shape: {x_batch.shape}\")\n",
    "print(f\"neighbor_pairs_batch shape: {neighbor_pairs_batch.shape}\")\n",
    "print(f\"labels_batch shape: {labels_batch.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "intantiate it, then call next, it will get a batch, validate by eye, then go through it and perform checks to make sure everything makes sense\n",
    "* especially check to see that the pairs and labels are not being messed up, that the labels and pairs are correctly associated with one another\n",
    "\n",
    "Can run a generator that takes another generator as an argument, takes one event to multiple events (concatenation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
