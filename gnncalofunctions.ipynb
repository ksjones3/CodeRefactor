{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407a0075-1c5f-4028-91a2-a089201ccd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split\n",
    "from collections import namedtuple, defaultdict\n",
    "import random\n",
    "random.seed(42)\n",
    "import h5py\n",
    "import pickle\n",
    "import blosc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import joblib # efficiently serializes python objects and is used in saving and loading machine learning models\n",
    "import json # encode/decode JSON data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61e3ca-46a0-4950-a0f9-44d74ea6ca76",
   "metadata": {},
   "source": [
    "## Methods for the pairing datapoints notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcd7c917-159d-4720-845e-f4714b4c0b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def branches_from_root_file(filename):\n",
    "    '''\n",
    "    Returns the branches from a root file\n",
    "    '''\n",
    "    file = uproot.open(filename)\n",
    "    tree = file[file.keys()[0]]\n",
    "    branches = tree.arrays()\n",
    "    return branches\n",
    "\n",
    "# Original Code:\n",
    "# file = uproot.open(\"MyxAODAnalysis_all2D.outputs.root\")\n",
    "# file.keys()\n",
    "# tree = file['analysis']\n",
    "# branches = tree.arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88079dc6-da2c-4564-a867-1f1fdc13cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dict(rangeValue, variables, dataDict):\n",
    "    for i in range(rangeValue):\n",
    "        dataDict[f\"data_{i}\"] = np.concatenate([np.expand_dims(var[i], axis=1) for var in variables], axis=1)\n",
    "    return dataDict\n",
    "    \n",
    "# Original Code:\n",
    "# data = {}\n",
    "# for i in range(generalized_range):\n",
    "#     data[f\"data_{i}\"] = np.concatenate((np.expand_dims(cell_coordinate_x[i], axis=1), np.expand_dims(cell_coordinate_y[i], axis=1), np.expand_dims(cell_coordinate_z[i], axis=1),\n",
    "#                         np.expand_dims(cell_eta[i], axis=1), np.expand_dims(cell_phi[i], axis=1),\n",
    "#                         np.expand_dims(cell_sampling[i], axis=1),\n",
    "#                         np.expand_dims(cell_noiseSigma[i], axis=1),\n",
    "#                         np.expand_dims(cell_e[i], axis=1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a01178d-6b6f-4dc7-8cf5-e6e2a5f45e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_hdf5(dic, filename):\n",
    "    \"\"\"Save a dictionary to an HDF5 file\"\"\"\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        _save_dict_to_hdf5(f, dic)\n",
    "# Opens hdf5 file in write mode. Calls a helper function\n",
    "# to recursively save the dictionary to the hdf5 file.\n",
    "\n",
    "def _save_dict_to_hdf5(group, dic):\n",
    "    \"\"\"Save a dictionary to an HDF5 group\"\"\"\n",
    "    for key, value in dic.items():\n",
    "        if isinstance(value, dict):\n",
    "            subgroup = group.create_group(key)\n",
    "            _save_dict_to_hdf5(subgroup, value)\n",
    "        else:\n",
    "            if isinstance(value, list):\n",
    "                \"\"\"Convert list to numpy array before saving\"\"\"\n",
    "                value = np.array(value)\n",
    "            group[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d06ee63b-c08c-4f64-ba5d-18d63b64faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A code to remove permutation variant\n",
    "def canonical_form(t):\n",
    "    \"\"\"Sorts elements of the tuple and converts the sorted list back into a tuple.\"\"\"\n",
    "    return tuple(sorted(t))\n",
    "\n",
    "def remove_permutation_variants(tuple_list):\n",
    "    \"\"\"\n",
    "    Creates a set of unique tuples by converting each tuple to its canonical form.\n",
    "    Remove permutation variants from a list of tuples.\n",
    "    Converts set back into a list of tuples.\n",
    "    \"\"\"\n",
    "    unique_tuples = set(canonical_form(t) for t in tuple_list)\n",
    "    return [tuple(sorted(t)) for t in unique_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f257bd8-f822-48bd-b49d-6bef1d3946ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = cell_to_cluster_index[i]\n",
    "# i = pair[0]=z[0], j = pair[1]=z[1]\n",
    "def cluster_cluster_true(y,i,j):\n",
    "    return y[i]==y[j] and y[i]!=0\n",
    "def lone_lone(y,i,j):\n",
    "    return y[i]==y[j] and y[i]==0\n",
    "def cluster_cluster_false(y,i,j):\n",
    "    return y[i]!=y[j] and y[i]!=0 and y[j]!=0\n",
    "def cluster_lone(y,i,j):\n",
    "    return y[i]!=y[j] and y[i]!=0 and y[j]==0\n",
    "def lone_cluster(y,i,j):\n",
    "    return y[i]!=y[j] and y[i]==0 and y[j]!=0\n",
    "\n",
    "# x = neighbor_pairs_unique_sorted\n",
    "# y = cell_to_cluster_index[i]\n",
    "# z = pair\n",
    "def assign_index(mapping, x, y):\n",
    "    out = []\n",
    "    for pair in x:\n",
    "        for index, test in mapping.items():\n",
    "            if test(y,z[0],z[1]):\n",
    "                out.append(index)\n",
    "                continue\n",
    "    return out\n",
    "\n",
    "def neighbor_pairs_mapping(ll_index, cl_index, lc_index, ccF_index):\n",
    "    '''\n",
    "    Set the class value for the background types (integer excluding 1) for the cases \n",
    "    where the neighbor pairs both lone cells (ll), one from a cluster and the other \n",
    "    a lone cell (cl or lc), or both are from differnt clusters (ccF)\n",
    "    '''\n",
    "    pairs_mapping = {1: cluster_cluster_true, ll_index: lone_lone, cl_index: cluster_lone, lc_index: lone_cluster, ccF_index: cluster_cluster_false}\n",
    "    return pairs_mapping\n",
    "\n",
    "# list_of_pair_indices=neighbor_pairs_unique_sorted\n",
    "# index of cluster cell is a part of =cell_to_cluster_index\n",
    "def label_neighbor_pairs(range_value, cell_to_cluster_index, list_of_pair_indices, mapping):\n",
    "    neighbor_labels = []\n",
    "    for i in range(range_value):\n",
    "        assign_index(mapping=mapping, x=list_of_pair_indices, y=cell_cluster_index[i])      \n",
    "    return np.array(neighbor_labels.append(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb2ba7-f130-4f44-8d5a-59b8ce6bce98",
   "metadata": {},
   "source": [
    "## Methods for the data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62cbd66-96e6-49d9-82e4-0754869008fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf5_to_dict(hdf5_file):\n",
    "    \"\"\"\n",
    "    Convert HDF5 file to Python dictionary\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    _hdf5_to_dict(hdf5_file, data_dict)\n",
    "    return data_dict\n",
    "# Initializes an empty dictionary and calls a function to recursively\n",
    "# fill this dictionary with data from the hdf5 file.\n",
    "\n",
    "\n",
    "def _hdf5_to_dict(group, dic):\n",
    "    \"\"\"\n",
    "    Convert HDF5 group to dictionary recursively\n",
    "    \"\"\"\n",
    "    for key, item in group.items():\n",
    "        if isinstance(item, h5py.Group):\n",
    "            subgroup = {}\n",
    "            _hdf5_to_dict(item, subgroup)\n",
    "            dic[key] = subgroup\n",
    "        else:\n",
    "            dic[key] = np.array(item)\n",
    "# Iterates over items in the hdf5 group. If the item is a group, \n",
    "# it creates a new dictionary and calls itself recursively. If the item\n",
    "# is a dataset, it converts it to a numpy array and stores it in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10bd6d64-b0f8-40d0-aebe-2021325adee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_of_edges(range_value, true_list, bkg_0, bkg_2, bkg_3):\n",
    "    '''\n",
    "    bkg_0 represents the background with a truth value of 0\n",
    "    bkg_2/3 represents the background with a truth value of 2/3\n",
    "    '''\n",
    "    for i in range(range_value):\n",
    "        true_list.append(list(np.where(neigbor_truth_100evs[i]==1)[0]))\n",
    "        bkg_0.append(list(np.where(neigbor_truth_100evs[i]==0)[0]))\n",
    "        bkg_2.append(list(np.where(neigbor_truth_100evs[i]==2)[0]))\n",
    "        bkg_3.append(list(np.where(neigbor_truth_100evs[i]==3)[0]))\n",
    "    return np.array(true_list), np.array(bkg_0), np.array(bkg_2), np.array(bkg_3)\n",
    "\n",
    "def pairs_of_edges(range_value, true_list, bkg_0, bkg_2, bkg_3):\n",
    "    '''\n",
    "    bkg_0 represents the background with a truth value of 0\n",
    "    bkg_2/3 represents the background with a truth value of 2/3\n",
    "    '''\n",
    "    for i in range(range_value):\n",
    "        true_list.append(len(true[i]))\n",
    "        bkg_0.append(len(bkg_0[i]))\n",
    "        bkg_2.append(len(bkg_2[i]))\n",
    "        bkg_3.append(len(bkg_3[i]))\n",
    "    return np.array(true_list), np.array(bkg_0), np.array(bkg_2), np.array(bkg_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646d1587-e006-4655-ba5f-675793a867dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_indices(pair_number_list, true_list, bkg_0, bkg_2, bkg_3):\n",
    "    sorted_indices = np.argsort(-pair_number_list)\n",
    "    return true_list[sorted_indices], bkg_0[sorted_indices], bkg_2[sorted_indices], bkg_3[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db713174-5683-4c5c-be7e-7cd6617dc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_set_test_set(split_num, true_list, bkg_0, bkg_2, bkg_3):\n",
    "    return true_list[:split_num],bkg_0[:split_num],bkg_2[:split_num],bkg_3[:split_num], true_list[split_num:],bkg_0[split_num:],bkg_2[split_num:],bkg_3[split_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e52e36e7-f17f-4df9-b568-6906e7d0c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_of_features(dynamic_variables, split_num, sorted_indices):\n",
    "    keys = list(dynamic_variables.keys())\n",
    "    values = list(dynamic_variables.values())\n",
    "    #Gets the keys and variables from the dynamic variables array and stores them in arrays.\n",
    "    \n",
    "    rearranged_values = [values[i] for i in sorted_indices]\n",
    "    #Sorts the values with the sorted cluster indices.\n",
    "    \n",
    "    rearranged_dict = dict(zip(keys, rearranged_values))\n",
    "    #Creates a rearranged dictionary out of the keys and sorted values.\n",
    "    \n",
    "    data_train = np.concatenate([value for key, value in list(rearranged_dict.items())[:split_num]])\n",
    "    #Create a training data array.\n",
    "    \n",
    "    data_test = np.concatenate([value for key, value in list(dynamic_variables.items())[split_num:]])\n",
    "    #Create a data testing array.\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    cellFeatures_trainS = scaler.fit_transform(data_train)\n",
    "    scaler_filename = \"./bscaler_neighbor_data_train_sorted.save\"\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    #Scales the training data with a minmaxscaler, put that scaled data into a training features array, and then save that scaler into a .save file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2f6a68-0482-4865-85cc-1909466facba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDataH5(fileName, dataDict, compressionType):\n",
    "    with h5py.File(fileName, 'w') as file:\n",
    "        for key, data in dataDict.items():\n",
    "            file.create_dataset(key, data=data, compression=compressionType)\n",
    "\n",
    "def loadDataH5(fileName):\n",
    "    dataDict = {}\n",
    "    with h5py.File(fileName, 'r') as file:\n",
    "        for key in file.keys():\n",
    "            dataDict[key] = np.array(file[key])\n",
    "    return dataDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59afa43a-4e08-471c-92df-17f6ffa6a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gnncalofunctions notebook has been successfully run\n"
     ]
    }
   ],
   "source": [
    "print(\"The gnncalofunctions notebook has been successfully run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
