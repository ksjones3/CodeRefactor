{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1645cd-246a-4043-93a8-b005583d6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "random.seed(42)\n",
    "import uproot\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import joblib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import BatchNorm1d\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3080b8a-88af-4979-9863-127839b23505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def branches_from_root_file(filename):\n",
    "    '''\n",
    "    Returns the branches from a root file\n",
    "    '''\n",
    "    file = uproot.open(filename)\n",
    "    tree = file[file.keys()[0]]\n",
    "    branches = tree.arrays()\n",
    "    return branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b96d0a-714b-4fd8-96ea-41b46bfd063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(branches, dataName):\n",
    "    return np.array(branches[dataName])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be119dfd-146a-414f-800d-6c13837579f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dict(rangeValue, variables, dataDict):\n",
    "    for i in range(rangeValue):\n",
    "        dataDict[f\"data_{i}\"] = np.concatenate([np.expand_dims(var[i], axis=1) for var in variables], axis=1)\n",
    "    return dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a8d257-6584-4e19-b88e-20c346ec27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_hdf5(dic, filename):\n",
    "    \"\"\"Save a dictionary to an HDF5 file\"\"\"\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        _save_dict_to_hdf5(f, dic)\n",
    "\n",
    "\n",
    "def _save_dict_to_hdf5(group, dic):\n",
    "    \"\"\"Save a dictionary to an HDF5 group\"\"\"\n",
    "    for key, value in dic.items():\n",
    "        if isinstance(value, dict):\n",
    "            subgroup = group.create_group(key)\n",
    "            _save_dict_to_hdf5(subgroup, value)\n",
    "        else:\n",
    "            if isinstance(value, list):\n",
    "                \"\"\"Convert list to numpy array before saving\"\"\"\n",
    "                value = np.array(value)\n",
    "            group[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9723967e-33d2-46cb-8325-5367df59ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A code to remove permutation variant\n",
    "def canonical_form(t):\n",
    "    \"\"\"Sorts elements of the tuple and converts the sorted list back into a tuple.\"\"\"\n",
    "    return tuple(sorted(t))\n",
    "\n",
    "def remove_permutation_variants(tuple_list):\n",
    "    \"\"\"\n",
    "    Creates a set of unique tuples by converting each tuple to its canonical form.\n",
    "    Remove permutation variants from a list of tuples.\n",
    "    Converts set back into a list of tuples.\n",
    "    \"\"\"\n",
    "    unique_tuples = set(canonical_form(t) for t in tuple_list)\n",
    "    return [tuple(sorted(t)) for t in unique_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6169262-f663-4a0d-9ed1-13efb123404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = cell_to_cluster_index[i]\n",
    "# i = pair[0]=z[0], j = pair[1]=z[1]\n",
    "def cluster_cluster_true(y,i,j):\n",
    "    return y[i]==y[j] and y[i]!=0\n",
    "def lone_lone(y,i,j):\n",
    "    return y[i]==y[j] and y[i]==0\n",
    "def cluster_cluster_false(y,i,j):\n",
    "    return y[i]!=y[j] and y[i]!=0 and y[j]!=0\n",
    "def cluster_lone(y,i,j):\n",
    "    return y[i]!=y[j] and y[i]!=0 and y[j]==0\n",
    "def lone_cluster(y,i,j):\n",
    "    return y[i]!=y[j] and y[i]==0 and y[j]!=0\n",
    "\n",
    "# x = neighbor_pairs_unique_sorted\n",
    "# y = cell_to_cluster_index[i]\n",
    "# z = pair\n",
    "def assign_index(mapping, x, y):\n",
    "    out = []\n",
    "    for pair in x:\n",
    "        for index, test in mapping.items():\n",
    "            if test(y,pair[0],pair[1]):\n",
    "                out.append(index)\n",
    "                continue\n",
    "    return out\n",
    "\n",
    "def neighbor_pairs_mapping(loneloneIndex, clusterloneIndex, \n",
    "                           loneclusterIndex, clusterclusterFalseIndex):\n",
    "    '''\n",
    "    Set the class value for the background types (integer excluding 1) for the cases \n",
    "    where the neighbor pairs both lone cells, one from a cluster and the other \n",
    "    a lone cell (and vice versa), or both are from differnt clusters\n",
    "    '''\n",
    "    pairs_mapping = {1: cluster_cluster_true, loneloneIndex: lone_lone, \n",
    "                     clusterloneIndex: cluster_lone, loneclusterIndex: lone_cluster, \n",
    "                     clusterclusterFalseIndex: cluster_cluster_false}\n",
    "    return pairs_mapping\n",
    "\n",
    "# list_of_pair_indices=neighbor_pairs_unique_sorted\n",
    "# index of cluster cell is a part of =cell_to_cluster_index\n",
    "def label_neighbor_pairs(range_value, cell_to_cluster_index, list_of_pair_indices, mapping):\n",
    "    neighbor_labels = []\n",
    "    for i in range(range_value):\n",
    "        neighbor_labels.append(assign_index(mapping=mapping, x=list_of_pair_indices, y=cell_to_cluster_index[i]))    \n",
    "    return np.array(neighbor_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7eac67-2ed1-4a2c-a9ac-cbc8bfbf597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf5_to_dict(hdf5_file):\n",
    "    \"\"\"\n",
    "    Convert HDF5 file to Python dictionary\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    _hdf5_to_dict(hdf5_file, data_dict)\n",
    "    return data_dict\n",
    "# Initializes an empty dictionary and calls a function to recursively\n",
    "# fill this dictionary with data from the hdf5 file.\n",
    "\n",
    "\n",
    "def _hdf5_to_dict(group, dic):\n",
    "    \"\"\"\n",
    "    Convert HDF5 group to dictionary recursively\n",
    "    \"\"\"\n",
    "    for key, item in group.items():\n",
    "        if isinstance(item, h5py.Group):\n",
    "            subgroup = {}\n",
    "            _hdf5_to_dict(item, subgroup)\n",
    "            dic[key] = subgroup\n",
    "        else:\n",
    "            dic[key] = np.array(item)\n",
    "# Iterates over items in the hdf5 group. If the item is a group, \n",
    "# it creates a new dictionary and calls itself recursively. If the item\n",
    "# is a dataset, it converts it to a numpy array and stores it in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60604dbb-67d9-48b6-a0ae-41aa0a717caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeEdges(neighbor_truth, label):\n",
    "    pair = np.where(neighbor_truth==label)\n",
    "    return list(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88ba87dc-5004-4da1-aefb-7d4ed3bbd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_of_edges(range_value, neighbor_truth):\n",
    "    '''\n",
    "    bkg_0 represents the background with a truth value of 0\n",
    "    bkg_2/3/4 represents the background with a truth value of 2/3/4\n",
    "    '''\n",
    "    true_list = []\n",
    "    bkg_0 = []\n",
    "    bkg_2 = []\n",
    "    bkg_3 = []\n",
    "    bkg_4 = []\n",
    "    for i in range(range_value):\n",
    "        # y = cell_to_cluster_index[i]\n",
    "        neighbor_truth_element = neighbor_truth[i]\n",
    "        true_list.append(makeEdges(neighbor_truth_element, 1))\n",
    "        bkg_0.append(makeEdges(neighbor_truth_element, 0))\n",
    "        bkg_2.append(makeEdges(neighbor_truth_element, 2))\n",
    "        bkg_3.append(makeEdges(neighbor_truth_element, 3))\n",
    "        bkg_4.append(makeEdges(neighbor_truth_element, 4))\n",
    "    return true_list, bkg_0, bkg_2, bkg_3, bkg_4\n",
    "\n",
    "def pairs_of_edges(range_value, true_list, bkg_0_list, bkg_2_list, bkg_3_list, bkg_4_list):\n",
    "    '''\n",
    "    bkg_0 represents the background with a truth value of 0\n",
    "    bkg_2/3 represents the background with a truth value of 2/3\n",
    "    '''\n",
    "    true_pairNumber = []\n",
    "    bkg_0_pairNumber = []\n",
    "    bkg_2_pairNumber = []\n",
    "    bkg_3_pairNumber = []\n",
    "    bkg_4_pairNumber = []\n",
    "    for i in range(range_value):\n",
    "        true_pairNumber.append(len(true_list[i]))\n",
    "        bkg_0_pairNumber.append(len(bkg_0_list[i]))\n",
    "        bkg_2_pairNumber.append(len(bkg_2_list[i]))\n",
    "        bkg_3_pairNumber.append(len(bkg_3_list[i]))\n",
    "        bkg_4_pairNumber.append(len(bkg_4_list[i]))\n",
    "    return np.array(true_pairNumber), np.array(bkg_0_pairNumber), np.array(bkg_2_pairNumber), np.array(bkg_3_pairNumber), np.array(bkg_4_pairNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f738152-9d64-4228-b750-ad6f259f92f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_indices(pair_number_list, true_list, bkg_0, bkg_2, bkg_3, bkg_4):\n",
    "    sorted_indices = np.argsort(-pair_number_list)\n",
    "    return true_list[sorted_indices], bkg_0[sorted_indices], bkg_2[sorted_indices], bkg_3[sorted_indices], bkg_4[sorted_indices], sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e7d7e55-f34f-4496-bd59-ec350a4a7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortList(data, sorted_indices):\n",
    "    return [data[i] for i in sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8ef6a21-5a9b-482d-af9e-10dc6af0d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_set_test_set(split_num, true_list, bkg_0, bkg_2, bkg_3, bkg_4):\n",
    "    return true_list[:split_num],bkg_0[:split_num],bkg_2[:split_num],bkg_3[:split_num], bkg_4[:split_num], true_list[split_num:],bkg_0[split_num:],bkg_2[split_num:],bkg_3[split_num:], bkg_4[split_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "776e9d30-a5dd-4713-8f0d-e7e7c6e17032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_of_features(dynamic_variables, split_num, sorted_indices, scaler_fileName):\n",
    "    keys = list(dynamic_variables.keys())\n",
    "    values = list(dynamic_variables.values())\n",
    "    #Gets the keys and variables from the dynamic variables array and stores them in arrays.\n",
    "    \n",
    "    rearranged_values = [values[i] for i in sorted_indices]\n",
    "    #Sorts the values with the sorted cluster indices.\n",
    "    \n",
    "    rearranged_dict = dict(zip(keys, rearranged_values))\n",
    "    #Creates a rearranged dictionary out of the keys and sorted values.\n",
    "    \n",
    "    data_train = np.concatenate([value for key, value in list(rearranged_dict.items())[:split_num]])\n",
    "    #Create a training data array.\n",
    "    \n",
    "    data_test = np.concatenate([value for key, value in list(dynamic_variables.items())[split_num:]])\n",
    "    #Create a data testing array.\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    cellFeatures_trainS = scaler.fit_transform(data_train)\n",
    "    cell_features_testS = scaler.transform(data_test)\n",
    "    scaler_filename = \"./bscaler_neighbor_data_train_sorted.save\"\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    return cellFeatures_trainS, cell_features_testS\n",
    "    #Scales the training data with a minmaxscaler, put that scaled data into a training features array, and then save that scaler into a .save file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa75fbfe-8d45-4288-ac6a-00461109643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapeFeatures(splitNum, features):\n",
    "    return features.reshape(splitNum, int(features.shape[0]/splitNum), int(features.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68856234-35c1-4b59-97a2-fd88d9ec0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDataH5(fileName, dataDict, compressionType):\n",
    "    with h5py.File(fileName, 'w') as file:\n",
    "        for key, data in dataDict.items():\n",
    "            file.create_dataset(key, data=data, compression=compressionType)\n",
    "\n",
    "def loadDataH5(fileName):\n",
    "    dataDict = {}\n",
    "    with h5py.File(fileName, 'r') as file:\n",
    "        for key in file.keys():\n",
    "            dataDict[key] = np.array(file[key])\n",
    "    return dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf8177-ed01-4149-8874-775df10e9b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e3ab5f8-fb58-45fc-a9da-06cd2a84b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBrokenCells(cell_noiseSigma, neighbor):\n",
    "    broken_cells = getBrokenCells(cell_noiseSigma)\n",
    "    return getNeighborPairs(broken_cells, neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7de949a4-aa19-4657-b4a7-4dba96bc732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBrokenCells(cell_noiseSigma):\n",
    "    broken_cell_indices = np.argwhere(cell_noiseSigma[0] == 0)\n",
    "    broken_cells = []\n",
    "    for arrays in broken_cell_indices:\n",
    "        for index in broken_cell_indices:\n",
    "            broken_cells.append(index)\n",
    "    return broken_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49845c41-a868-4cd0-86b5-21e9a02e96b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeighborPairs(broken_cells, neighbor):\n",
    "    neighbor_pairs_set = []\n",
    "    for i in range(len(neighbor)):\n",
    "        if i in broken_cells:\n",
    "            continue\n",
    "        for cell in neighbor[i]:\n",
    "            if cell in broken_cells:\n",
    "                continue\n",
    "            neighbor_pairs_set.append(((i, cell)))\n",
    "    return neighbor_pairs_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fec2d60d-a659-4fd7-b110-3875d906d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeH5File(fileName, datasetName, data):\n",
    "    with h5py.File(fileName, \"w\") as f:\n",
    "        dset = f.create_dataset(datasetName, data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "533db341-9c5d-4979-a44b-d987da748ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readH5File(fileName, datasetName):\n",
    "    file = h5py.File(fileName, \"r\")\n",
    "    data = file.get(datasetName)[:]\n",
    "    file.close()\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0740953c-5aeb-4be9-889a-4e53ad00d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleDataTraining(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster):\n",
    "    true_sample_size, bkg_sample_size = getTrainingSampleSizes(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster)\n",
    "    return sampleData(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster, true_sample_size, bkg_sample_size, bkg_sample_size, bkg_sample_size, bkg_sample_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "367745a2-4a8e-41eb-96bf-13ab271505d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleDataTesting(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster):\n",
    "    true_sample_size = getTestingSampleSize(true)\n",
    "    bkg_lone_sample_size = getTestingSampleSize(bkg_lone)\n",
    "    bkg_cluster_lone_sample_size = getTestingSampleSize(bkg_cluster_lone)\n",
    "    bkg_lone_cluster_sample_size = getTestingSampleSize(bkg_lone_cluster)\n",
    "    bkg_cluster_cluster_sample_size = getTestingSampleSize(bkg_cluster_cluster)\n",
    "    return sampleData(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster, true_sample_size, bkg_lone_sample_size, bkg_cluster_lone_sample_size, bkg_lone_cluster_sample_size, bkg_cluster_cluster_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c0bf805-458d-4420-8089-df229844f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestingSampleSize(data):\n",
    "    minimum = getMinimum(data)\n",
    "    sample_size = minimum - (minimum % 100)\n",
    "    return sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7040dd54-00b2-4336-8407-91d6d62e6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinimum(data):\n",
    "    return min([len(row) for row in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "681f0cc8-efe7-494f-9970-9c4072c5cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBackgroundMin(bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster):\n",
    "    bkg_lone_min = getMinimum(bkg_lone)\n",
    "    bkg_cluster_lone_min = getMinimum(bkg_cluster_lone)\n",
    "    bkg_lone_cluster_min = getMinimum(bkg_lone_cluster)\n",
    "    bkg_cluster_cluster_min = getMinimum(bkg_cluster_cluster)\n",
    "    bkg_min = min([bkg_lone_min, bkg_cluster_lone_min, bkg_lone_cluster_min, bkg_cluster_cluster_min])\n",
    "    return bkg_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4122db0-3377-417d-9b41-46a8eb045653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingSampleSizes(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster):\n",
    "    true_min = getMinimum(true)\n",
    "    bkg_min = getBackgroundMin(bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster)\n",
    "    bkg_sample_size = bkg_min - (bkg_min % 100)\n",
    "    true_sample_size = bkg_sample_size*4\n",
    "    if true_sample_size > true_min:\n",
    "        true_sample_size = true_min - (true_min % 100)\n",
    "        true_sample_size = true_sample_size - (true_sample_size % 4)\n",
    "        bkg_sample_size = true_sample_size/4\n",
    "    return true_sample_size, bkg_sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "837e7c39-5f0b-49e8-ba25-76c3e321dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleData(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster, true_sample_size, bkg_lone_sample_size, bkg_cluster_lone_sample_size, bkg_lone_cluster_sample_size, bkg_cluster_cluster_sample_size):\n",
    "    true_sample = sampleDataset(true, true_sample_size)\n",
    "    bkg_lone_sample = sampleDataset(bkg_lone, bkg_lone_sample_size)\n",
    "    bkg_cluster_lone_sample = sampleDataset(bkg_cluster_lone, bkg_cluster_lone_sample_size)\n",
    "    bkg_lone_cluster_sample = sampleDataset(bkg_lone_cluster, bkg_lone_cluster_sample_size)\n",
    "    bkg_cluster_cluster_sample = sampleDataset(bkg_cluster_cluster, bkg_cluster_cluster_sample_size)\n",
    "    return np.array(true_sample), np.array(bkg_lone_sample), np.array(bkg_cluster_lone_sample), np.array(bkg_lone_cluster_sample), np.array(bkg_cluster_cluster_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb020edd-c5a3-4507-a31a-0d111a00c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleDataset(data, data_sample_size):\n",
    "    return [random.sample(row, data_sample_size) for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3de1b0cd-9938-42fd-8e9c-3d72ed3ed6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRandomIndices(total_indices_shape):\n",
    "    rand_index = []\n",
    "    for i in range(total_indices_shape[0]):\n",
    "        arr = np.arange(total_indices_shape[1])\n",
    "        np.random.shuffle(arr)\n",
    "        rand_index.append(arr)\n",
    "    return np.array(rand_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "832cf9b5-0483-4325-a79c-98b1a5f5b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize2DArray(rand_indices, unrandomized_array):\n",
    "    randomized_list = []\n",
    "    for i in range(unrandomized_array.shape[0]):\n",
    "        randomized_list.append(unrandomized_array[i][rand_indices[i]])\n",
    "    return np.array(randomized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "392f3ace-e87f-4a24-a4df-9021d115e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomizeEdges(rand_indices, unrandomized_edges):\n",
    "    randomized_list = []\n",
    "    for i in range(rand_indices.shape[0]):\n",
    "        randomized_list.append(unrandomized_edges[rand_indices[i]])\n",
    "    return np.array(randomized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f41cbaa4-0985-460b-a4dc-a65a6f504503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEdgeArrays(inputData):\n",
    "    source_BD = []\n",
    "    dest_BD = []\n",
    "    source_noBD = []\n",
    "    dest_noBD = []\n",
    "    for i in range(inputData.shape[0]):\n",
    "        source_BD_element, dest_BD_element, source_noBD_element, dest_noBD_element = createBDAndNoBDArrays(inputData[i])\n",
    "\n",
    "        source_BD.append(source_BD_element)\n",
    "        dest_BD.append(dest_BD_element)\n",
    "        source_noBD.append(source_noBD_element)\n",
    "        dest_noBD.append(dest_noBD_element)\n",
    "    return np.array(source_BD), np.array(dest_BD), np.array(source_noBD), np.array(dest_noBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6be203e8-0099-4f8c-badf-ef4b3427908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBDAndNoBDArrays(inputData):\n",
    "    source_BD = []\n",
    "    dest_BD = []\n",
    "    source_noBD = []\n",
    "    dest_noBD = []\n",
    "\n",
    "    for pair in inputData:\n",
    "\n",
    "        source_BD.append(pair[0])\n",
    "        source_BD.append(pair[1])\n",
    "        \n",
    "        dest_BD.append(pair[1])\n",
    "        dest_BD.append(pair[0])\n",
    "\n",
    "        source_noBD.append(pair[0])\n",
    "        dest_noBD.append(pair[1])\n",
    "        \n",
    "    return source_BD, dest_BD, source_noBD, dest_noBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f76299-92a4-4643-ac8a-4ef7e9b9a6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d999eb8-a495-4e13-b294-cd4b41d4f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3c0097e-6885-443d-9c71-b2d7d14d91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data_list):\n",
    "    batch_x = [data.x for data in data_list]\n",
    "    batch_edge_index = [data.edge_index for data in data_list]\n",
    "    batch_edge_index_out = [data.edge_index_out for data in data_list]\n",
    "    batch_y = [data.y for data in data_list]\n",
    "\n",
    "    return batch_x, batch_edge_index, batch_edge_index_out, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be620c83-85a1-4943-bb59-5b952b4857f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_true(data_list_true):\n",
    "    batch_x = [data.x for data in data_list_true]\n",
    "    batch_edge_index = [data.edge_index for data in data_list_true]\n",
    "    batch_edge_index_out = [data.edge_index_out for data in data_list_true]\n",
    "    batch_y = [data.y for data in data_list_true]\n",
    "\n",
    "    return batch_x, batch_edge_index, batch_edge_index_out, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "778796da-bb15-4609-aa3e-a636371d05cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_bkg_lone(data_list_bkg_lone):\n",
    "    batch_x = [data.x for data in data_list_bkg_lone]\n",
    "    batch_edge_index = [data.edge_index for data in data_list_bkg_lone]\n",
    "    batch_edge_index_out = [data.edge_index_out for data in data_list_bkg_lone]\n",
    "    batch_y = [data.y for data in data_list_bkg_lone]\n",
    "\n",
    "    return batch_x, batch_edge_index, batch_edge_index_out, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9561ab0b-ccf7-4906-aee0-7eabeee0d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_bkg_cluster_lone(data_list_bkg_cluster_lone):\n",
    "    batch_x = [data.x for data in data_list_bkg_cluster_lone]\n",
    "    batch_edge_index = [data.edge_index for data in data_list_bkg_cluster_lone]\n",
    "    batch_edge_index_out = [data.edge_index_out for data in data_list_bkg_cluster_lone]\n",
    "    batch_y = [data.y for data in data_list_bkg_cluster_lone]\n",
    "\n",
    "    return batch_x, batch_edge_index, batch_edge_index_out, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80f1318c-5065-4350-8f60-c9a65e3b95c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_bkg_cluster_cluster(data_list_bkg_cluster_cluster):\n",
    "    batch_x = [data.x for data in data_list_bkg_cluster_cluster]\n",
    "    batch_edge_index = [data.edge_index for data in data_list_bkg_cluster_cluster]\n",
    "    batch_edge_index_out = [data.edge_index_out for data in data_list_bkg_cluster_cluster]\n",
    "    batch_y = [data.y for data in data_list_bkg_cluster_cluster]\n",
    "\n",
    "    return batch_x, batch_edge_index, batch_edge_index_out, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bb68dea-c165-4485-8a11-121fc4d47c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_bkg_total(data_list_total_bkg):\n",
    "    batch_x = [data.x for data in data_list_total_bkg]\n",
    "    batch_edge_index = [data.edge_index for data in data_list_total_bkg]\n",
    "    batch_edge_index_out = [data.edge_index_out for data in data_list_total_bkg]\n",
    "    batch_y = [data.y for data in data_list_total_bkg]\n",
    "\n",
    "    return batch_x, batch_edge_index, batch_edge_index_out, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7130fe26-5c76-4107-89e1-3b6aa348fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(EdgeClassifier, self).__init__()\n",
    "\n",
    "        # Node embedding layer\n",
    "        self.node_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(hidden_dim, 128)\n",
    "        self.bn1 = BatchNorm1d(128)\n",
    "        \n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "        self.bn2 = BatchNorm1d(64)\n",
    "        \n",
    "        # Edge classification layer\n",
    "        self.fc = nn.Linear(128 , output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_index_out):\n",
    "        edge_index = edge_index\n",
    "        x = self.node_embedding(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Edge representations\n",
    "        edge_index_to_compare = edge_index_out\n",
    "        edge_rep = torch.cat([x[edge_index_to_compare[0]], x[edge_index_to_compare[1]]], dim=1)\n",
    "\n",
    "        # Edge classification\n",
    "        edge_scores = torch.sigmoid(self.fc(edge_rep))\n",
    "\n",
    "        return edge_scores\n",
    "#Defines an edge classifier convolutional GNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "270f1242-a593-4fcf-b109-fa7f05637897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch < 10:\n",
    "        return 1.0  # No change for the first 10 epochs\n",
    "    else:\n",
    "        return 0.1  # Decrease learning rate by a factor of 10 after 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a22e3af7-e3ed-47cf-bfd2-270426dccd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    #model = nn.parallel.DistributedDataParallel(model)\n",
    "    model.to(device)\n",
    "    #output = []\n",
    "    totalLossPerEpoch = []\n",
    "    for batch_x, batch_edge_index, batch_edge_index_out, batch_y in data_loader:\n",
    "        batch_x = torch.stack(batch_x).to(device)\n",
    "        batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "        #print(batch_edge_index[0].shape)\n",
    "        batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "        batch_y = [y.to(device) for y in batch_y]\n",
    "        #print(len(batch_y))\n",
    "        optimizer.zero_grad()\n",
    "        loss_per_batch = []\n",
    "        for i in range(len(batch_edge_index)):\n",
    "            _output = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "            #print(len(_output))\n",
    "            #output.append(_output)\n",
    "            loss = criterion(_output.squeeze(), batch_y[i].squeeze())\n",
    "            #print(loss)\n",
    "            loss_per_batch.append(loss)\n",
    "        #print(loss_per_batch)\n",
    "        #loss_per = torch.tensor(loss_per, dtype=torch.float)\n",
    "        total_loss_per_batch = sum(loss_per_batch)/ len(loss_per_batch)\n",
    "        totalLossPerEpoch.append(total_loss_per_batch)\n",
    "        #total_loss = torch.tensor(total_loss, requires_grad=True) \n",
    "        #print(\"total_loss_per_batch: \",total_loss_per_batch)\n",
    "        #total_loss.backward()\n",
    "        total_loss_per_batch.backward()\n",
    "        optimizer.step()\n",
    "    #print(\"totalLossPerEpoch: \",totalLossPerEpoch)\n",
    "    total_loss_per_epoch = sum(totalLossPerEpoch)/len(totalLossPerEpoch)\n",
    "    print(\"total_loss_per_epoch:\",total_loss_per_epoch)\n",
    "    return total_loss_per_epoch\n",
    "#Creates a method to train the GNN and return the loss for that epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7a64021-3ae3-4957-aafe-0a14ce9caa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, device, data_loader_true, data_loader_bkg_total, criterion):\n",
    "    all_scores = []\n",
    "    true_labels = []\n",
    "    totalLossPerEpochTestTrue = []\n",
    "    totalLossPerEpochTestBackground = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        for batch_x, batch_edge_index, batch_edge_index_out, batch_y in data_loader_true:\n",
    "            batch_x = torch.stack(batch_x).to(device)\n",
    "            batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "            batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "            batch_y = [y.to(device) for y in batch_y]\n",
    "            loss_per_batch = []\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                loss = criterion(test_edge_scores.squeeze(), batch_y[i].squeeze())\n",
    "                loss_per_batch.append(loss)\n",
    "                all_scores.append(test_edge_scores)\n",
    "                true_labels.append(torch.ones(test_edge_scores.size(0)))\n",
    "            total_loss_per_batch = sum(loss_per_batch)/len(loss_per_batch)\n",
    "            totalLossPerEpochTestTrue.append(total_loss_per_batch)\n",
    "        for batch_x, batch_edge_index, batch_edge_index_out, batch_y in data_loader_bkg_total:\n",
    "            batch_x = torch.stack(batch_x).to(device)\n",
    "            batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "            batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "            batch_y = [y.to(device) for y in batch_y]\n",
    "            loss_per_batch = []\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                loss = criterion(test_edge_scores.squeeze(), batch_y[i].squeeze())\n",
    "                loss_per_batch.append(loss)\n",
    "                all_scores.append(test_edge_scores)\n",
    "                true_labels.append(torch.zeros(test_edge_scores.size(0)))\n",
    "            total_loss_per_batch = sum(loss_per_batch)/len(loss_per_batch)\n",
    "            totalLossPerEpochTestBackground.append(total_loss_per_batch)\n",
    "        \n",
    "    all_scores = torch.cat(all_scores, dim = 0).cpu().numpy()\n",
    "    true_labels = torch.cat(true_labels, dim = 0).cpu().numpy()\n",
    "    total_loss_per_epoch_test_true = sum(totalLossPerEpochTestTrue)/len(totalLossPerEpochTestTrue)\n",
    "    total_loss_per_epoch_test_background = sum(totalLossPerEpochTestBackground)/len(totalLossPerEpochTestBackground)\n",
    "    return all_scores, true_labels, total_loss_per_epoch_test_true, total_loss_per_epoch_test_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fa0155c-7607-433d-bb70-7a0e5567139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickleData(fileName, data):\n",
    "    file = open(fileName, 'wb')\n",
    "    pickle.dump(data, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4208c3cc-24d4-42be-8bab-3f73e05dd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTruthArray(data, truth):\n",
    "    shape = data.shape\n",
    "    if truth:\n",
    "        return torch.ones(getTruthArrayShape(shape))\n",
    "    else:\n",
    "        return torch.zeros(getTruthArrayShape(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3461cb03-a166-40c6-8862-f0ab8885c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTruthArrayShape(shape):\n",
    "    return (int(shape[0]), int(shape[1]-1), int(shape[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
