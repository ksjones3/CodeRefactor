{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1645cd-246a-4043-93a8-b005583d6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "random.seed(42)\n",
    "import uproot\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3080b8a-88af-4979-9863-127839b23505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def branches_from_root_file(filename):\n",
    "    '''\n",
    "    Returns the branches from a root file\n",
    "    '''\n",
    "    file = uproot.open(filename)\n",
    "    tree = file[file.keys()[0]]\n",
    "    branches = tree.arrays()\n",
    "    return branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b96d0a-714b-4fd8-96ea-41b46bfd063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(branches, dataName):\n",
    "    return np.array(branches[dataName])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be119dfd-146a-414f-800d-6c13837579f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dict(rangeValue, variables, dataDict):\n",
    "    for i in range(rangeValue):\n",
    "        dataDict[f\"data_{i}\"] = np.concatenate([np.expand_dims(var[i], axis=1) for var in variables], axis=1)\n",
    "    return dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a8d257-6584-4e19-b88e-20c346ec27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_hdf5(dic, filename):\n",
    "    \"\"\"Save a dictionary to an HDF5 file\"\"\"\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        _save_dict_to_hdf5(f, dic)\n",
    "\n",
    "\n",
    "def _save_dict_to_hdf5(group, dic):\n",
    "    \"\"\"Save a dictionary to an HDF5 group\"\"\"\n",
    "    for key, value in dic.items():\n",
    "        if isinstance(value, dict):\n",
    "            subgroup = group.create_group(key)\n",
    "            _save_dict_to_hdf5(subgroup, value)\n",
    "        else:\n",
    "            if isinstance(value, list):\n",
    "                \"\"\"Convert list to numpy array before saving\"\"\"\n",
    "                value = np.array(value)\n",
    "            group[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9723967e-33d2-46cb-8325-5367df59ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A code to remove permutation variant\n",
    "def canonical_form(t):\n",
    "    \"\"\"Sorts elements of the tuple and converts the sorted list back into a tuple.\"\"\"\n",
    "    return tuple(sorted(t))\n",
    "\n",
    "def remove_permutation_variants(tuple_list):\n",
    "    \"\"\"\n",
    "    Creates a set of unique tuples by converting each tuple to its canonical form.\n",
    "    Remove permutation variants from a list of tuples.\n",
    "    Converts set back into a list of tuples.\n",
    "    \"\"\"\n",
    "    unique_tuples = set(canonical_form(t) for t in tuple_list)\n",
    "    return [tuple(sorted(t)) for t in unique_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6169262-f663-4a0d-9ed1-13efb123404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = cell_to_cluster_index[i]\n",
    "# i = pair[0]=z[0], j = pair[1]=z[1]\n",
    "def cluster_cluster_true(y,i,j):\n",
    "    return y[i]==y[j] and y[i]!=0\n",
    "def lone_lone(y,i,j):\n",
    "    return y[i]==y[j] and y[i]==0\n",
    "def cluster_cluster_false(y,i,j):\n",
    "    return y[i]!=y[j] and y[i]!=0 and y[j]!=0\n",
    "def cluster_lone(y,i,j):\n",
    "    return y[i]!=y[j] and y[i]!=0 and y[j]==0\n",
    "def lone_cluster(y,i,j):\n",
    "    return y[i]!=y[j] and y[i]==0 and y[j]!=0\n",
    "\n",
    "# x = neighbor_pairs_unique_sorted\n",
    "# y = cell_to_cluster_index[i]\n",
    "# z = pair\n",
    "def assign_index(mapping, x, y):\n",
    "    out = []\n",
    "    for pair in x:\n",
    "        for index, test in mapping.items():\n",
    "            if test(y,pair[0],pair[1]):\n",
    "                out.append(index)\n",
    "                continue\n",
    "    return out\n",
    "\n",
    "def neighbor_pairs_mapping(loneloneIndex, clusterloneIndex, \n",
    "                           loneclusterIndex, clusterclusterFalseIndex):\n",
    "    '''\n",
    "    Set the class value for the background types (integer excluding 1) for the cases \n",
    "    where the neighbor pairs both lone cells, one from a cluster and the other \n",
    "    a lone cell (and vice versa), or both are from differnt clusters\n",
    "    '''\n",
    "    pairs_mapping = {1: cluster_cluster_true, loneloneIndex: lone_lone, \n",
    "                     clusterloneIndex: cluster_lone, loneclusterIndex: lone_cluster, \n",
    "                     clusterclusterFalseIndex: cluster_cluster_false}\n",
    "    return pairs_mapping\n",
    "\n",
    "# list_of_pair_indices=neighbor_pairs_unique_sorted\n",
    "# index of cluster cell is a part of =cell_to_cluster_index\n",
    "def label_neighbor_pairs(range_value, cell_to_cluster_index, list_of_pair_indices, mapping):\n",
    "    neighbor_labels = []\n",
    "    for i in range(range_value):\n",
    "        neighbor_labels.append(assign_index(mapping=mapping, x=list_of_pair_indices, y=cell_to_cluster_index[i]))    \n",
    "    return np.array(neighbor_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7eac67-2ed1-4a2c-a9ac-cbc8bfbf597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf5_to_dict(hdf5_file):\n",
    "    \"\"\"\n",
    "    Convert HDF5 file to Python dictionary\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    _hdf5_to_dict(hdf5_file, data_dict)\n",
    "    return data_dict\n",
    "# Initializes an empty dictionary and calls a function to recursively\n",
    "# fill this dictionary with data from the hdf5 file.\n",
    "\n",
    "\n",
    "def _hdf5_to_dict(group, dic):\n",
    "    \"\"\"\n",
    "    Convert HDF5 group to dictionary recursively\n",
    "    \"\"\"\n",
    "    for key, item in group.items():\n",
    "        if isinstance(item, h5py.Group):\n",
    "            subgroup = {}\n",
    "            _hdf5_to_dict(item, subgroup)\n",
    "            dic[key] = subgroup\n",
    "        else:\n",
    "            dic[key] = np.array(item)\n",
    "# Iterates over items in the hdf5 group. If the item is a group, \n",
    "# it creates a new dictionary and calls itself recursively. If the item\n",
    "# is a dataset, it converts it to a numpy array and stores it in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60604dbb-67d9-48b6-a0ae-41aa0a717caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeEdges(neighbor_truth, label):\n",
    "    pair = np.where(neighbor_truth==label)\n",
    "    return list(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88ba87dc-5004-4da1-aefb-7d4ed3bbd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_of_edges(range_value, neighbor_truth):\n",
    "    '''\n",
    "    bkg_0 represents the background with a truth value of 0\n",
    "    bkg_2/3/4 represents the background with a truth value of 2/3/4\n",
    "    '''\n",
    "    true_list = []\n",
    "    bkg_0 = []\n",
    "    bkg_2 = []\n",
    "    bkg_3 = []\n",
    "    bkg_4 = []\n",
    "    for i in range(range_value):\n",
    "        # y = cell_to_cluster_index[i]\n",
    "        neighbor_truth_element = neighbor_truth[i]\n",
    "        true_list.append(makeEdges(neighbor_truth_element, 1))\n",
    "        bkg_0.append(makeEdges(neighbor_truth_element, 0))\n",
    "        bkg_2.append(makeEdges(neighbor_truth_element, 2))\n",
    "        bkg_3.append(makeEdges(neighbor_truth_element, 3))\n",
    "        bkg_4.append(makeEdges(neighbor_truth_element, 4))\n",
    "    return true_list, bkg_0, bkg_2, bkg_3, bkg_4\n",
    "\n",
    "def pairs_of_edges(range_value, true_list, bkg_0_list, bkg_2_list, bkg_3_list, bkg_4_list):\n",
    "    '''\n",
    "    bkg_0 represents the background with a truth value of 0\n",
    "    bkg_2/3 represents the background with a truth value of 2/3\n",
    "    '''\n",
    "    true_pairNumber = []\n",
    "    bkg_0_pairNumber = []\n",
    "    bkg_2_pairNumber = []\n",
    "    bkg_3_pairNumber = []\n",
    "    bkg_4_pairNumber = []\n",
    "    for i in range(range_value):\n",
    "        true_pairNumber.append(len(true_list[i]))\n",
    "        bkg_0_pairNumber.append(len(bkg_0_list[i]))\n",
    "        bkg_2_pairNumber.append(len(bkg_2_list[i]))\n",
    "        bkg_3_pairNumber.append(len(bkg_3_list[i]))\n",
    "        bkg_4_pairNumber.append(len(bkg_4_list[i]))\n",
    "    return np.array(true_pairNumber), np.array(bkg_0_pairNumber), np.array(bkg_2_pairNumber), np.array(bkg_3_pairNumber), np.array(bkg_4_pairNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f738152-9d64-4228-b750-ad6f259f92f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_indices(pair_number_list, true_list, bkg_0, bkg_2, bkg_3, bkg_4):\n",
    "    sorted_indices = np.argsort(-pair_number_list)\n",
    "    return true_list[sorted_indices], bkg_0[sorted_indices], bkg_2[sorted_indices], bkg_3[sorted_indices], bkg_4[sorted_indices], sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e7d7e55-f34f-4496-bd59-ec350a4a7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortList(data, sorted_indices):\n",
    "    return [data[i] for i in sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8ef6a21-5a9b-482d-af9e-10dc6af0d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_set_test_set(split_num, true_list, bkg_0, bkg_2, bkg_3, bkg_4):\n",
    "    return true_list[:split_num],bkg_0[:split_num],bkg_2[:split_num],bkg_3[:split_num], bkg_4[:split_num], true_list[split_num:],bkg_0[split_num:],bkg_2[split_num:],bkg_3[split_num:], bkg_4[split_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "776e9d30-a5dd-4713-8f0d-e7e7c6e17032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_of_features(dynamic_variables, split_num, sorted_indices, scaler_fileName):\n",
    "    keys = list(dynamic_variables.keys())\n",
    "    values = list(dynamic_variables.values())\n",
    "    #Gets the keys and variables from the dynamic variables array and stores them in arrays.\n",
    "    \n",
    "    rearranged_values = [values[i] for i in sorted_indices]\n",
    "    #Sorts the values with the sorted cluster indices.\n",
    "    \n",
    "    rearranged_dict = dict(zip(keys, rearranged_values))\n",
    "    #Creates a rearranged dictionary out of the keys and sorted values.\n",
    "    \n",
    "    data_train = np.concatenate([value for key, value in list(rearranged_dict.items())[:split_num]])\n",
    "    #Create a training data array.\n",
    "    \n",
    "    data_test = np.concatenate([value for key, value in list(dynamic_variables.items())[split_num:]])\n",
    "    #Create a data testing array.\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    cellFeatures_trainS = scaler.fit_transform(data_train)\n",
    "    cell_features_testS = scaler.transform(data_test)\n",
    "    scaler_filename = \"./bscaler_neighbor_data_train_sorted.save\"\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    return cellFeatures_trainS, cell_features_testS\n",
    "    #Scales the training data with a minmaxscaler, put that scaled data into a training features array, and then save that scaler into a .save file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa75fbfe-8d45-4288-ac6a-00461109643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapeFeatures(splitNum, features):\n",
    "    return features.reshape(splitNum, int(features.shape[0]/splitNum), int(features.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68856234-35c1-4b59-97a2-fd88d9ec0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDataH5(fileName, dataDict, compressionType):\n",
    "    with h5py.File(fileName, 'w') as file:\n",
    "        for key, data in dataDict.items():\n",
    "            file.create_dataset(key, data=data, compression=compressionType)\n",
    "\n",
    "def loadDataH5(fileName):\n",
    "    dataDict = {}\n",
    "    with h5py.File(fileName, 'r') as file:\n",
    "        for key in file.keys():\n",
    "            dataDict[key] = np.array(file[key])\n",
    "    return dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf8177-ed01-4149-8874-775df10e9b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e3ab5f8-fb58-45fc-a9da-06cd2a84b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBrokenCells(cell_noiseSigma, neighbor):\n",
    "    broken_cells = getBrokenCells(cell_noiseSigma)\n",
    "    return getNeighborPairs(broken_cells, neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7de949a4-aa19-4657-b4a7-4dba96bc732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBrokenCells(cell_noiseSigma):\n",
    "    broken_cell_indices = np.argwhere(cell_noiseSigma[0] == 0)\n",
    "    broken_cells = []\n",
    "    for arrays in broken_cell_indices:\n",
    "        for index in broken_cell_indices:\n",
    "            broken_cells.append(index)\n",
    "    return broken_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49845c41-a868-4cd0-86b5-21e9a02e96b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeighborPairs(broken_cells, neighbor):\n",
    "    neighbor_pairs_set = []\n",
    "    for i in range(len(neighbor)):\n",
    "        if i in broken_cells:\n",
    "            continue\n",
    "        for cell in neighbor[i]:\n",
    "            if cell in broken_cells:\n",
    "                continue\n",
    "            neighbor_pairs_set.append(((i, cell)))\n",
    "    return neighbor_pairs_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fec2d60d-a659-4fd7-b110-3875d906d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeH5File(fileName, datasetName, data):\n",
    "    with h5py.File(fileName, \"w\") as f:\n",
    "        dset = f.create_dataset(datasetName, data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "533db341-9c5d-4979-a44b-d987da748ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readH5File(fileName, datasetName):\n",
    "    file = h5py.File(fileName, \"r\")\n",
    "    data = file.get(datasetName)[:]\n",
    "    file.close()\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0740953c-5aeb-4be9-889a-4e53ad00d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleDataTraining(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster):\n",
    "    true_sample_size, bkg_sample_size = getTrainingSampleSizes(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster)\n",
    "    return sampleData(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster, true_sample_size, bkg_sample_size, bkg_sample_size, bkg_sample_size, bkg_sample_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "367745a2-4a8e-41eb-96bf-13ab271505d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleDataTesting(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster):\n",
    "    true_sample_size = getTestingSampleSize(true)\n",
    "    bkg_lone_sample_size = getTestingSampleSize(bkg_lone)\n",
    "    bkg_cluster_lone_sample_size = getTestingSampleSize(bkg_cluster_lone)\n",
    "    bkg_lone_cluster_sample_size = getTestingSampleSize(bkg_lone_cluster)\n",
    "    bkg_cluster_cluster_sample_size = getTestingSampleSize(bkg_cluster_cluster)\n",
    "    return sampleData(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster, true_sample_size, bkg_lone_sample_size, bkg_cluster_lone_sample_size, bkg_lone_cluster_sample_size, bkg_cluster_cluster_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c0bf805-458d-4420-8089-df229844f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestingSampleSize(data):\n",
    "    minimum = getMinimum(data)\n",
    "    sample_size = minimum - (minimum % 100)\n",
    "    return sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7040dd54-00b2-4336-8407-91d6d62e6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinimum(data):\n",
    "    return min([len(row) for row in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "681f0cc8-efe7-494f-9970-9c4072c5cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBackgroundMin(bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster):\n",
    "    bkg_lone_min = getMinimum(bkg_lone)\n",
    "    bkg_cluster_lone_min = getMinimum(bkg_cluster_lone)\n",
    "    bkg_lone_cluster_min = getMinimum(bkg_lone_cluster)\n",
    "    bkg_cluster_cluster_min = getMinimum(bkg_cluster_cluster)\n",
    "    bkg_min = min([bkg_lone_min, bkg_cluster_lone_min, bkg_lone_cluster_min, bkg_cluster_cluster_min])\n",
    "    return bkg_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4122db0-3377-417d-9b41-46a8eb045653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingSampleSizes(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster):\n",
    "    true_min = getMinimum(true)\n",
    "    bkg_min = getBackgroundMin(bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster)\n",
    "    bkg_sample_size = bkg_min - (bkg_min % 100)\n",
    "    true_sample_size = bkg_sample_size*4\n",
    "    if true_sample_size > true_min:\n",
    "        true_sample_size = true_min - (true_min % 100)\n",
    "        true_sample_size = true_sample_size - (true_sample_size % 4)\n",
    "        bkg_sample_size = true_sample_size/4\n",
    "    return true_sample_size, bkg_sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "837e7c39-5f0b-49e8-ba25-76c3e321dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleData(true, bkg_lone, bkg_cluster_lone, bkg_lone_cluster, bkg_cluster_cluster, true_sample_size, bkg_lone_sample_size, bkg_cluster_lone_sample_size, bkg_lone_cluster_sample_size, bkg_cluster_cluster_sample_size):\n",
    "    true_sample = sampleDataset(true, true_sample_size)\n",
    "    bkg_lone_sample = sampleDataset(bkg_lone, bkg_lone_sample_size)\n",
    "    bkg_cluster_lone_sample = sampleDataset(bkg_cluster_lone, bkg_cluster_lone_sample_size)\n",
    "    bkg_lone_cluster_sample = sampleDataset(bkg_lone_cluster, bkg_lone_cluster_sample_size)\n",
    "    bkg_cluster_cluster_sample = sampleDataset(bkg_cluster_cluster, bkg_cluster_cluster_sample_size)\n",
    "    return np.array(true_sample), np.array(bkg_lone_sample), np.array(bkg_cluster_lone_sample), np.array(bkg_lone_cluster_sample), np.array(bkg_cluster_cluster_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb020edd-c5a3-4507-a31a-0d111a00c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleDataset(data, data_sample_size):\n",
    "    return [random.sample(row, data_sample_size) for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3de1b0cd-9938-42fd-8e9c-3d72ed3ed6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRandomIndices(total_indices_shape):\n",
    "    rand_index = []\n",
    "    for i in range(total_indices_shape[0]):\n",
    "        arr = np.arange(total_indices_shape[1])\n",
    "        np.random.shuffle(arr)\n",
    "        rand_index.append(arr)\n",
    "    return np.array(rand_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "832cf9b5-0483-4325-a79c-98b1a5f5b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize2DArray(rand_indices, unrandomized_array):\n",
    "    randomized_list = []\n",
    "    for i in range(unrandomized_array.shape[0]):\n",
    "        randomized_list.append(unrandomized_array[i][rand_indices[i]])\n",
    "    return np.array(randomized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "392f3ace-e87f-4a24-a4df-9021d115e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomizeEdges(rand_indices, unrandomized_edges):\n",
    "    randomized_list = []\n",
    "    for i in range(rand_indices.shape[0]):\n",
    "        randomized_list.append(unrandomized_edges[rand_indices[i]])\n",
    "    return np.array(randomized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f41cbaa4-0985-460b-a4dc-a65a6f504503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEdgeArrays(inputData):\n",
    "    source_BD = []\n",
    "    dest_BD = []\n",
    "    source_noBD = []\n",
    "    dest_noBD = []\n",
    "    for i in range(inputData.shape[0]):\n",
    "        source_BD_element, dest_BD_element, source_noBD_element, dest_noBD_element = createBDAndNoBDArrays(inputData[i])\n",
    "\n",
    "        source_BD.append(source_BD_element)\n",
    "        dest_BD.append(dest_BD_element)\n",
    "        source_noBD.append(source_noBD_element)\n",
    "        dest_noBD.append(dest_noBD_element)\n",
    "    return np.array(source_BD), np.array(dest_BD), np.array(source_noBD), np.array(dest_noBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6be203e8-0099-4f8c-badf-ef4b3427908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBDAndNoBDArrays(inputData):\n",
    "    source_BD = []\n",
    "    dest_BD = []\n",
    "    source_noBD = []\n",
    "    dest_noBD = []\n",
    "\n",
    "    for pair in inputData:\n",
    "\n",
    "        source_BD.append(pair[0])\n",
    "        source_BD.append(pair[1])\n",
    "        \n",
    "        dest_BD.append(pair[1])\n",
    "        dest_BD.append(pair[0])\n",
    "\n",
    "        source_noBD.append(pair[0])\n",
    "        dest_noBD.append(pair[1])\n",
    "        \n",
    "    return source_BD, dest_BD, source_noBD, dest_noBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
