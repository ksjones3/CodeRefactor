{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "random.seed(42)  \n",
    "\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "\n",
    "from collections import Counter  \n",
    "\n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F  \n",
    "import torch.optim as optim  \n",
    "\n",
    "from torch.nn import BatchNorm1d  \n",
    "from torch_geometric.data import Data  \n",
    "from torch_geometric.nn import GCNConv  \n",
    "from torch_geometric.transforms import ToUndirected  \n",
    "from torch_geometric.utils import add_self_loops  \n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix  \n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "\n",
    "import uproot  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['analysis;1']\n"
     ]
    }
   ],
   "source": [
    "file = uproot.open('/home/mxg1065/MyxAODAnalysis_super3D.outputs.root')\n",
    "print(file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RunNumber', 'EventNumber', 'cell_eta', 'cell_phi', 'cell_x', 'cell_y', 'cell_z', 'cell_subCalo', 'cell_sampling', 'cell_size', 'cell_hashID', 'neighbor', 'seedCell_id', 'cell_e', 'cell_noiseSigma', 'cell_SNR', 'cell_time', 'cell_weight', 'cell_truth', 'cell_truth_indices', 'cell_shared_indices', 'cell_cluster_index', 'cluster_to_cell_indices', 'cluster_to_cell_weights', 'cell_to_cluster_e', 'cell_to_cluster_eta', 'cell_to_cluster_phi', 'cluster_eta', 'cluster_phi', 'cluster_e', 'cellsNo_cluster', 'clustersNo_event', 'jetEnergyWtdTimeAve', 'jetEta', 'jetPhi', 'jetE', 'jetPt', 'jetNumberPerEvent', 'cellIndices_per_jet']\n"
     ]
    }
   ],
   "source": [
    "tree = file['analysis;1']\n",
    "branches = tree.arrays()\n",
    "print(tree.keys()) # Variables per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 events and 187652 cells\n",
    "# Arrays containing information about the energy, noise, and snr for each cell\n",
    "cell_e = np.array(branches['cell_e'])\n",
    "cell_noise = np.array(branches['cell_noiseSigma'])\n",
    "cell_snr = np.array(branches['cell_SNR'])\n",
    "\n",
    "# Represents the index of the cluster that each cell corresponds to. If the index\n",
    "# is 0, that means that the given cell does not belong to a cluster.\n",
    "cell_to_cluster_index = np.array(branches['cell_cluster_index'])\n",
    "\n",
    "# For each entry, contains the IDs of cells neighboring a given cell\n",
    "neighbor = branches['neighbor']\n",
    "\n",
    "num_of_events = len(cell_e) # 100 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02640459 0.24943821 0.09700817]\n",
      " [0.02567646 0.24627675 0.09700815]\n",
      " [0.02508186 0.24369499 0.09700817]\n",
      " ...\n",
      " [0.02632877 0.24791998 0.03177016]\n",
      " [0.02705116 0.2511391  0.07512318]\n",
      " [0.02638626 0.24820389 0.04149057]]\n",
      "(187652, 3)\n"
     ]
    }
   ],
   "source": [
    "# We use the data arrays to crete a data dictionary, where each entry corresponds\n",
    "# to the data of a given event; we scale this data.\n",
    "data = {}\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    data[f'data_{i}'] = np.concatenate((np.expand_dims(cell_snr[i], axis=1),\n",
    "                                        np.expand_dims(cell_e[i], axis=1),\n",
    "                                        np.expand_dims(cell_noise[i], axis=1)), axis=1)\n",
    "    \n",
    "# We combine the data into one array and apply the MinMaxScaler\n",
    "combined_data = np.vstack([data[key] for key in data])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_combined_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "# The scaled data is split to have the save structure as the original data dict\n",
    "scaled_data = {}\n",
    "start_idx = 0\n",
    "for i in range(num_of_events):\n",
    "    end_idx = start_idx + data[f\"data_{i}\"].shape[0]\n",
    "    scaled_data[f\"data_{i}\"] = scaled_combined_data[start_idx:end_idx]\n",
    "    start_idx = end_idx\n",
    "\n",
    "print(scaled_data[\"data_0\"])\n",
    "print(scaled_data[\"data_0\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Neighbor Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186986 187352]\n"
     ]
    }
   ],
   "source": [
    "# The IDs of the broken cells (those with zero noise) are collected\n",
    "broken_cells = []\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    cells = np.argwhere(cell_noise[i]==0)\n",
    "    broken_cells = np.squeeze(cells)\n",
    "\n",
    "print(broken_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the values associated with neighbor[0] and neighbor[1] are all equal\n",
    "# we will just work with neighbor[0] to simplify our calculations\n",
    "neighbor = neighbor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187652"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We loop through the neighbor awkward array and remove the IDs associated\n",
    "# with the broken cells.  Loops through all cells in the neighbor list. If the loop \n",
    "# reaches the cell numbers 186986 or 187352, loop skips over these inoperative cells. \n",
    "# The final list contains tuples (i,j) where i is the cell ID in question and the \n",
    "# js are the neighboring cell IDs\n",
    "neighbor_pairs_list = []\n",
    "num_of_cells = len(neighbor) # 187652 cells\n",
    "\n",
    "for i in range(num_of_cells):\n",
    "    if i in broken_cells:\n",
    "        continue\n",
    "    for j in neighbor[i]:\n",
    "        if j in broken_cells:\n",
    "            continue\n",
    "        neighbor_pairs_list.append((i, int(j)))\n",
    "\n",
    "\n",
    "# # This code checks to see if the broken cells were removed\n",
    "# found_broken_cells = []\n",
    "\n",
    "# for pair in neighbor_pairs_list:\n",
    "#     # Loop through each cell in pair\n",
    "#     for cell in pair:\n",
    "#         # If the cell is broken, appends to list\n",
    "#         if cell in broken_cells:\n",
    "#             found_broken_cells.append(cell)\n",
    "\n",
    "# if found_broken_cells:\n",
    "#     print(\"Error: Broken cells are still present in neighbor pairs.\")\n",
    "# else:\n",
    "#     print(\"Successfully excluded broken cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 90345 119588]\n",
      " [  4388  17680]\n",
      " [ 39760  39825]\n",
      " ...\n",
      " [159757 168717]\n",
      " [ 62911  78974]\n",
      " [135353 135609]]\n",
      "(1250242, 2)\n"
     ]
    }
   ],
   "source": [
    "# These functions remove permutation variants\n",
    "def canonical_form(t):\n",
    "    return tuple(sorted(t))\n",
    "\n",
    "def remove_permutation_variants(tuple_list):\n",
    "    unique_tuples = set(canonical_form(t) for t in tuple_list)\n",
    "    return [tuple(sorted(t)) for t in unique_tuples]\n",
    "\n",
    "neighbor_pairs_list = np.array(remove_permutation_variants(neighbor_pairs_list))\n",
    "print(neighbor_pairs_list)\n",
    "print(neighbor_pairs_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Labels for the Neighbor Pairs\n",
    "\n",
    "For a given pair of cells and the IDs of the clusters that they belong to (i, j), if\n",
    "1. i=j and both are nonzero, then both cells are part of the same cluster. \n",
    "    * We call these True-True pairs and label them with 1\n",
    "2. i=j and both are zero, then both cells are not part of any cluster. \n",
    "    * We call these Lone-Lone pairs and label them with 0\n",
    "3. i is nonzero and j=0, then cell i is part of a cluster while cell j is not. \n",
    "    * We call these Cluster-Lone pairs and label them with 2\n",
    "4. i=0 and j is nonzero, then cell i is not part of a cluste while cell j is. \n",
    "    * We call these Lone-Cluster pairs and label them with 3\n",
    "5. i is not the same as j and both are nonzero, then both cells are part of different clusters. \n",
    "    * We call these Cluster-Cluster pairs and label them with 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1250242)\n",
      "[[0 0 0 ... 0 0 2]\n",
      " [3 3 0 ... 0 0 2]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 2 0 0]]\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "labels_for_neighbor_pairs = []\n",
    "for i in range(num_of_events):\n",
    "    labels_for_neighbor_pairs_for_event_i = []\n",
    "    for pair in neighbor_pairs_list:\n",
    "        if cell_to_cluster_index[i][pair[0]] == cell_to_cluster_index[i][pair[1]]:\n",
    "            if cell_to_cluster_index[i][pair[0]] != 0:\n",
    "                labels_for_neighbor_pairs_for_event_i.append(1) # True-True\n",
    "            else:\n",
    "                labels_for_neighbor_pairs_for_event_i.append(0) # Lone-Lone\n",
    "        else:\n",
    "            if cell_to_cluster_index[i][pair[0]] != 0 and cell_to_cluster_index[i][pair[1]] != 0:\n",
    "                labels_for_neighbor_pairs_for_event_i.append(4) # Cluster-Cluster\n",
    "            elif cell_to_cluster_index[i][pair[0]] == 0 and cell_to_cluster_index[i][pair[1]] != 0:\n",
    "                labels_for_neighbor_pairs_for_event_i.append(3) # Lone-Cluster\n",
    "            else:\n",
    "                labels_for_neighbor_pairs_for_event_i.append(2) # Cluster-Lone\n",
    "    labels_for_neighbor_pairs.append(labels_for_neighbor_pairs_for_event_i)\n",
    "\n",
    "labels_for_neighbor_pairs = np.array(labels_for_neighbor_pairs)\n",
    "print(labels_for_neighbor_pairs.shape)\n",
    "print(labels_for_neighbor_pairs)\n",
    "print(np.unique(labels_for_neighbor_pairs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating plots for the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here we create a dictionary to store features by class\n",
    "# features_by_class = {cls: {'SNR': [], 'Energy': [], 'Noise': []}\n",
    "#                      for cls in range(5)}\n",
    "\n",
    "# # The features for all events are computed\n",
    "# for i in range(num_of_events):  # Iterate through events\n",
    "#     for pair_idx, pair in enumerate(neighbor_pairs_list):\n",
    "#         class_label = labels_for_neighbor_pairs[i][pair_idx]\n",
    "\n",
    "#         # Vectorized feature extraction for cell pairs\n",
    "#         cell_1_features = [cell_snr[i][pair[0]], cell_e[i][pair[0]], cell_noise[i][pair[0]]]\n",
    "#         cell_2_features = [cell_snr[i][pair[1]], cell_e[i][pair[1]], cell_noise[i][pair[1]]]\n",
    "\n",
    "#         # Append features to the corresponding class\n",
    "#         features_by_class[class_label]['SNR'] += [cell_1_features[0],cell_2_features[0]]\n",
    "#         features_by_class[class_label]['Energy'] += [cell_1_features[1], cell_2_features[1]]\n",
    "#         features_by_class[class_label]['Noise'] += [cell_1_features[2],cell_2_features[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = plt.hist(cell_snr[:][neighbor_pairs_list[labels_for_neighbor_pairs[:]==1][:,0]], bins=np.arange(-20,20,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (cell_snr[0][neighbor_pairs_list[labels_for_neighbor_pairs[0]==1]][:,0]).shape\n",
    "# # cell_snr[0][neighbor_pairs_list[0][labels_for_neighbor_pairs[0]==1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ =plt.hist(cell_snr[0], bins=np.arange(-10,10,.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We create a function to precompute bin edges for all features\n",
    "# def precompute_bins(features_by_class, snr_xlim=None, energy_xlim=None, noise_xlim=None, bins=50):\n",
    "#     bins_dict = {}\n",
    "\n",
    "#     # Compute bins for SNR\n",
    "#     if snr_xlim:\n",
    "#         bins_dict['SNR'] = np.linspace(snr_xlim[0], snr_xlim[1], bins + 1)\n",
    "#     else:\n",
    "#         all_snr = np.concatenate([features['SNR']for features in features_by_class.values()])\n",
    "#         bins_dict['SNR'] = np.histogram_bin_edges(all_snr, bins=bins)\n",
    "\n",
    "#     # Compute bins for Energy\n",
    "#     if energy_xlim:\n",
    "#         bins_dict['Energy'] = np.linspace(energy_xlim[0], energy_xlim[1], bins + 1)\n",
    "#     else:\n",
    "#         all_energy = np.concatenate([features['Energy']for features in features_by_class.values()])\n",
    "#         bins_dict['Energy'] = np.histogram_bin_edges(all_energy, bins=bins)\n",
    "\n",
    "#     # Compute bins for Noise\n",
    "#     if noise_xlim:\n",
    "#         bins_dict['Noise'] = np.linspace(noise_xlim[0], noise_xlim[1], bins + 1)\n",
    "#     else:\n",
    "#         all_noise = np.concatenate([features['Noise']for features in features_by_class.values()])\n",
    "#         bins_dict['Noise'] = np.histogram_bin_edges(all_noise, bins=bins)\n",
    "\n",
    "#     return bins_dict\n",
    "\n",
    "\n",
    "# # Optimized function to plot histograms using precomputed bins\n",
    "# def plot_histograms_optimized(features_by_class, bins_dict):\n",
    "#     for class_label, features in features_by_class.items():\n",
    "#         if len(features['SNR']) > 0:  # Ensure there are features to plot\n",
    "#             plt.figure(figsize=(15, 5))\n",
    "\n",
    "#             # Plot SNR histogram with precomputed bins\n",
    "#             plt.subplot(1, 3, 1)\n",
    "#             plt.hist(features['SNR'], bins=bins_dict['SNR'],\n",
    "#                      alpha=0.6, label=f'Class {class_label}')\n",
    "#             plt.xlabel('SNR')\n",
    "#             plt.ylabel('Frequency')\n",
    "#             plt.title(f'Class {class_label}: SNR')\n",
    "#             plt.grid(True)\n",
    "\n",
    "#             # Plot Energy histogram with precomputed bins\n",
    "#             plt.subplot(1, 3, 2)\n",
    "#             plt.hist(features['Energy'], bins=bins_dict['Energy'],\n",
    "#                      alpha=0.6, label=f'Class {class_label}')\n",
    "#             plt.xlabel('Energy')\n",
    "#             plt.ylabel('Frequency')\n",
    "#             plt.title(f'Class {class_label}: Energy')\n",
    "#             plt.grid(True)\n",
    "\n",
    "#             # Plot Noise histogram with precomputed bins\n",
    "#             plt.subplot(1, 3, 3)\n",
    "#             plt.hist(features['Noise'], bins=bins_dict['Noise'],\n",
    "#                      alpha=0.6, label=f'Class {class_label}')\n",
    "#             plt.xlabel('Noise')\n",
    "#             plt.ylabel('Frequency')\n",
    "#             plt.title(f'Class {class_label}: Noise')\n",
    "#             plt.grid(True)\n",
    "\n",
    "#             plt.tight_layout()\n",
    "#             plt.suptitle(\n",
    "#                 f'Feature Distributions for Class {class_label}', fontsize=16)\n",
    "#             plt.subplots_adjust(top=0.85)  # Adjust title to prevent overlap\n",
    "#             plt.show()\n",
    "\n",
    "\n",
    "# # Precompute bins for SNR, Energy, and Noise\n",
    "# bins_dict = precompute_bins(features_by_class, snr_xlim=(-6.5, 6.5),\n",
    "#                             energy_xlim=(-500, 500), noise_xlim=(0, 500))\n",
    "\n",
    "# # Call the optimized function to plot histograms\n",
    "# plot_histograms_optimized(features_by_class, bins_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for Multi-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we collect the indices of the neighbor pairs by the pair type\n",
    "indices_for_tt_pairs = []  # Label 1\n",
    "indices_for_ll_pairs = []  # Label 0\n",
    "indices_for_cl_pairs = []  # Label 2\n",
    "indices_for_lc_pairs = []  # Label 3\n",
    "indices_for_cc_pairs = []  # Label 4\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    indices_for_tt_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 1)[0]))\n",
    "    indices_for_ll_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 0)[0]))\n",
    "    indices_for_cl_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 2)[0]))\n",
    "    indices_for_lc_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 3)[0]))\n",
    "    indices_for_cc_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 4)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we collect the number of each pair type across the events\n",
    "number_of_tt_pairs = [len(indices_for_tt_pairs[i])for i in range(num_of_events)]\n",
    "number_of_ll_pairs = [len(indices_for_ll_pairs[i])for i in range(num_of_events)]\n",
    "number_of_cl_pairs = [len(indices_for_cl_pairs[i])for i in range(num_of_events)]\n",
    "number_of_lc_pairs = [len(indices_for_lc_pairs[i])for i in range(num_of_events)]\n",
    "number_of_cc_pairs = [len(indices_for_cc_pairs[i])for i in range(num_of_events)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we perform a 70-30 split on the indices of neighbor pairs\n",
    "training_indices_tt = indices_for_tt_pairs[:70]\n",
    "training_indices_ll = indices_for_ll_pairs[:70]\n",
    "training_indices_cl = indices_for_cl_pairs[:70]\n",
    "training_indices_lc = indices_for_lc_pairs[:70]\n",
    "training_indices_cc = indices_for_cc_pairs[:70]\n",
    "\n",
    "testing_indices_tt = indices_for_tt_pairs[70:]\n",
    "testing_indices_ll = indices_for_ll_pairs[70:]\n",
    "testing_indices_cl = indices_for_cl_pairs[70:]\n",
    "testing_indices_lc = indices_for_lc_pairs[70:]\n",
    "testing_indices_cc = indices_for_cc_pairs[70:]\n",
    "\n",
    "# Here we perform a 70-30 split on the number of neighbor pairs\n",
    "training_num_tt = number_of_tt_pairs[:70]\n",
    "training_num_ll = number_of_ll_pairs[:70]\n",
    "training_num_cl = number_of_cl_pairs[:70]\n",
    "training_num_lc = number_of_lc_pairs[:70]\n",
    "training_num_cc = number_of_cc_pairs[:70]\n",
    "\n",
    "testing_num_tt = number_of_tt_pairs[70:]\n",
    "testing_num_ll = number_of_ll_pairs[70:]\n",
    "testing_num_cl = number_of_cl_pairs[70:]\n",
    "testing_num_lc = number_of_lc_pairs[70:]\n",
    "testing_num_cc = number_of_cc_pairs[70:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of pairs for training:\n",
      "True-True 45600\n",
      "Lone-Lone 926119\n",
      "Cluster-Lone 41654\n",
      "Lone-Cluster 45689\n",
      "Cluster-Cluster 3334\n",
      "\n",
      "Minimum number of pairs for testing:\n",
      "True-True 51518\n",
      "Lone-Lone 906630\n",
      "Cluster-Lone 44444\n",
      "Lone-Cluster 48069\n",
      "Cluster-Cluster 4936\n"
     ]
    }
   ],
   "source": [
    "# We check the minimum number of each pair type across the events. When we\n",
    "# randomly sample from the indices, if our sample is greater than the minimum\n",
    "# numbers, then we will run into errors\n",
    "print(\"Minimum number of pairs for training:\")\n",
    "print(\"True-True\", min(training_num_tt))\n",
    "print(\"Lone-Lone\", min(training_num_ll))\n",
    "print(\"Cluster-Lone\", min(training_num_cl))\n",
    "print(\"Lone-Cluster\", min(training_num_lc))\n",
    "print(\"Cluster-Cluster\", min(training_num_cc))\n",
    "print('\\nMinimum number of pairs for testing:')\n",
    "print(\"True-True\", min(testing_num_tt))\n",
    "print(\"Lone-Lone\", min(testing_num_ll))\n",
    "print(\"Cluster-Lone\", min(testing_num_cl))\n",
    "print(\"Lone-Cluster\", min(testing_num_lc))\n",
    "print(\"Cluster-Cluster\", min(testing_num_cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_concatenate(indices, sample_size):\n",
    "    \"\"\"Sample indices and concatenate them into a single array.\"\"\"\n",
    "    # Ensure sample_size does not exceed the minimum number of pairs\n",
    "    min_pairs = min(len(row) for row in indices)\n",
    "    sample_size = min(sample_size, min_pairs)\n",
    "    \n",
    "    # Sample indices and reshape to 2D\n",
    "    sampled_pairs = np.array([random.sample(row, sample_size) for row in indices])\n",
    "    return sampled_pairs\n",
    "\n",
    "def create_total_indices(training_indices, testing_indices, sample_sizes_train, sample_sizes_test):\n",
    "    \"\"\"Create training and testing indices for all pair types.\"\"\"\n",
    "    # Sample and concatenate training indices\n",
    "    train_indices_pairs = {\n",
    "        key: sample_and_concatenate(indices, size) \n",
    "        for key, indices, size in zip([\"tt\", \"ll\", \"cl\", \"lc\", \"cc\"], training_indices, sample_sizes_train)\n",
    "    }\n",
    "    train_indices_bkg = np.concatenate([train_indices_pairs[key] for key in [\"ll\", \"cl\", \"lc\", \"cc\"]], axis=1)\n",
    "    total_training_indices = np.concatenate((train_indices_pairs[\"tt\"], train_indices_bkg), axis=1)\n",
    "    \n",
    "    # Sample and concatenate testing indices\n",
    "    test_indices_pairs = {\n",
    "        key: sample_and_concatenate(indices, size) \n",
    "        for key, indices, size in zip([\"tt\", \"ll\", \"cl\", \"lc\", \"cc\"], testing_indices, sample_sizes_test)\n",
    "    }\n",
    "    test_indices_bkg = np.concatenate([test_indices_pairs[key] for key in [\"ll\", \"cl\", \"lc\", \"cc\"]], axis=1)\n",
    "    total_testing_indices = np.concatenate((test_indices_pairs[\"tt\"], test_indices_bkg), axis=1)\n",
    "    \n",
    "    return train_indices_pairs, test_indices_pairs, total_training_indices, total_testing_indices\n",
    "\n",
    "def create_labels(num_events, num_samples, label_value):\n",
    "    \"\"\"Create a label array for a specific pair type.\"\"\"\n",
    "    return np.full((num_events, num_samples), label_value, dtype=int)\n",
    "\n",
    "def create_total_labels(num_events_train, num_events_test, sample_sizes_train, sample_sizes_test):\n",
    "    \"\"\"Create training and testing labels for all pair types.\"\"\"\n",
    "    # Define label values for each pair type\n",
    "    label_values = {\n",
    "        \"tt\": 1,  # True-True\n",
    "        \"ll\": 0,  # Lone-Lone\n",
    "        \"cl\": 2,  # Cluster-Lone\n",
    "        \"lc\": 3,  # Lone-Cluster\n",
    "        \"cc\": 4,  # Cluster-Cluster\n",
    "    }\n",
    "    \n",
    "    # Create training labels\n",
    "    labels_train = {\n",
    "        key: create_labels(num_events_train, size, value) \n",
    "        for key, value, size in zip(label_values.keys(), label_values.values(), sample_sizes_train)\n",
    "    }\n",
    "    labels_bkg_train = np.concatenate([labels_train[key] for key in [\"ll\", \"cl\", \"lc\", \"cc\"]], axis=1)\n",
    "    labels_training = np.concatenate((labels_train[\"tt\"], labels_bkg_train), axis=1)\n",
    "    \n",
    "    # Create testing labels\n",
    "    labels_test = {\n",
    "        key: create_labels(num_events_test, size, value) \n",
    "        for key, value, size in zip(label_values.keys(), label_values.values(), sample_sizes_test)\n",
    "    }\n",
    "    labels_bkg_test = np.concatenate([labels_test[key] for key in [\"ll\", \"cl\", \"lc\", \"cc\"]], axis=1)\n",
    "    labels_testing = np.concatenate((labels_test[\"tt\"], labels_bkg_test), axis=1)\n",
    "    \n",
    "    return labels_train, labels_test, labels_training, labels_testing\n",
    "\n",
    "def randomize_data(indices, labels):\n",
    "    \"\"\"Randomize indices and labels while keeping the same permutation.\"\"\"\n",
    "    for i in range(indices.shape[0]):\n",
    "        perm = np.random.permutation(indices.shape[1])\n",
    "        indices[i] = indices[i, perm]\n",
    "        labels[i] = labels[i, perm]\n",
    "    return indices, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0:\n",
      "Training indices shape: (70, 24000)\n",
      "Training labels shape: (70, 24000)\n",
      "Testing indices shape: (30, 24000)\n",
      "Testing labels shape: (30, 24000)\n",
      "\n",
      "Version 1:\n",
      "Training indices shape: (70, 24000)\n",
      "Training labels shape: (70, 24000)\n",
      "Testing indices shape: (30, 24000)\n",
      "Testing labels shape: (30, 24000)\n",
      "\n",
      "Version 2:\n",
      "Training indices shape: (70, 24000)\n",
      "Training labels shape: (70, 24000)\n",
      "Testing indices shape: (30, 24000)\n",
      "Testing labels shape: (30, 24000)\n",
      "\n",
      "Version 3:\n",
      "Training indices shape: (70, 24000)\n",
      "Training labels shape: (70, 24000)\n",
      "Testing indices shape: (30, 24000)\n",
      "Testing labels shape: (30, 24000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define sample sizes for training and testing for each version\n",
    "sample_sizes_train_list = [\n",
    "    [12000, 3000, 3000, 3000, 3000],  # Version 0\n",
    "    [3000, 12000, 3000, 3000, 3000],  # Version 1\n",
    "    [3000, 3000, 12000, 3000, 3000],  # Version 2\n",
    "    [3000, 3000, 3000, 12000, 3000],  # Version 3\n",
    "]\n",
    "\n",
    "sample_sizes_test_list = [\n",
    "    [12000, 3000, 3000, 3000, 3000],  # Version 0\n",
    "    [3000, 12000, 3000, 3000, 3000],  # Version 1\n",
    "    [3000, 3000, 12000, 3000, 3000],  # Version 2\n",
    "    [3000, 3000, 3000, 12000, 3000],  # Version 3\n",
    "]\n",
    "\n",
    "# Combine training and testing indices into lists\n",
    "training_indices = [training_indices_tt, training_indices_ll, training_indices_cl, training_indices_lc, training_indices_cc]\n",
    "testing_indices = [testing_indices_tt, testing_indices_ll, testing_indices_cl, testing_indices_lc, testing_indices_cc]\n",
    "\n",
    "# Number of events for training and testing\n",
    "num_events_train = 70\n",
    "num_events_test = 30\n",
    "\n",
    "# Dictionaries to store the four versions of the indices and labels\n",
    "indices_versions = {}\n",
    "labels_versions = {}\n",
    "\n",
    "# Loop through sample sizes\n",
    "for i, (sample_sizes_train, sample_sizes_test) in enumerate(zip(sample_sizes_train_list, sample_sizes_test_list)):\n",
    "    # Create indices\n",
    "    train_indices_pairs, test_indices_pairs, total_training_indices, total_testing_indices = create_total_indices(\n",
    "        training_indices, testing_indices, sample_sizes_train, sample_sizes_test\n",
    "    )\n",
    "    \n",
    "    # Create labels\n",
    "    labels_train, labels_test, labels_training, labels_testing = create_total_labels(\n",
    "        num_events_train, num_events_test, sample_sizes_train, sample_sizes_test\n",
    "    )\n",
    "    \n",
    "    # Randomize training data\n",
    "    total_training_indices, labels_training = randomize_data(total_training_indices, labels_training)\n",
    "    \n",
    "    # Randomize testing data\n",
    "    total_testing_indices, labels_testing = randomize_data(total_testing_indices, labels_testing)\n",
    "    \n",
    "    # Store the randomized data in the dictionaries\n",
    "    indices_versions[f\"version_{i}\"] = {\n",
    "        \"training_indices\": total_training_indices,\n",
    "        \"testing_indices\": total_testing_indices,\n",
    "        \"testing_tt\": test_indices_pairs[\"tt\"],\n",
    "        \"testing_ll\": test_indices_pairs[\"ll\"],\n",
    "        \"testing_cl\": test_indices_pairs[\"cl\"],\n",
    "        \"testing_lc\": test_indices_pairs[\"lc\"],\n",
    "        \"testing_cc\": test_indices_pairs[\"cc\"],\n",
    "    }\n",
    "    \n",
    "    labels_versions[f\"version_{i}\"] = {\n",
    "        \"training_labels\": labels_training,\n",
    "        \"testing_labels\": labels_testing,\n",
    "        \"testing_tt\": labels_test[\"tt\"],\n",
    "        \"testing_ll\": labels_test[\"ll\"],\n",
    "        \"testing_cl\": labels_test[\"cl\"],\n",
    "        \"testing_lc\": labels_test[\"lc\"],\n",
    "        \"testing_cc\": labels_test[\"cc\"],\n",
    "    }\n",
    "    \n",
    "    # Print shapes for verification\n",
    "    print(f\"Version {i}:\")\n",
    "    print(f\"Training indices shape: {total_training_indices.shape}\")\n",
    "    print(f\"Training labels shape: {labels_training.shape}\")\n",
    "    print(f\"Testing indices shape: {total_testing_indices.shape}\")\n",
    "    print(f\"Testing labels shape: {labels_testing.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 650927,  720647,  961651, ...,  502670,  812034,  605121],\n",
       "       [  86023,  600807,  957669, ...,   35772,  938896,  215158],\n",
       "       [ 402697,  711389, 1240295, ...,   72706,  585196,  271946],\n",
       "       ...,\n",
       "       [  20387,  223281,   64746, ...,  558189,   50685,  609182],\n",
       "       [ 944789,   72571,  406632, ...,  676971,  556269,   22966],\n",
       "       [1172955,  492553, 1172522, ...,  363329,   62132,  241743]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_versions[\"version_0\"][\"testing_tt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0:\n",
      "Training neighbors shape: (70, 24000, 2)\n",
      "Testing neighbors shape: (30, 24000, 2)\n",
      "\n",
      "Version 1:\n",
      "Training neighbors shape: (70, 24000, 2)\n",
      "Testing neighbors shape: (30, 24000, 2)\n",
      "\n",
      "Version 2:\n",
      "Training neighbors shape: (70, 24000, 2)\n",
      "Testing neighbors shape: (30, 24000, 2)\n",
      "\n",
      "Version 3:\n",
      "Training neighbors shape: (70, 24000, 2)\n",
      "Testing neighbors shape: (30, 24000, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the neighbor pairs for each version\n",
    "neighbor_pairs_versions = {}\n",
    "\n",
    "# Loop through each version\n",
    "for i in range(len(sample_sizes_train_list)):\n",
    "    # Get the indices for the current version\n",
    "    train_indices_version = indices_versions[f\"version_{i}\"][\"training_indices\"]\n",
    "    test_indices_version = indices_versions[f\"version_{i}\"][\"testing_indices\"]\n",
    "    \n",
    "    # Efficiently index into neighbor_pairs_list for training and testing indices\n",
    "    total_train_neighbor_random = neighbor_pairs_list[train_indices_version]\n",
    "    total_test_neighbor_random = neighbor_pairs_list[test_indices_version]\n",
    "    \n",
    "    # Store the neighbor pairs in the dictionary for each version\n",
    "    neighbor_pairs_versions[f\"version_{i}\"] = {\n",
    "        \"train_neighbors\": total_train_neighbor_random,\n",
    "        \"test_neighbors\": total_test_neighbor_random,\n",
    "    }\n",
    "    \n",
    "    # Print shapes for verification\n",
    "    print(f\"Version {i}:\")\n",
    "    print(f\"Training neighbors shape: {total_train_neighbor_random.shape}\")\n",
    "    print(f\"Testing neighbors shape: {total_test_neighbor_random.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function assists in creating data arrays associated with bi- and\n",
    "# uni-directional arrays\n",
    "def createArray(input_data, num_of_data, is_source, is_bi_directional):\n",
    "    # Initialize an empty list to store the output data\n",
    "    data = []\n",
    "\n",
    "    # Loop through each set of data in input_data\n",
    "    for i in range(num_of_data):\n",
    "        _data = []\n",
    "\n",
    "        # Loop through each pair of data in the current data set\n",
    "        for pair in input_data[i]:\n",
    "\n",
    "            # Process data depending on is_bi_directional flag\n",
    "            if is_bi_directional:\n",
    "                # If is_source is True, append both elements in original order\n",
    "                if is_source:\n",
    "                    _data.append(pair[0])\n",
    "                    _data.append(pair[1])\n",
    "                else:\n",
    "                    # If is_source is False, append elements in reversed order\n",
    "                    _data.append(pair[1])\n",
    "                    _data.append(pair[0])\n",
    "            else:\n",
    "                # If is_bi_directional is False, append only one element depending on is_source flag\n",
    "                if is_source:\n",
    "                    _data.append(pair[0])\n",
    "                else:\n",
    "                    _data.append(pair[1])\n",
    "\n",
    "        # Add the processed data set to the output list\n",
    "        data.append(_data)\n",
    "\n",
    "    # Return the final processed list of data\n",
    "    data = np.array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store bi-/uni-directional arrays for each version\n",
    "neighbor_arrays_versions = {}\n",
    "\n",
    "# Loop through each version\n",
    "for i in range(len(sample_sizes_train_list)):\n",
    "    # Get the neighbor pairs for the current version\n",
    "    total_train_neighbor_random = neighbor_pairs_versions[f\"version_{i}\"][\"train_neighbors\"]\n",
    "    total_test_neighbor_random = neighbor_pairs_versions[f\"version_{i}\"][\"test_neighbors\"]\n",
    "    \n",
    "    # Create bi- and uni-directional arrays for training\n",
    "    train_edge_source_bi = createArray(total_train_neighbor_random, 70, True, True)\n",
    "    train_edge_dest_bi = createArray(total_train_neighbor_random, 70, False, True)\n",
    "    train_edge_source_uni = createArray(total_train_neighbor_random, 70, True, False)\n",
    "    train_edge_dest_uni = createArray(total_train_neighbor_random, 70, False, False)\n",
    "    \n",
    "    # Create bi- and uni-directional arrays for testing\n",
    "    test_edge_source_bi = createArray(total_test_neighbor_random, 30, True, True)\n",
    "    test_edge_dest_bi = createArray(total_test_neighbor_random, 30, False, True)\n",
    "    test_edge_source_uni = createArray(total_test_neighbor_random, 30, True, False)\n",
    "    test_edge_dest_uni = createArray(total_test_neighbor_random, 30, False, False)\n",
    "    \n",
    "    # Store the arrays in the dictionary for each version\n",
    "    neighbor_arrays_versions[f\"version_{i}\"] = {\n",
    "        \"train_edge_source_bi\": train_edge_source_bi,\n",
    "        \"train_edge_dest_bi\": train_edge_dest_bi,\n",
    "        \"train_edge_source_uni\": train_edge_source_uni,\n",
    "        \"train_edge_dest_uni\": train_edge_dest_uni,\n",
    "        \"test_edge_source_bi\": test_edge_source_bi,\n",
    "        \"test_edge_dest_bi\": test_edge_dest_bi,\n",
    "        \"test_edge_source_uni\": test_edge_source_uni,\n",
    "        \"test_edge_dest_uni\": test_edge_dest_uni,\n",
    "    }\n",
    "    \n",
    "    # # Print the shapes of the arrays for verification\n",
    "    # print(f\"Version {i}:\")\n",
    "    # print(f\"train_edge_source_bi shape: {train_edge_source_bi.shape}\")\n",
    "    # print(f\"train_edge_dest_bi shape: {train_edge_dest_bi.shape}\")\n",
    "    # print(f\"train_edge_source_uni shape: {train_edge_source_uni.shape}\")\n",
    "    # print(f\"train_edge_dest_uni shape: {train_edge_dest_uni.shape}\")\n",
    "    # print(f\"test_edge_source_bi shape: {test_edge_source_bi.shape}\")\n",
    "    # print(f\"test_edge_dest_bi shape: {test_edge_dest_bi.shape}\")\n",
    "    # print(f\"test_edge_source_uni shape: {test_edge_source_uni.shape}\")\n",
    "    # print(f\"test_edge_dest_uni shape: {test_edge_dest_uni.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 24000, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbor_pairs_versions['version_0']['test_neighbors'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 24000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 12000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_versions['version_0']['testing_tt'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create edge index tensors with correct permutation\n",
    "def make_edge_index_tensor(source, dest):\n",
    "    source = np.array(source)\n",
    "    dest = np.array(dest)\n",
    "    edge_index = torch.tensor([source, dest], dtype=torch.long)\n",
    "    return edge_index.permute(1, 0, 2)\n",
    "\n",
    "# Function to generate edge arrays for all versions, including training\n",
    "def generate_edge_arrays_for_versions(neighbor_pairs_list, indices_versions, labels_testing, sample_size):\n",
    "    edge_arrays_versions = {}\n",
    "    valid_neighbor_types = ['tt', 'll', 'cl', 'lc', 'cc']\n",
    "\n",
    "    for version_key in indices_versions:\n",
    "        version_name = version_key  \n",
    "        edge_arrays_versions[version_name] = {}\n",
    "\n",
    "        # Handle training edges\n",
    "        train_pairs = [neighbor_pairs_list[i] for i in indices_versions[version_key]['training_indices']]\n",
    "        train_pairs = np.array(train_pairs)\n",
    "\n",
    "        edge_arrays_versions[version_name]['train'] = {\n",
    "            \"source_bi\": createArray(train_pairs, sample_size, True, True),\n",
    "            \"dest_bi\": createArray(train_pairs, sample_size, False, True),\n",
    "            \"source_uni\": createArray(train_pairs, sample_size, True, False),\n",
    "            \"dest_uni\": createArray(train_pairs, sample_size, False, False),\n",
    "        }\n",
    "\n",
    "        # Handle testing edges by neighbor type\n",
    "        test_neighbor_pairs = {ntype: [] for ntype in valid_neighbor_types}\n",
    "\n",
    "        for neighbor_type in valid_neighbor_types:\n",
    "            indices = indices_versions[version_key].get(f'testing_{neighbor_type}')\n",
    "            if indices is not None:\n",
    "                for i in range(len(labels_testing)):\n",
    "                    test_neighbor_pairs[neighbor_type].append(neighbor_pairs_list[indices[i]])\n",
    "                test_neighbor_pairs[neighbor_type] = np.array(test_neighbor_pairs[neighbor_type])\n",
    "\n",
    "        # Generate edge arrays for each neighbor type in testing\n",
    "        for neighbor_type, pairs in test_neighbor_pairs.items():\n",
    "            edge_arrays_versions[version_name][neighbor_type] = {\n",
    "                \"source_bi\": createArray(pairs, sample_size, True, True),\n",
    "                \"dest_bi\": createArray(pairs, sample_size, False, True),\n",
    "                \"source_uni\": createArray(pairs, sample_size, True, False),\n",
    "                \"dest_uni\": createArray(pairs, sample_size, False, False),\n",
    "            }\n",
    "\n",
    "    return edge_arrays_versions\n",
    "\n",
    "# Generate edge arrays for all versions (training + testing)\n",
    "edge_arrays_versions = generate_edge_arrays_for_versions(neighbor_pairs_list, indices_versions, labels_testing, sample_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2913903/2626518317.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  edge_index = torch.tensor([source, dest], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store edge index tensors for all versions\n",
    "edge_indices_versions = {}\n",
    "\n",
    "for version in edge_arrays_versions:\n",
    "    edge_indices_versions[version] = {}\n",
    "\n",
    "    # Process training data\n",
    "    edge_indices_versions[version]['train'] = {\n",
    "        \"bi\": make_edge_index_tensor(edge_arrays_versions[version]['train'][\"source_bi\"],\n",
    "                                     edge_arrays_versions[version]['train'][\"dest_bi\"]),\n",
    "        \"uni\": make_edge_index_tensor(edge_arrays_versions[version]['train'][\"source_uni\"],\n",
    "                                      edge_arrays_versions[version]['train'][\"dest_uni\"])\n",
    "    }\n",
    "\n",
    "    # Process testing data by neighbor type\n",
    "    for neighbor_type in edge_arrays_versions[version]:\n",
    "        if neighbor_type == 'train':  # Skip training since it's already processed\n",
    "            continue\n",
    "        \n",
    "        edge_indices_versions[version][neighbor_type] = {\n",
    "            \"bi\": make_edge_index_tensor(edge_arrays_versions[version][neighbor_type][\"source_bi\"],\n",
    "                                         edge_arrays_versions[version][neighbor_type][\"dest_bi\"]),\n",
    "            \"uni\": make_edge_index_tensor(edge_arrays_versions[version][neighbor_type][\"source_uni\"],\n",
    "                                          edge_arrays_versions[version][neighbor_type][\"dest_uni\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'tt', 'll', 'cl', 'lc', 'cc'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_arrays_versions['version_0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices_versions = {\n",
    "#     'version_0': {\n",
    "#         'training_indices': indices_versions['version_0']['training_indices'],  # Replace with your actual training indices\n",
    "#         'testing_indices': indices_versions['version_0']['testing_indices'],  # Replace with your actual testing indices\n",
    "#         'testing_tt': indices_versions['version_0']['testing_tt'],  # Indices for version 0 for testing_tt\n",
    "#         'testing_ll': indices_versions['version_0']['testing_ll'],  # Indices for version 0 for testing_ll\n",
    "#         'testing_cl': indices_versions['version_0']['testing_cl'],  # Indices for version 0 for testing_cl\n",
    "#         'testing_lc': indices_versions['version_0']['testing_lc'],  # Indices for version 0 for testing_lc\n",
    "#         'testing_cc': indices_versions['version_0']['testing_cc'],  # Indices for version 0 for testing_cc\n",
    "#     },\n",
    "#     'version_1': {\n",
    "#         'training_indices': indices_versions['version_1']['training_indices'],  \n",
    "#         'testing_indices': indices_versions['version_1']['testing_indices'], \n",
    "#         'testing_tt': indices_versions['version_1']['testing_tt'], \n",
    "#         'testing_ll': indices_versions['version_1']['testing_ll'], \n",
    "#         'testing_cl': indices_versions['version_1']['testing_cl'],\n",
    "#         'testing_lc': indices_versions['version_1']['testing_lc'],\n",
    "#         'testing_cc': indices_versions['version_1']['testing_cc'],\n",
    "#     },\n",
    "#     'version_2': {\n",
    "#         'training_indices': indices_versions['version_2']['training_indices'],  \n",
    "#         'testing_indices': indices_versions['version_2']['testing_indices'], \n",
    "#         'testing_tt': indices_versions['version_2']['testing_tt'], \n",
    "#         'testing_ll': indices_versions['version_2']['testing_ll'], \n",
    "#         'testing_cl': indices_versions['version_2']['testing_cl'],\n",
    "#         'testing_lc': indices_versions['version_2']['testing_lc'],\n",
    "#         'testing_cc': indices_versions['version_2']['testing_cc'],\n",
    "#     },\n",
    "#     'version_3': {\n",
    "#         'training_indices': indices_versions['version_3']['training_indices'],  \n",
    "#         'testing_indices': indices_versions['version_3']['testing_indices'], \n",
    "#         'testing_tt': indices_versions['version_3']['testing_tt'], \n",
    "#         'testing_ll': indices_versions['version_3']['testing_ll'], \n",
    "#         'testing_cl': indices_versions['version_3']['testing_cl'],\n",
    "#         'testing_lc': indices_versions['version_3']['testing_lc'],\n",
    "#         'testing_cc': indices_versions['version_3']['testing_cc'],\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate edge arrays for all versions\n",
    "# edge_arrays_versions = generate_edge_arrays_for_versions(neighbor_pairs_list, indices_versions, labels_testing, sample_size=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0, TT Pair Edge Arrays:\n",
      "Source Bi-directional: (30, 24000)\n",
      "Destination Bi-directional: (30, 24000)\n",
      "Source Uni-directional: (30, 12000)\n",
      "Destination Uni-directional: (30, 12000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Version 0, TT Pair Edge Arrays:\")\n",
    "print(f\"Source Bi-directional: {edge_arrays_versions['version_0']['tt']['source_bi'].shape}\")\n",
    "print(f\"Destination Bi-directional: {edge_arrays_versions['version_0']['tt']['dest_bi'].shape}\")\n",
    "print(f\"Source Uni-directional: {edge_arrays_versions['version_0']['tt']['source_uni'].shape}\")\n",
    "print(f\"Destination Uni-directional: {edge_arrays_versions['version_0']['tt']['dest_uni'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0, TT Pair Edge Index Tensors:\n",
      "Bi-directional: torch.Size([30, 2, 24000])\n",
      "Uni-directional: torch.Size([30, 2, 12000])\n"
     ]
    }
   ],
   "source": [
    "# Print shapes for Version 0 as an example\n",
    "print(\"Version 0, TT Pair Edge Index Tensors:\")\n",
    "print(f\"Bi-directional: {edge_indices_versions['version_0']['tt']['bi'].shape}\")\n",
    "print(f\"Uni-directional: {edge_indices_versions['version_0']['tt']['uni'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the features array for training and testing\n",
    "\n",
    "# Here we create a rearranged dictionary from the scaled data dictionary\n",
    "keys = list(scaled_data.keys())\n",
    "values = list(scaled_data.values())\n",
    "features_dict = dict(zip(keys, values))\n",
    "\n",
    "# Here these features are split into training and testing sets\n",
    "features_training = np.concatenate([value for key, value in list(features_dict.items())[:70]])\n",
    "features_testing = np.concatenate([value for key, value in list(features_dict.items())[70:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13135640, 3)\n",
      "(5629560, 3)\n"
     ]
    }
   ],
   "source": [
    "print(features_training.shape)\n",
    "print(features_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training = features_training.reshape(70, 187652, 3)\n",
    "features_testing = features_testing.reshape(30, 187652, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the scaled features into a torch tensor (inputs)\n",
    "x_train = torch.tensor(features_training, dtype=torch.float)\n",
    "x_test = torch.tensor(features_testing, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here the currect dimension permutations are applied for the model\n",
    "# def make_edge_index_tensor(source, dest):\n",
    "#     source = np.array(source)\n",
    "#     dest = np.array(dest)\n",
    "#     edge_index = torch.tensor([source, dest], dtype=torch.long)\n",
    "#     return edge_index.permute(1, 0, 2)\n",
    "\n",
    "# # Training set (Bi-directional and Uni-directional)\n",
    "# train_edge_indices_bi = make_edge_index_tensor(train_edge_source_bi, train_edge_dest_bi)\n",
    "# train_edge_indices_uni = make_edge_index_tensor(train_edge_source_uni, train_edge_dest_uni)\n",
    "# print(train_edge_indices_bi.shape)\n",
    "# print(train_edge_indices_uni.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge index tensor shapes for version_0:\n",
      "train_bi: torch.Size([30, 2, 48000])\n",
      "train_uni: torch.Size([30, 2, 24000])\n",
      "tt_bi: torch.Size([30, 2, 24000])\n",
      "tt_uni: torch.Size([30, 2, 12000])\n",
      "ll_bi: torch.Size([30, 2, 6000])\n",
      "ll_uni: torch.Size([30, 2, 3000])\n",
      "cl_bi: torch.Size([30, 2, 6000])\n",
      "cl_uni: torch.Size([30, 2, 3000])\n",
      "lc_bi: torch.Size([30, 2, 6000])\n",
      "lc_uni: torch.Size([30, 2, 3000])\n",
      "cc_bi: torch.Size([30, 2, 6000])\n",
      "cc_uni: torch.Size([30, 2, 3000])\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store edge index tensors for all versions\n",
    "edge_indices_versions_generalized = {}\n",
    "\n",
    "# Iterate over each version\n",
    "for version in edge_indices_versions:\n",
    "    edge_indices_versions_generalized[version] = {}\n",
    "\n",
    "    # Iterate over each neighbor type (tt, ll, cl, lc, cc)\n",
    "    for neighbor_type in edge_indices_versions[version]:\n",
    "        \n",
    "        # Get bi-directional and uni-directional tensors\n",
    "        bi_tensor = edge_indices_versions[version][neighbor_type][\"bi\"]\n",
    "        uni_tensor = edge_indices_versions[version][neighbor_type][\"uni\"]\n",
    "        \n",
    "        # Store them with standardized keys\n",
    "        edge_indices_versions_generalized[version][f\"{neighbor_type}_bi\"] = bi_tensor\n",
    "        edge_indices_versions_generalized[version][f\"{neighbor_type}_uni\"] = uni_tensor\n",
    "\n",
    "# Example: Print edge index tensor shapes for version_0\n",
    "print(\"Edge index tensor shapes for version_0:\")\n",
    "for key, tensor in edge_indices_versions_generalized[\"version_0\"].items():\n",
    "    print(f\"{key}: {tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_index_data = {\n",
    "#     \"tt_bi\": (test_edge_tt_source_bi, test_edge_tt_dest_bi),\n",
    "#     \"tt_uni\": (test_edge_tt_source_uni, test_edge_tt_dest_uni),\n",
    "#     \"ll_bi\": (test_edge_ll_source_bi, test_edge_ll_dest_bi),\n",
    "#     \"ll_uni\": (test_edge_ll_source_uni, test_edge_ll_dest_uni),\n",
    "#     \"cl_bi\": (test_edge_cl_source_bi, test_edge_cl_dest_bi),\n",
    "#     \"cl_uni\": (test_edge_cl_source_uni, test_edge_cl_dest_uni),\n",
    "#     \"lc_bi\": (test_edge_lc_source_bi, test_edge_lc_dest_bi),\n",
    "#     \"lc_uni\": (test_edge_lc_source_uni, test_edge_lc_dest_uni),\n",
    "#     \"cc_bi\": (test_edge_cc_source_bi, test_edge_cc_dest_bi),\n",
    "#     \"cc_uni\": (test_edge_cc_source_uni, test_edge_cc_dest_uni),\n",
    "# }\n",
    "\n",
    "# # Create and permute tensors for all edge types\n",
    "# edge_indices = {key: make_edge_index_tensor(\n",
    "#     sources, dests) for key, (sources, dests) in edge_index_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 1, 24000])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.expand_dims(labels_training, axis=1)\n",
    "y_train = torch.tensor(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.expand_dims(labels_testing, axis=1)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of custom data lists, collate functions, and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class that inherents from the torch.utils.data.Dataset class\n",
    "# The pytorch class is abstract, meaning we need to define certain methods\n",
    "# like __len__() and __getitem__()\n",
    "class custom_dataset(torch.utils.data.Dataset):\n",
    "    # Class constructor that takes in data list and\n",
    "    # stores it as an instance, making it avaliable\n",
    "    # to other methods in the class\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    # Method return length of data set\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    # Method returns data point at index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "# Used to handle batch loading, shuffling, and parallel loading during\n",
    "# training and testing in the ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with information regarding a homogenous graph (a graph\n",
    "# where all nodes represent instances of the same type [cells in the\n",
    "# detector] and all edges represent relations of the same type [connections\n",
    "# between cells])\n",
    "def create_data_list(bi_edge_indices, uni_edge_indices, x, y):\n",
    "    data_list = []\n",
    "    for i in range(len(bi_edge_indices)):\n",
    "        # Create the feature matrix\n",
    "        x_mat = x[i]\n",
    "        # Create graph connectivity matrix\n",
    "        edge_index = bi_edge_indices[i]\n",
    "        edge_index, _ = add_self_loops(edge_index)\n",
    "\n",
    "        # Convert y[i] to a PyTorch tensor\n",
    "        y_tensor = torch.tensor(y[i], dtype=torch.long) if not isinstance(\n",
    "            y[i], torch.Tensor) else y[i]\n",
    "\n",
    "        # Create the data object describing a homogeneous graph\n",
    "        data = Data(x=x_mat, edge_index=edge_index,\n",
    "                    edge_index_out=uni_edge_indices[i], y=y_tensor)\n",
    "        data = ToUndirected()(data)\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def collate_data(data_list):\n",
    "    return ([data.x for data in data_list],\n",
    "            [data.edge_index for data in data_list],\n",
    "            [data.edge_index_out for data in data_list],\n",
    "            torch.cat([data.y for data in data_list], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_bi', 'train_uni', 'tt_bi', 'tt_uni', 'll_bi', 'll_uni', 'cl_bi', 'cl_uni', 'lc_bi', 'lc_uni', 'cc_bi', 'cc_uni'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_indices_versions_generalized['version_0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_versions['version_0']['testing_tt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the data lists for all edge types and categories\n",
    "\n",
    "# data_list_train0 = create_data_list(edge_indices_versions['version_0']['train']['bi'], \n",
    "#                                     edge_indices_versions['version_0']['train']['uni'], \n",
    "#                                     x_train, \n",
    "#                                     y_train)  # Training Edges\n",
    "\n",
    "# data_list_tt0 = create_data_list(edge_indices_versions_generalized['version_0']['tt_bi'], \n",
    "#                                  edge_indices_versions_generalized['version_0']['tt_uni'], \n",
    "#                                  x_test, \n",
    "#                                  np.expand_dims(labels_versions['version_0']['testing_tt'], axis=1))  # True-True Edges\n",
    "\n",
    "# data_list_ll0 = create_data_list(edge_indices_versions_generalized['version_0']['ll_bi'], \n",
    "#                                  edge_indices_versions_generalized['version_0']['ll_uni'], \n",
    "#                                  x_test, \n",
    "#                                  np.expand_dims(labels_versions['version_0']['testing_ll'], axis=1))  # Lone-lone Edges\n",
    "\n",
    "# data_list_cl0 = create_data_list(edge_indices_versions_generalized['version_0']['cl_bi'], \n",
    "#                                  edge_indices_versions_generalized['version_0']['cl_uni'], \n",
    "#                                  x_test, \n",
    "#                                  np.expand_dims(labels_versions['version_0']['testing_cl'], axis=1))  # Cluster-Lone Edges\n",
    "\n",
    "# data_list_lc0 = create_data_list(edge_indices_versions_generalized['version_0']['lc_bi'],\n",
    "#                                   edge_indices_versions_generalized['version_0']['lc_uni'],\n",
    "#                                     x_test, \n",
    "#                                     np.expand_dims(labels_versions['version_0']['testing_lc'], axis=1))  # Lone-Cluster Edges\n",
    "\n",
    "# data_list_cc0 = create_data_list(edge_indices_versions_generalized['version_0']['cc_bi'], \n",
    "#                                  edge_indices_versions_generalized['version_0']['cc_uni'], \n",
    "#                                  x_test, \n",
    "#                                  np.expand_dims(labels_versions['version_0']['testing_cc'], axis=1))  # Cluster-Cluster Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store data lists for all versions\n",
    "data_lists_versions = {}\n",
    "\n",
    "# Define all neighbor types\n",
    "neighbor_types = ['tt', 'll', 'cl', 'lc', 'cc']\n",
    "\n",
    "# Iterate over all versions\n",
    "for version in ['version_0', 'version_1', 'version_2', 'version_3']:\n",
    "    data_lists_versions[version] = {}\n",
    "\n",
    "    # Training edges\n",
    "    data_lists_versions[version]['train'] = create_data_list(\n",
    "        edge_indices_versions[version]['train']['bi'], \n",
    "        edge_indices_versions[version]['train']['uni'], \n",
    "        x_train, \n",
    "        y_train\n",
    "    )\n",
    "\n",
    "    # Testing edges for each neighbor type\n",
    "    for neighbor_type in neighbor_types:\n",
    "        data_lists_versions[version][neighbor_type] = create_data_list(\n",
    "            edge_indices_versions[version][neighbor_type]['bi'], \n",
    "            edge_indices_versions[version][neighbor_type]['uni'], \n",
    "            x_test, \n",
    "            np.expand_dims(labels_versions[version][f'testing_{neighbor_type}'], axis=1)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[187652, 3], edge_index=[2, 235637], y=[1, 24000], edge_index_out=[2, 24000])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lists_versions['version_0']['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Batch size value\n",
    "# batch_size = 1\n",
    "\n",
    "# # Create the data loaders\n",
    "# data_loader = {}\n",
    "# data_list_mapping = {\n",
    "#     \"train\": data_list_train,  # Training Edges\n",
    "#     \"tt\": data_list_tt,           # True-True Edges\n",
    "#     \"ll\": data_list_ll,           # Lone-Lone Edges\n",
    "#     \"lc\": data_list_lc,           # Lone-Cluster Edges\n",
    "#     \"cl\": data_list_cl,           # Cluster-Lone Edges\n",
    "#     \"cc\": data_list_cc            # Cluster-Cluster Edges\n",
    "# }\n",
    "\n",
    "# # Total background dataset\n",
    "# data_list_total_bkg = data_list_ll + data_list_cl + data_list_lc + data_list_cc\n",
    "# data_loader_total_bkg = torch.utils.data.DataLoader(\n",
    "#     custom_dataset(data_list_total_bkg),\n",
    "#     batch_size=batch_size,\n",
    "#     collate_fn=lambda batch: collate_data(batch)\n",
    "# )\n",
    "# # For the other datasets\n",
    "# for key, data_list in data_list_mapping.items():\n",
    "#     data_loader[key] = torch.utils.data.DataLoader(\n",
    "#         custom_dataset(data_list),\n",
    "#         batch_size=batch_size,\n",
    "#         collate_fn=lambda batch: collate_data(batch)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 1\n",
    "\n",
    "# Dictionary to store data loaders for all versions\n",
    "data_loaders_versions = {}\n",
    "\n",
    "# Iterate over all versions\n",
    "for version in ['version_0', 'version_1', 'version_2', 'version_3']:\n",
    "    data_loaders_versions[version] = {}\n",
    "\n",
    "    # Define dataset for each category\n",
    "    data_list_mapping = {\n",
    "        \"train\": data_lists_versions[version]['train'],  # Training Edges\n",
    "        \"tt\": data_lists_versions[version]['tt'],       # True-True Edges\n",
    "        \"ll\": data_lists_versions[version]['ll'],       # Lone-Lone Edges\n",
    "        \"lc\": data_lists_versions[version]['lc'],       # Lone-Cluster Edges\n",
    "        \"cl\": data_lists_versions[version]['cl'],       # Cluster-Lone Edges\n",
    "        \"cc\": data_lists_versions[version]['cc']        # Cluster-Cluster Edges\n",
    "    }\n",
    "\n",
    "    # Create background dataset (sum of all background types)\n",
    "    data_list_total_bkg = (\n",
    "        data_lists_versions[version]['ll'] + \n",
    "        data_lists_versions[version]['cl'] + \n",
    "        data_lists_versions[version]['lc'] + \n",
    "        data_lists_versions[version]['cc']\n",
    "    )\n",
    "\n",
    "    # Create data loader for total background dataset\n",
    "    data_loaders_versions[version]['total_bkg'] = torch.utils.data.DataLoader(\n",
    "        custom_dataset(data_list_total_bkg),\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=lambda batch: collate_data(batch)\n",
    "    )\n",
    "\n",
    "    # Create data loaders for all other datasets\n",
    "    for key, data_list in data_list_mapping.items():\n",
    "        data_loaders_versions[version][key] = torch.utils.data.DataLoader(\n",
    "            custom_dataset(data_list),\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=lambda batch: collate_data(batch)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Model, Weights, and Functions to Train and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model without learnable layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiEdgeClassifier(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, num_layers, debug=False):\n",
    "#         super(MultiEdgeClassifier, self).__init__()\n",
    "#         # Set the debug mode\n",
    "#         self.debug = debug\n",
    "\n",
    "#         # Node embedding layer\n",
    "#         self.node_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "#         # Initialize first convolution and batch norm layers\n",
    "#         self.convs = nn.ModuleList()\n",
    "#         self.bns = nn.ModuleList()\n",
    "#         self.convs.append(GCNConv(hidden_dim, 128))\n",
    "#         self.bns.append(BatchNorm1d(128))\n",
    "\n",
    "#         # Additional conv and bn layers based on 'num_layers' param\n",
    "#         for i in range(1, num_layers):\n",
    "#             in_channels = 128 if i == 1 else 64\n",
    "#             out_channels = 64\n",
    "#             self.convs.append(GCNConv(in_channels, out_channels))\n",
    "#             self.bns.append(BatchNorm1d(out_channels))\n",
    "\n",
    "#         # Edge classification layer\n",
    "#         self.fc = nn.Linear(128, output_dim)\n",
    "\n",
    "#     def debug_print(self, message):\n",
    "#         if self.debug:\n",
    "#             print(message)\n",
    "\n",
    "#     def forward(self, x, edge_index, edge_index_out):\n",
    "#         # Node embedding\n",
    "#         x = self.node_embedding(x)\n",
    "#         self.debug_print(f\"Node embedding output shape: {x.shape}\")\n",
    "\n",
    "#         if x.dim() == 3 and x.size(0) == 1:  # Check and remove batch dimension\n",
    "#             x = x.squeeze(0)\n",
    "\n",
    "#         # Loop through convolution layers\n",
    "#         for i, conv in enumerate(self.convs):\n",
    "#             x = conv(x, edge_index)\n",
    "#             self.debug_print(f\"After GCNConv {i+1}: {x.shape}\")\n",
    "#             if x.dim() == 3 and x.size(0) == 1:  # Check and remove batch dimension\n",
    "#                 x = x.squeeze(0)\n",
    "#             x = self.bns[i](x)\n",
    "#             x = torch.relu(x)\n",
    "#             self.debug_print(f\"After BatchNorm {i+1}: {x.shape}\")\n",
    "\n",
    "#         # Edge representations\n",
    "#         edge_rep = torch.cat(\n",
    "#             [x[edge_index_out[0]], x[edge_index_out[1]]], dim=1)\n",
    "#         self.debug_print(f\"Edge representation shape: {edge_rep.shape}\")\n",
    "\n",
    "#         # Return Logits\n",
    "#         edge_scores = self.fc(edge_rep)\n",
    "#         return edge_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with learnable layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have different weights for the different layers\n",
    "\n",
    "class MultiEdgeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, debug=False):\n",
    "        super(MultiEdgeClassifier, self).__init__()\n",
    "        # Set the debug mode\n",
    "        self.debug = debug\n",
    "\n",
    "        # Node embedding layer\n",
    "        self.node_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Initialize first convolution and batch norm layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        # List to store learnable weights for each layer\n",
    "        self.layer_weights = nn.ParameterList()\n",
    "\n",
    "        self.convs.append(GCNConv(hidden_dim, 128))\n",
    "        self.bns.append(BatchNorm1d(128))\n",
    "        self.layer_weights.append(nn.Parameter(torch.tensor(\n",
    "            1.0, requires_grad=True)))  # Weight for layer 1\n",
    "\n",
    "        # Additional conv and bn layers based on 'num_layers' param\n",
    "        for i in range(1, num_layers):\n",
    "            in_channels = 128 if i == 1 else 64\n",
    "            out_channels = 64\n",
    "            self.convs.append(GCNConv(in_channels, out_channels))\n",
    "            self.bns.append(BatchNorm1d(out_channels))\n",
    "            self.layer_weights.append(nn.Parameter(torch.tensor(\n",
    "                1.0, requires_grad=True)))  # Weight for each layer\n",
    "\n",
    "        # Edge classification layer\n",
    "        self.fc = nn.Linear(128, output_dim)\n",
    "\n",
    "    def debug_print(self, message):\n",
    "        if self.debug:\n",
    "            print(message)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_index_out):\n",
    "        # Node embedding\n",
    "        x = self.node_embedding(x)\n",
    "        self.debug_print(f\"Node embedding output shape: {x.shape}\")\n",
    "\n",
    "        if x.dim() == 3 and x.size(0) == 1:  # Check and remove batch dimension\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "        # Loop through convolution layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            self.debug_print(f\"After GCNConv {i+1}: {x.shape}\")\n",
    "            if x.dim() == 3 and x.size(0) == 1:  # Check and remove batch dimension\n",
    "                x = x.squeeze(0)\n",
    "            x = self.bns[i](x)\n",
    "            x = torch.relu(x)\n",
    "            # Multiply output of each layer by its corresponding weight\n",
    "            x = x * self.layer_weights[i]\n",
    "            self.debug_print(f\"After Layer Weight {i+1}: {x.shape}\")\n",
    "\n",
    "        # Edge representations\n",
    "        edge_rep = torch.cat(\n",
    "            [x[edge_index_out[0]], x[edge_index_out[1]]], dim=1)\n",
    "        self.debug_print(f\"Edge representation shape: {edge_rep.shape}\")\n",
    "\n",
    "        # Return Logits\n",
    "        edge_scores = self.fc(edge_rep)\n",
    "        return edge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = 3\n",
    "hidden_dim = 256\n",
    "output_dim = 5  # Multiclass classification\n",
    "num_layers = 5\n",
    "\n",
    "# Assigning different GPUs for different versions\n",
    "devices = {\n",
    "    'version_0': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'version_1': torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'version_2': torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'version_3': torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "# Creating models for each version on its respective device\n",
    "models = {\n",
    "    version: MultiEdgeClassifier(input_dim, hidden_dim, output_dim, num_layers).to(devices[version])\n",
    "    for version in devices\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten labels for training\n",
    "labels_training_flat = labels_training.flatten()\n",
    "\n",
    "# Compute class frequencies\n",
    "train_label_counts = Counter(labels_training_flat)\n",
    "\n",
    "# Total number of training samples\n",
    "total_train_samples = labels_training_flat.size\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = {label: total_train_samples / count for label, count in train_label_counts.items()}\n",
    "\n",
    "# Normalize class weights\n",
    "total_weight = sum(class_weights.values())\n",
    "normalized_class_weights = {label: weight / total_weight for label, weight in class_weights.items()}\n",
    "\n",
    "# Convert weights to tensors (for each version on the correct device)\n",
    "unique_classes = np.unique(labels_training_flat)\n",
    "weight_tensors = {\n",
    "    version: torch.tensor([class_weights[label] for label in unique_classes], dtype=torch.float).to(devices[version])\n",
    "    for version in devices\n",
    "}\n",
    "normalized_weight_tensors = {\n",
    "    version: torch.tensor([normalized_class_weights[label] for label in unique_classes], dtype=torch.float).to(devices[version])\n",
    "    for version in devices\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a modified version of cross entropy loss by Lin et al. 2017\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Apply softmax to logits to get probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Get the probabilities of the true class\n",
    "        targets_one_hot = F.one_hot(\n",
    "            targets, num_classes=logits.size(1)).float()\n",
    "        probs = probs * targets_one_hot\n",
    "        probs = probs.sum(dim=1)  # Shape: (batch_size,)\n",
    "\n",
    "        # Compute the focal loss components\n",
    "        log_probs = torch.log(probs + 1e-8)  # Add epsilon to avoid log(0)\n",
    "        focal_weights = (1 - probs) ** self.gamma\n",
    "\n",
    "        # Apply class weights (if provided)\n",
    "        if self.alpha is not None:\n",
    "            alpha_weights = self.alpha[targets]  # Shape: (batch_size,)\n",
    "            focal_loss = -alpha_weights * focal_weights * log_probs\n",
    "        else:\n",
    "            focal_loss = -focal_weights * log_probs\n",
    "\n",
    "        # Reduce the loss based on the reduction method\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:  # 'none'\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of possible losses:\n",
    "# 1. nn.CrossEntropyLoss(weight=weight_tensors['version_something that matches the version number'])\n",
    "# 2. nn.CrossEntropyLoss(weight=normalized_weight_tensors['version_something that matches the version number'])\n",
    "# 3. FocalLoss(alpha=weight_tensors['version_3'], gamma=2.0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "# Loss functions per version\n",
    "criterions = {\n",
    "    'version_0': nn.CrossEntropyLoss(),\n",
    "    'version_1': nn.CrossEntropyLoss(),\n",
    "    'version_2': nn.CrossEntropyLoss(),\n",
    "    'version_3': nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "# Optimizers per version\n",
    "optimizers = {\n",
    "    version: optim.Adam(models[version].parameters(), lr=0.1)\n",
    "    for version in devices\n",
    "}\n",
    "\n",
    "# Learning rate schedulers per version\n",
    "schedulers = {\n",
    "    version: torch.optim.lr_scheduler.ExponentialLR(optimizers[version], gamma=0.99)\n",
    "    for version in devices\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I HAVE STOPED HERE, GET THIS FIXED BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, data_loader, optimizer, criterion):\n",
    "    # Sets the model into training mode\n",
    "    model.train()\n",
    "    # Sends model to GPU if available, otherwise uses the CPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Assumes there is only one batch in the data loader\n",
    "    # Retrieve the single batch from the data loader\n",
    "    batch_x, batch_edge_index, batch_edge_index_out, batch_y = next(iter(data_loader))\n",
    "\n",
    "    # Sends the input features, the edge indices, and target\n",
    "    # labels to the GPU if available, otherwise the CPU\n",
    "    batch_x = torch.stack(batch_x).to(device)\n",
    "    batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "    batch_edge_index_out = [edge_index.to(device) for edge_index in batch_edge_index_out]\n",
    "\n",
    "    # Convert target labels to LongTensor (torch.int64)\n",
    "    batch_y = [y.long().to(device) for y in batch_y]\n",
    "\n",
    "    # Clears the gradients of the model parameters to ensure\n",
    "    # they are not accumulated across batches\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss tracking for subgraphs in the single batch\n",
    "    loss_per_batch = []\n",
    "\n",
    "    # Model processes each graph in the batch one by one\n",
    "    for i in range(len(batch_edge_index)):\n",
    "        # Pass the features and the edge indices into the model and store\n",
    "        # the output (logits)\n",
    "        _output = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "\n",
    "        # Ensure that model outputs (logits) are of type float32\n",
    "        _output = _output.float()\n",
    "\n",
    "        # Calculate the difference between the model output and the targets\n",
    "        # via the provided criterion (loss function)\n",
    "        loss = criterion(_output.squeeze(), batch_y[i].squeeze())\n",
    "\n",
    "        # This difference is stored in the loss_per_batch list\n",
    "        loss_per_batch.append(loss)\n",
    "\n",
    "    # The average loss across all subgraphs within the single batch is calculated\n",
    "    total_loss_per_batch = sum(loss_per_batch) / len(loss_per_batch)\n",
    "\n",
    "    # Computes the loss gradients with respect to the model parameters\n",
    "    total_loss_per_batch.backward()\n",
    "\n",
    "    # Updates the model parameters using the gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Returns the total loss for the single batch\n",
    "    return total_loss_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, device, data_loader_true, data_loader_bkg_dict):\n",
    "    all_scores = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        # Process true edges (positive class, label 1)\n",
    "        batch_x, batch_edge_index, batch_edge_index_out, batch_y = next(iter(data_loader_true))\n",
    "        batch_x = torch.stack(batch_x).to(device)\n",
    "        batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "        batch_edge_index_out = [edge_index_out.to(device) for edge_index_out in batch_edge_index_out]\n",
    "\n",
    "        for i in range(len(batch_edge_index)):\n",
    "            test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "            test_edge_scores = F.softmax(test_edge_scores, dim=1)\n",
    "\n",
    "            all_scores.append(test_edge_scores)\n",
    "            true_labels.append(torch.ones(test_edge_scores.size(0), dtype=torch.long, device=device))\n",
    "\n",
    "        # Process background edges\n",
    "        for background_type, data_loader_bkg in data_loader_bkg_dict.items():\n",
    "            batch_x, batch_edge_index, batch_edge_index_out, batch_y = next(iter(data_loader_bkg))\n",
    "            batch_x = torch.stack(batch_x).to(device)\n",
    "            batch_edge_index = [edge_index.to(device) for edge_index in batch_edge_index]\n",
    "            batch_edge_index_out = [edge_index_out.to(device) for edge_index_out in batch_edge_index_out]\n",
    "\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                test_edge_scores = model(batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                test_edge_scores = F.softmax(test_edge_scores, dim=1)\n",
    "\n",
    "                all_scores.append(test_edge_scores)\n",
    "                true_labels.append(torch.full((test_edge_scores.size(0),), background_type, dtype=torch.long, device=device))\n",
    "\n",
    "    # Concatenate all scores and labels\n",
    "    all_scores = torch.cat(all_scores, dim=0)\n",
    "    true_labels = torch.cat(true_labels, dim=0)\n",
    "\n",
    "    # Reorder scores and labels to match the desired order (0, 1, 2, 3, 4)\n",
    "    desired_order = [0, 1, 2, 3, 4]\n",
    "    # Ensure reordering matches desired labels\n",
    "    mask = torch.argsort(true_labels).argsort()\n",
    "    all_scores = all_scores[mask]\n",
    "    true_labels = true_labels[mask]\n",
    "\n",
    "    return all_scores.cpu().numpy(), true_labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_for_train_and_test(model, loader, loss_fn, optimizer, training, device):\n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    all_logits = []  # Store raw logits in testing mode\n",
    "\n",
    "    for batch in loader:\n",
    "        if training:\n",
    "            batch_x, batch_edge_index, batch_edge_index_out, batch_y = batch\n",
    "            batch_y = batch_y.to(device)\n",
    "        else:\n",
    "            batch_x, batch_edge_index, batch_edge_index_out, *_ = batch\n",
    "\n",
    "        # Move features and edge indices to the device\n",
    "        batch_x = torch.stack(batch_x).to(device)\n",
    "        batch_edge_index = [edge_index.to(device)for edge_index in batch_edge_index]\n",
    "        batch_edge_index_out = [edge_index_out.to(\n",
    "            device) for edge_index_out in batch_edge_index_out]\n",
    "\n",
    "        if training:\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                logits = model(\n",
    "                    batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(logits, batch_y[i])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(batch_edge_index)):\n",
    "                    logits = model(\n",
    "                        batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                    all_logits.append(logits)  # Store raw logits\n",
    "                    num_batches += 1\n",
    "\n",
    "    average_loss = total_loss / num_batches if training else None\n",
    "    all_logits = torch.cat(all_logits, dim=0).cpu().numpy() if all_logits else None\n",
    "\n",
    "    return average_loss, all_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping background data loaders to the correct labels per version\n",
    "data_loader_bkg_dict = {\n",
    "    'version_0': {\n",
    "        0: data_loaders_versions['version_0']['ll'],  # Label 0\n",
    "        2: data_loaders_versions['version_0']['cl'],  # Label 2\n",
    "        3: data_loaders_versions['version_0']['lc'],  # Label 3\n",
    "        4: data_loaders_versions['version_0']['cc']   # Label 4\n",
    "    },\n",
    "    'version_1': {\n",
    "        0: data_loaders_versions['version_1']['ll'],\n",
    "        2: data_loaders_versions['version_1']['cl'],\n",
    "        3: data_loaders_versions['version_1']['lc'],\n",
    "        4: data_loaders_versions['version_1']['cc']\n",
    "    },\n",
    "    'version_2': {\n",
    "        0: data_loaders_versions['version_2']['ll'],\n",
    "        2: data_loaders_versions['version_2']['cl'],\n",
    "        3: data_loaders_versions['version_2']['lc'],\n",
    "        4: data_loaders_versions['version_2']['cc']\n",
    "    },\n",
    "    'version_3': {\n",
    "        0: data_loaders_versions['version_3']['ll'],\n",
    "        2: data_loaders_versions['version_3']['cl'],\n",
    "        3: data_loaders_versions['version_3']['lc'],\n",
    "        4: data_loaders_versions['version_3']['cc']\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss version_0: 1.6575 | Loss version_1: 2.1245 | Loss version_2: 1.6683 | Loss version_3: 1.6496\n",
      "Epoch: 2 | Loss version_0: 1.3865 | Loss version_1: 1.3863 | Loss version_2: 1.3865 | Loss version_3: 1.2908\n",
      "Epoch: 3 | Loss version_0: 1.3863 | Loss version_1: 1.3863 | Loss version_2: 1.3866 | Loss version_3: 1.2006\n",
      "Epoch: 4 | Loss version_0: 1.3863 | Loss version_1: 1.3863 | Loss version_2: 1.3865 | Loss version_3: 1.1551\n",
      "Epoch: 5 | Loss version_0: 1.3863 | Loss version_1: 1.3863 | Loss version_2: 1.3863 | Loss version_3: 1.1381\n",
      "Epoch: 6 | Loss version_0: 1.3863 | Loss version_1: 1.3863 | Loss version_2: 1.3863 | Loss version_3: 1.1171\n",
      "Epoch: 7 | Loss version_0: 1.3863 | Loss version_1: 1.3863 | Loss version_2: 1.3863 | Loss version_3: 1.1120\n",
      "Epoch: 8 | Loss version_0: 1.3863 | Loss version_1: 1.3863 | Loss version_2: 1.3863 | Loss version_3: 1.1144\n",
      "Epoch: 9 | Loss version_0: 1.3863 | Loss version_1: 1.3863 | Loss version_2: 1.3863 | Loss version_3: 1.1047\n",
      "Epoch: 10 | Loss version_0: 1.3863 | Loss version_1: 1.3863 | Loss version_2: 1.3863 | Loss version_3: 1.1058\n",
      "Epoch: 11 | Loss version_0: 1.3863 | Loss version_1: 1.3863 | Loss version_2: 1.3863 | Loss version_3: 1.1061\n",
      "Epoch: 12 | Loss version_0: 1.3863 | Loss version_1: 1.3863 | Loss version_2: 1.3864 | Loss version_3: 1.1024\n",
      "Epoch: 13 | Loss version_0: 1.3863 | Loss version_1: 1.3863 | Loss version_2: 1.3863 | Loss version_3: 1.0994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m results[version][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruth_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(epoch_true_labels)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Compute the average loss for true and background edges\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m avgLossTrueTrain, logitsTrueTrain \u001b[38;5;241m=\u001b[39m \u001b[43mloss_for_train_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loaders_versions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m avgLossTrueTest, logitsTrueTest \u001b[38;5;241m=\u001b[39m loss_for_train_and_test(model, data_loaders_versions[version][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtt\u001b[39m\u001b[38;5;124m'\u001b[39m], criterion, optimizer, \u001b[38;5;28;01mFalse\u001b[39;00m, device)\n\u001b[1;32m     43\u001b[0m avgLossBkgTrain, logitsBkgTrain \u001b[38;5;241m=\u001b[39m loss_for_train_and_test(model, data_loaders_versions[version][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], criterion, optimizer, \u001b[38;5;28;01mTrue\u001b[39;00m, device)\n",
      "Cell \u001b[0;32mIn[65], line 29\u001b[0m, in \u001b[0;36mloss_for_train_and_test\u001b[0;34m(model, loader, loss_fn, optimizer, training, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_edge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_edge_index_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, batch_y[i])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 49\u001b[0m, in \u001b[0;36mMultiEdgeClassifier.forward\u001b[0;34m(self, x, edge_index, edge_index_out)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Loop through convolution layers\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs):\n\u001b[0;32m---> 49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug_print(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter GCNConv \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Check and remove batch dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:99\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    104\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/utils/loop.py:651\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    647\u001b[0m     is_undirected \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39mis_undirected\n\u001b[1;32m    649\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m edge_index[:, mask]\n\u001b[0;32m--> 651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_scripting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, EdgeIndex):\n\u001b[1;32m    652\u001b[0m     edge_index\u001b[38;5;241m.\u001b[39m_is_undirected \u001b[38;5;241m=\u001b[39m is_undirected\n\u001b[1;32m    654\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_index, loop_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_jit_internal.py:1130\u001b[0m, in \u001b[0;36mis_scripting\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastingList\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m BroadcastingList1\n\u001b[0;32m-> 1130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_scripting\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m    Function that returns True when in compilation and False otherwise. This\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;124;03m    is useful especially with the @unused decorator to leave code in your\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m              return unsupported_linear_op(x)\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Number of models (versions)\n",
    "num_models = 4\n",
    "versions = ['version_0', 'version_1', 'version_2', 'version_3']\n",
    "\n",
    "results = {version: {\n",
    "    \"loss_per_epoch\": [],\n",
    "    \"scores\": [],\n",
    "    \"truth_labels\": [],\n",
    "    \"avg_loss_training_true\": [],\n",
    "    \"logits_training_true\": [],\n",
    "    \"avg_loss_testing_true\": [],\n",
    "    \"logits_testing_true\": [],\n",
    "    \"avg_loss_training_bkg\": [],\n",
    "    \"logits_training_bkg\": [],\n",
    "    \"avg_loss_testing_bkg\": [],\n",
    "    \"logits_testing_bkg\": []\n",
    "} for version in devices}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for version in devices:\n",
    "        model = models[version]\n",
    "        device = devices[version]\n",
    "        optimizer = optimizers[version]\n",
    "        criterion = criterions[version]\n",
    "        scheduler = schedulers[version]\n",
    "        \n",
    "        # Train the model\n",
    "        total_loss = train_model(model, device, data_loaders_versions[version]['train'], optimizer, criterion)\n",
    "        results[version][\"loss_per_epoch\"].append(total_loss.cpu().detach().numpy())\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Test the model\n",
    "        epoch_scores, epoch_true_labels = test_model(model, device, data_loaders_versions[version]['tt'], data_loader_bkg_dict[version])\n",
    "        results[version][\"scores\"].append(epoch_scores)\n",
    "        results[version][\"truth_labels\"].append(epoch_true_labels)\n",
    "\n",
    "        # Compute the average loss for true and background edges\n",
    "        avgLossTrueTrain, logitsTrueTrain = loss_for_train_and_test(model, data_loaders_versions[version]['train'], criterion, optimizer, True, device)\n",
    "        avgLossTrueTest, logitsTrueTest = loss_for_train_and_test(model, data_loaders_versions[version]['tt'], criterion, optimizer, False, device)\n",
    "        avgLossBkgTrain, logitsBkgTrain = loss_for_train_and_test(model, data_loaders_versions[version]['train'], criterion, optimizer, True, device)\n",
    "        avgLossBkgTest, logitsBkgTest = loss_for_train_and_test(model, data_loaders_versions[version]['total_bkg'], criterion, optimizer, False, device)\n",
    "\n",
    "        results[version][\"avg_loss_training_true\"].append(avgLossTrueTrain)\n",
    "        results[version][\"logits_training_true\"].append(logitsTrueTrain)\n",
    "        results[version][\"avg_loss_testing_true\"].append(avgLossTrueTest)\n",
    "        results[version][\"logits_testing_true\"].append(logitsTrueTest)\n",
    "        results[version][\"avg_loss_training_bkg\"].append(avgLossBkgTrain)\n",
    "        results[version][\"logits_training_bkg\"].append(logitsBkgTrain)\n",
    "        results[version][\"avg_loss_testing_bkg\"].append(avgLossBkgTest)\n",
    "        results[version][\"logits_testing_bkg\"].append(logitsBkgTest)\n",
    "\n",
    "        losses.append(f\"Loss {version}: {total_loss.item():.4f}\")\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1} | \" + \" | \".join(losses))\n",
    "\n",
    "# Convert all lists to numpy arrays\n",
    "def convert_to_numpy(results):\n",
    "    for key, value in results.items():\n",
    "        for sub_key in value:\n",
    "            value[sub_key] = np.array(value[sub_key])\n",
    "    return results\n",
    "\n",
    "results = convert_to_numpy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for different loss plots\n",
    "colors = ['dimgray', 'silver', 'olive', 'lightgreen']\n",
    "\n",
    "# Plot loss per epoch for each model version\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for version_idx, version in enumerate(results.keys()):\n",
    "    epoch_number = np.arange(1, len(results[version][\"loss_per_epoch\"]) + 1)\n",
    "\n",
    "    plt.plot(epoch_number, results[version][\"loss_per_epoch\"], label=f'Loss per Epoch - {version}', linestyle='-', marker='o')\n",
    "    plt.plot(epoch_number, results[version][\"avg_loss_training_true\"], colors[0], label=f'Avg Loss True Train - {version}', linestyle='--')\n",
    "    plt.plot(epoch_number, results[version][\"avg_loss_testing_true\"], colors[1], label=f'Avg Loss True Test - {version}', linestyle='--')\n",
    "    plt.plot(epoch_number, results[version][\"avg_loss_training_bkg\"], colors[2], label=f'Avg Loss Bkg Train - {version}', linestyle='-.')\n",
    "    plt.plot(epoch_number, results[version][\"avg_loss_testing_bkg\"], colors[3], label=f'Avg Loss Bkg Test - {version}', linestyle='-.')\n",
    "\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Epoch for Multi-Class Case with Three Layers\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the class names\n",
    "# class_names = ['Lone-Lone', 'True-True', 'Cluster-Lone', 'Lone-Cluster', 'Cluster-Cluster']\n",
    "\n",
    "# # Define colormap\n",
    "# cmap = 'YlGnBu'\n",
    "\n",
    "# # Loop through each model version in results\n",
    "# for version in results.keys():\n",
    "#     print(f\"Plotting Confusion Matrix for {version}...\")\n",
    "\n",
    "#     # Extract scores and truth labels for this version\n",
    "#     scores = results[version][\"scores\"]  # Shape (500, 24000, 5)\n",
    "#     truth_labels = results[version][\"truth_labels\"]  # Shape (500, 24000)\n",
    "\n",
    "#     # Get predicted labels\n",
    "#     predicted_labels = np.argmax(scores, axis=-1)  # Shape (500, 24000)\n",
    "\n",
    "#     # Flatten arrays to compute confusion matrix across all samples\n",
    "#     predicted_labels_flat = predicted_labels.flatten()  # (500 * 24000,)\n",
    "#     truth_labels_flat = truth_labels.flatten()  # (500 * 24000,)\n",
    "\n",
    "#     # Compute confusion matrix\n",
    "#     conf_matrix = confusion_matrix(truth_labels_flat, predicted_labels_flat, labels=[0, 1, 2, 3, 4])\n",
    "\n",
    "#     # Normalize the confusion matrix to get percentages\n",
    "#     conf_matrix_percent = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "#     # Create figure for confusion matrices\n",
    "#     plt.figure(figsize=(16, 6))\n",
    "\n",
    "#     # Plot raw count confusion matrix\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=cmap, xticklabels=class_names, yticklabels=class_names)\n",
    "#     plt.xlabel('Predicted Labels')\n",
    "#     plt.ylabel('True Labels')\n",
    "#     plt.title(f'Confusion Matrix - {version}')\n",
    "\n",
    "#     # Plot percentage confusion matrix\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     sns.heatmap(conf_matrix_percent, annot=True, fmt='.2f', cmap=cmap, xticklabels=class_names, yticklabels=class_names)\n",
    "#     plt.xlabel('Predicted Labels')\n",
    "#     plt.ylabel('True Labels')\n",
    "#     plt.title(f'Confusion Matrix (Percentage) - {version}')\n",
    "\n",
    "#     # Adjust layout and show\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom class labels\n",
    "class_names = [\"Lone-Lone\", \"True Cluster\", \"Cluster-Lone\", \"Lone-Cluster\", \"Cluster-Cluster\"]\n",
    "class_order = [0, 1, 2, 3, 4]  # Labels to iterate over\n",
    "\n",
    "def plot_roc_curves(data_loaders_versions, model_names, num_epochs, interval=50, num_classes=5):\n",
    "    \"\"\"\n",
    "    Plots ROC curves for each model and version, showing curves at every 50th epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - data_loaders_versions: Dictionary with keys ('version_0', 'version_1', ...) containing the data for each version.\n",
    "    - model_names: List of model names.\n",
    "    - num_epochs: Total number of epochs.\n",
    "    - interval: Epoch interval for plotting (default is 50).\n",
    "    - num_classes: Number of classes (default is 5).\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_indices = list(range(0, num_epochs, interval))  # Select epochs at intervals\n",
    "    class_order = list(range(num_classes))  # Class order for iteration\n",
    "    \n",
    "    for version_key in data_loaders_versions.keys():\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))  # 2x3 grid for ROC curves\n",
    "        axes = axes.flatten()  # Flatten for easier subplot indexing\n",
    "\n",
    "        for model_idx, model_name in enumerate(model_names):\n",
    "            for plot_idx, epoch in enumerate(epoch_indices):\n",
    "                if plot_idx >= 6:  # Ensure we only plot within a 2x3 layout\n",
    "                    break\n",
    "\n",
    "                ax = axes[plot_idx]  # Get subplot axis for the current plot\n",
    "\n",
    "                # Get scores and labels for the given epoch and version\n",
    "                epoch_scores = data_loaders_versions[version_key]['scores'][epoch]  # Shape (N, 5)\n",
    "                epoch_truth_labels = data_loaders_versions[version_key]['truth_labels'][epoch]  # Shape (N,)\n",
    "\n",
    "                # Iterate over each class and plot the ROC curve\n",
    "                for class_idx in class_order:\n",
    "                    binary_truth_labels = (epoch_truth_labels == class_idx).astype(int)  # True labels for this class\n",
    "                    class_scores = epoch_scores[:, class_idx]  # Scores for this class\n",
    "\n",
    "                    fpr, tpr, _ = roc_curve(binary_truth_labels, class_scores)\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "                    ax.plot(fpr, tpr, label=f'{class_names[class_idx]} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "                ax.set_title(f\"{model_name} - {version_key} - Epoch {epoch + 1}\")\n",
    "                ax.set_xlabel(\"False Positive Rate\")\n",
    "                ax.set_ylabel(\"True Positive Rate\")\n",
    "                ax.legend(loc=\"lower right\")\n",
    "\n",
    "        # Adjust layout for each version and model\n",
    "        plt.suptitle(f\"ROC Curves for {version_key}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "model_names = [\"Model A\", \"Model B\", \"Model C\", \"Model D\"]\n",
    "plot_roc_curves(data_loaders_versions, model_names, num_epochs, interval=50, num_classes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(data_loaders_versions, model_names, num_epochs, interval=50, thresholds=None):\n",
    "    \"\"\"\n",
    "    Plots confusion matrices for multiple models at specified epoch intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - data_loaders_versions: Dictionary with keys ('version_0', 'version_1', ...) containing the data for each version.\n",
    "    - model_names: List of model names.\n",
    "    - num_epochs: Total number of epochs.\n",
    "    - interval: Epoch interval for plotting (default is 50).\n",
    "    - thresholds: List of threshold values per class (default is None, uses argmax).\n",
    "    \"\"\"\n",
    "\n",
    "    # Default thresholds to 0.5 for all classes if not provided\n",
    "    if thresholds is None:\n",
    "        thresholds = [0.5] * len(class_names)\n",
    "\n",
    "    epoch_indices = list(range(0, num_epochs, interval))  # Select epochs at intervals\n",
    "\n",
    "    for version_key in data_loaders_versions.keys():\n",
    "        for model_idx, model_name in enumerate(model_names):\n",
    "            for epoch in epoch_indices:\n",
    "                # Extract scores and truth labels for the given epoch\n",
    "                epoch_scores = data_loaders_versions[version_key]['scores'][epoch]  # Shape (N, 5)\n",
    "                epoch_truth_labels = data_loaders_versions[version_key]['truth_labels'][epoch]  # Shape (N,)\n",
    "\n",
    "                # Convert scores to predicted labels using custom thresholds\n",
    "                predicted_labels = np.full(epoch_scores.shape[0], -1)  # Initialize with -1 (unclassified)\n",
    "                \n",
    "                for i in range(len(class_order)):  # Iterate over each class\n",
    "                    mask = (epoch_scores[:, i] >= thresholds[i]) & (predicted_labels == -1)\n",
    "                    predicted_labels[mask] = i  # Assign class index based on threshold\n",
    "\n",
    "                # Assign unclassified samples to the most probable class if all thresholds failed\n",
    "                unclassified_mask = predicted_labels == -1\n",
    "                predicted_labels[unclassified_mask] = np.argmax(epoch_scores[unclassified_mask], axis=-1)\n",
    "\n",
    "                # Compute confusion matrix\n",
    "                conf_matrix = confusion_matrix(epoch_truth_labels, predicted_labels, labels=class_order)\n",
    "\n",
    "                # Normalize confusion matrix (percentage)\n",
    "                conf_matrix_percent = conf_matrix.astype(float) / conf_matrix.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "                # Plot confusion matrices\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "                # Raw Counts Confusion Matrix\n",
    "                sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"YlGnBu\", xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
    "                axes[0].set_xlabel('Predicted Labels')\n",
    "                axes[0].set_ylabel('True Labels')\n",
    "                axes[0].set_title(f'{model_name} - {version_key} - Epoch {epoch+1} (Raw Counts)')\n",
    "\n",
    "                # Percentage Confusion Matrix\n",
    "                sns.heatmap(conf_matrix_percent, annot=True, fmt='.2f', cmap=\"YlGnBu\", xticklabels=class_names, yticklabels=class_names, ax=axes[1])\n",
    "                axes[1].set_xlabel('Predicted Labels')\n",
    "                axes[1].set_ylabel('True Labels')\n",
    "                axes[1].set_title(f'{model_name} - {version_key} - Epoch {epoch+1} (Percentage)')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "# Example usage:\n",
    "model_names = [\"Model A\", \"Model B\", \"Model C\", \"Model D\"]\n",
    "# custom_thresholds = [0.5, 0.6, 0.7, 0.4, 0.5]  # Set per-class thresholds\n",
    "plot_confusion_matrices(data_loaders_versions, model_names, num_epochs, interval=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['Blue', 'Orange', 'Green', 'Red', 'Purple']\n",
    "output_classes = np.arange(5)  # Classes 0, 1, 2, 3, 4\n",
    "\n",
    "for model_idx in range(num_models):  # Loop over 4 models\n",
    "    epoch_index = -1  # Last epoch\n",
    "    \n",
    "    # Extract scores and truth labels for the current model\n",
    "    epoch_scores = results[model_idx][\"scores\"][epoch_index]  # Shape: (24000, 5)\n",
    "    epoch_truth_labels = results[model_idx][\"truth_labels\"][epoch_index]  # Shape: (24000,)\n",
    "\n",
    "    # --- First plot: Class-wise Score Distributions ---\n",
    "    rows, cols = 2, 3\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    fig.suptitle(f\"Model {model_idx}: Class-wise Score Distributions at Final Epoch\", fontsize=16)\n",
    "\n",
    "    for class_idx in range(len(output_classes)):\n",
    "        ax = axes[class_idx]\n",
    "        ax.set_title(f'Output Class {class_idx}')\n",
    "        ax.set_xlabel('Score')\n",
    "        ax.set_ylabel('Density')\n",
    "\n",
    "        for truth_type in sorted(np.unique(epoch_truth_labels)):\n",
    "            scores_for_truth_type = epoch_scores[epoch_truth_labels == truth_type, class_idx]\n",
    "            fraction_above_0_5 = np.mean(scores_for_truth_type > 0.5)\n",
    "\n",
    "            ax.hist(\n",
    "                scores_for_truth_type,\n",
    "                bins=50,\n",
    "                density=True,\n",
    "                alpha=0.6,\n",
    "                label=f'Truth {truth_type} (>{fraction_above_0_5:.2%})',\n",
    "                color=colors[truth_type % len(colors)]\n",
    "            )\n",
    "\n",
    "        ax.legend()\n",
    "\n",
    "    if len(axes) > len(output_classes):\n",
    "        fig.delaxes(axes[-1])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # --- Second plot: Truth Type-wise Score Distributions ---\n",
    "    rows, cols = 2, 3\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    unique_truth_types = sorted(np.unique(epoch_truth_labels))\n",
    "\n",
    "    fig.suptitle(f\"Model {model_idx}: Truth Type-wise Score Distributions at Final Epoch\", fontsize=16)\n",
    "\n",
    "    if len(unique_truth_types) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for truth_type_idx, truth_type in enumerate(unique_truth_types):\n",
    "        ax = axes[truth_type_idx]\n",
    "        ax.set_title(f'Truth Type {truth_type}')\n",
    "        ax.set_xlabel('Score')\n",
    "        ax.set_ylabel('Density')\n",
    "\n",
    "        for class_idx in output_classes:\n",
    "            scores_for_truth_type_class = epoch_scores[epoch_truth_labels == truth_type, class_idx]\n",
    "            fraction_above_0_5 = np.mean(scores_for_truth_type_class > 0.5)\n",
    "\n",
    "            ax.hist(\n",
    "                scores_for_truth_type_class,\n",
    "                bins=50,\n",
    "                density=True,\n",
    "                alpha=0.6,\n",
    "                label=f'Class {class_idx} (>{fraction_above_0_5:.2%})',\n",
    "                color=colors[class_idx % len(colors)]\n",
    "            )\n",
    "\n",
    "        ax.legend()\n",
    "\n",
    "    for idx in range(len(unique_truth_types), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for different truth types and output classes\n",
    "colors = ['Blue', 'Orange', 'Green', 'Red', 'Purple']\n",
    "\n",
    "def plot_classwise_score_distributions(results, model_names, num_epochs, thresholds=[0.5], output_dim=5):\n",
    "    \"\"\"\n",
    "    Plots the class-wise score distributions for each model version at the final epoch with custom thresholds.\n",
    "    \"\"\"\n",
    "    epoch_index = -1  # Use the last epoch\n",
    "\n",
    "    # Loop through each version in the results\n",
    "    for version_key, version_data in results.items():\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Add a global title for the entire figure\n",
    "        fig.suptitle(f\"Class-wise Score Distributions with Truth Label Overlays at Final Epoch ({version_key})\", fontsize=16)\n",
    "\n",
    "        for class_idx in range(output_dim):\n",
    "            ax = axes[class_idx]\n",
    "            ax.set_title(f'Output Class {class_idx}')\n",
    "            ax.set_xlabel('Score')\n",
    "            ax.set_ylabel('Density')\n",
    "\n",
    "            # Get the scores and truth labels for the final epoch in this version\n",
    "            epoch_scores = version_data[\"scores\"][epoch_index]  # Shape: (24000, 5)\n",
    "            epoch_truth_labels = version_data[\"truth_labels\"][epoch_index]  # Shape: (24000,)\n",
    "\n",
    "            # For each truth type (i.e., label), plot the score distribution\n",
    "            for truth_type in sorted(np.unique(epoch_truth_labels)):\n",
    "                # Get scores for the current truth type and current class\n",
    "                scores_for_truth_type = epoch_scores[epoch_truth_labels == truth_type, class_idx]\n",
    "\n",
    "                # Plot normalized histogram\n",
    "                for threshold in thresholds:\n",
    "                    # Calculate the fraction of samples with scores > threshold\n",
    "                    fraction_above_threshold = np.mean(scores_for_truth_type > threshold)\n",
    "                    ax.hist(\n",
    "                        scores_for_truth_type,\n",
    "                        bins=50,\n",
    "                        density=True,  # Normalize the histogram\n",
    "                        alpha=0.6,\n",
    "                        label=f'Truth {truth_type} (>{fraction_above_threshold:.2%} > {threshold})',\n",
    "                        color=colors[truth_type % len(colors)]\n",
    "                    )\n",
    "\n",
    "            ax.legend()\n",
    "\n",
    "        # Remove the third column in the second row if output_dim < rows * cols\n",
    "        if len(axes) > output_dim:\n",
    "            fig.delaxes(axes[-1])\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the title\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_truth_typewise_score_distributions(results, model_names, num_epochs, thresholds=[0.5], output_dim=5):\n",
    "    \"\"\"\n",
    "    Plots the truth type-wise score distributions for each model version at the final epoch with custom thresholds.\n",
    "    \"\"\"\n",
    "    epoch_index = -1  # Use the last epoch\n",
    "\n",
    "    # Loop through each version in the results\n",
    "    for version_key, version_data in results.items():\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Add a global title for the entire figure\n",
    "        fig.suptitle(f\"Truth Type-wise Score Distributions for All Output Classes at Final Epoch ({version_key})\", fontsize=16)\n",
    "\n",
    "        unique_truth_types = sorted(np.unique(version_data[\"truth_labels\"][epoch_index]))\n",
    "\n",
    "        # Handle cases where only one truth type is present\n",
    "        if len(unique_truth_types) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for truth_type_idx, truth_type in enumerate(unique_truth_types):\n",
    "            ax = axes[truth_type_idx]\n",
    "            ax.set_title(f'Truth Type {truth_type}')\n",
    "            ax.set_xlabel('Score')\n",
    "            ax.set_ylabel('Density')\n",
    "\n",
    "            # Get the scores and truth labels for the final epoch in this version\n",
    "            epoch_scores = version_data[\"scores\"][epoch_index]  # Shape: (24000, 5)\n",
    "            epoch_truth_labels = version_data[\"truth_labels\"][epoch_index]  # Shape: (24000,)\n",
    "\n",
    "            for class_idx in range(output_dim):\n",
    "                # Get scores for the current output class and the current truth type\n",
    "                scores_for_truth_type_class = epoch_scores[epoch_truth_labels == truth_type, class_idx]\n",
    "\n",
    "                # Plot normalized histogram for the current output class\n",
    "                for threshold in thresholds:\n",
    "                    # Calculate the fraction of scores > threshold\n",
    "                    fraction_above_threshold = np.mean(scores_for_truth_type_class > threshold)\n",
    "                    ax.hist(\n",
    "                        scores_for_truth_type_class,\n",
    "                        bins=50,\n",
    "                        density=True,  # Normalize the histogram\n",
    "                        alpha=0.6,\n",
    "                        label=f'Class {class_idx} (>{fraction_above_threshold:.2%} > {threshold})',\n",
    "                        color=colors[class_idx % len(colors)]\n",
    "                    )\n",
    "\n",
    "            # Add legend with updated labels\n",
    "            ax.legend()\n",
    "\n",
    "        # Remove unused subplots\n",
    "        for idx in range(len(unique_truth_types), len(axes)):\n",
    "            fig.delaxes(axes[idx])\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the title\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with custom thresholds:\n",
    "model_names = [\"Model A\", \"Model B\", \"Model C\", \"Model D\"]\n",
    "thresholds = [0.3, 0.5, 0.7]  # Set custom thresholds for score distribution\n",
    "plot_classwise_score_distributions(results, model_names, num_epochs, thresholds=thresholds, output_dim=5)\n",
    "plot_truth_typewise_score_distributions(results, model_names, num_epochs, thresholds=thresholds, output_dim=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
