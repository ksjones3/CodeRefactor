{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6b1226-a989-4de3-bfce-87f5c1a3dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "random.seed(42)  \n",
    "\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns  \n",
    "\n",
    "from collections import Counter  \n",
    "\n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F  \n",
    "import torch.optim as optim  \n",
    "\n",
    "from torch.nn import BatchNorm1d  \n",
    "from torch_geometric.data import Data  \n",
    "from torch_geometric.nn import GCNConv, GATConv  \n",
    "from torch_geometric.transforms import ToUndirected  \n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch_geometric.utils import add_self_loops  \n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix  \n",
    "from sklearn.preprocessing import MinMaxScaler, label_binarize\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf1431b-1dc6-49ce-8f62-8c3230125cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['analysis;1']\n"
     ]
    }
   ],
   "source": [
    "file = uproot.open('/storage/mxg1065/MyxAODAnalysis_super3D.outputs.root')\n",
    "print(file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "510207d7-9622-4c62-b9f3-5f5e724ff76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RunNumber', 'EventNumber', 'cell_eta', 'cell_phi', 'cell_x', 'cell_y', 'cell_z', 'cell_subCalo', 'cell_sampling', 'cell_size', 'cell_hashID', 'neighbor', 'seedCell_id', 'cell_e', 'cell_noiseSigma', 'cell_SNR', 'cell_time', 'cell_weight', 'cell_truth', 'cell_truth_indices', 'cell_shared_indices', 'cell_cluster_index', 'cluster_to_cell_indices', 'cluster_to_cell_weights', 'cell_to_cluster_e', 'cell_to_cluster_eta', 'cell_to_cluster_phi', 'cluster_eta', 'cluster_phi', 'cluster_e', 'cellsNo_cluster', 'clustersNo_event', 'jetEnergyWtdTimeAve', 'jetEta', 'jetPhi', 'jetE', 'jetPt', 'jetNumberPerEvent', 'cellIndices_per_jet']\n"
     ]
    }
   ],
   "source": [
    "tree = file['analysis;1']\n",
    "branches = tree.arrays()\n",
    "print(tree.keys()) # Variables per event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ff68c-72cf-46c9-8e01-baf83ebd9ed8",
   "metadata": {},
   "source": [
    "### Prepairing the Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00463fb9-9804-4bb7-9f7a-bdc68bb94568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 events and 187652 cells\n",
    "# Arrays containing information about the energy, noise, snr, \n",
    "cell_e = np.array(branches['cell_e'])\n",
    "cell_noise = np.array(branches['cell_noiseSigma'])\n",
    "cell_snr = np.array(branches['cell_SNR'])\n",
    "cell_eta = np.array(branches['cell_eta'])\n",
    "cell_phi = np.array(branches['cell_phi'])\n",
    "\n",
    "# Represents the index of the cluster that each cell corresponds to. If the index\n",
    "# is 0, that means that the given cell does not belong to a cluster.\n",
    "cell_to_cluster_index = np.array(branches['cell_cluster_index'])\n",
    "\n",
    "# For each entry, contains the IDs of cells neighboring a given cell\n",
    "neighbor = branches['neighbor']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "448dbc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Events: 100\n",
      "Number of Cells: 187652\n"
     ]
    }
   ],
   "source": [
    "num_of_events = len(cell_snr)\n",
    "num_of_cells = len(cell_snr[0])\n",
    "print(\"Number of Events:\", num_of_events)\n",
    "print(\"Number of Cells:\", num_of_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0285e4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02640459 0.24943821 0.09700817 0.23466232 0.5085498 ]\n",
      " [0.02567646 0.24627675 0.09700815 0.23466876 0.52418756]\n",
      " [0.02508186 0.24369499 0.09700817 0.23467347 0.5398244 ]\n",
      " ...\n",
      " [0.02632877 0.24791998 0.03177016 0.62017125 0.4921177 ]\n",
      " [0.02705116 0.2511391  0.07512318 0.6461531  0.4921177 ]\n",
      " [0.02638626 0.24820389 0.04149057 0.6731673  0.4921177 ]]\n",
      "(187652, 5)\n"
     ]
    }
   ],
   "source": [
    "# We use the data arrays to crete a data dictionary, where each entry corresponds\n",
    "# to the data of a given event; we scale this data.\n",
    "data = {}\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    data[f'data_{i}'] = np.concatenate((np.expand_dims(cell_snr[i], axis=1),\n",
    "                                        np.expand_dims(cell_e[i], axis=1),\n",
    "                                        np.expand_dims(cell_noise[i], axis=1),\n",
    "                                        np.expand_dims(cell_eta[i], axis=1),\n",
    "                                        np.expand_dims(cell_phi[i], axis=1)), axis=1)\n",
    "    \n",
    "# We combine the data into one array and apply the MinMaxScaler\n",
    "combined_data = np.vstack([data[key] for key in data])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_combined_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "# The scaled data is split to have the save structure as the original data dict\n",
    "scaled_data = {}\n",
    "start_idx = 0\n",
    "for i in range(num_of_events):\n",
    "    end_idx = start_idx + data[f\"data_{i}\"].shape[0]\n",
    "    scaled_data[f\"data_{i}\"] = scaled_combined_data[start_idx:end_idx]\n",
    "    start_idx = end_idx\n",
    "\n",
    "print(scaled_data[\"data_0\"])\n",
    "print(scaled_data[\"data_0\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54959f5c-b033-443c-8bf1-e73358d696d2",
   "metadata": {},
   "source": [
    "### Preparing Neighbor Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b8ff2a-44c4-4110-9abc-13979f761a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186986 187352]\n"
     ]
    }
   ],
   "source": [
    "# The IDs of the broken cells (those with zero noise) are collected\n",
    "broken_cells = []\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    cells = np.argwhere(cell_noise[i]==0)\n",
    "    broken_cells = np.squeeze(cells)\n",
    "\n",
    "print(broken_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70ed47c5-6530-467f-ba45-d57e898eabb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the values associated with neighbor[0] and neighbor[1] are all equal\n",
    "# we will just work with neighbor[0] to simplify our calculations\n",
    "neighbor = neighbor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d8b1108-24d8-4d37-a7eb-86a621744c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We loop through the neighbor awkward array and remove the IDs associated\n",
    "# with the broken cells.  Loops through all cells in the neighbor list. If the loop \n",
    "# reaches the cell numbers 186986 or 187352, loop skips over these inoperative cells. \n",
    "# The final list contains tuples (i,j) where i is the cell ID in question and the \n",
    "# js are the neighboring cell IDs\n",
    "neighbor_pairs_list = []\n",
    "\n",
    "for i in range(num_of_cells):\n",
    "    if i in broken_cells:\n",
    "        continue\n",
    "    for j in neighbor[i]:\n",
    "        if j in broken_cells:\n",
    "            continue\n",
    "        neighbor_pairs_list.append((i, int(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef500146-1e14-46c9-a3a7-8e6dbab0b566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully excluded broken cells.\n"
     ]
    }
   ],
   "source": [
    "# This code checks to see if the broken cells were removed\n",
    "found_broken_cells = []\n",
    "\n",
    "for pair in neighbor_pairs_list:\n",
    "    # Loop through each cell in pair\n",
    "    for cell in pair:\n",
    "        # If the cell is broken, appends to list\n",
    "        if cell in broken_cells:\n",
    "            found_broken_cells.append(cell)\n",
    "\n",
    "if found_broken_cells:\n",
    "    print(\"Error: Broken cells are still present in neighbor pairs.\")\n",
    "else:\n",
    "    print(\"Successfully excluded broken cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1af53e81-a169-4b8a-8878-a0a3bd1a643f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 90345 119588]\n",
      " [  4388  17680]\n",
      " [ 39760  39825]\n",
      " ...\n",
      " [159757 168717]\n",
      " [ 62911  78974]\n",
      " [135353 135609]]\n",
      "(1250242, 2)\n"
     ]
    }
   ],
   "source": [
    "# These functions remove permutation variants\n",
    "def canonical_form(t):\n",
    "    return tuple(sorted(t))\n",
    "\n",
    "def remove_permutation_variants(tuple_list):\n",
    "    unique_tuples = set(canonical_form(t) for t in tuple_list)\n",
    "    return [tuple(sorted(t)) for t in unique_tuples]\n",
    "\n",
    "neighbor_pairs_list = np.array(remove_permutation_variants(neighbor_pairs_list))\n",
    "print(neighbor_pairs_list)\n",
    "print(neighbor_pairs_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddec80a-83ec-424f-92c5-4370d7d4292b",
   "metadata": {},
   "source": [
    "### Creating Labels for the Neighbor Pairs\n",
    "\n",
    "For a given pair of cells and the IDs of the clusters that they belong to (i, j), if\n",
    "1. i=j and both are nonzero, then both cells are part of the same cluster. \n",
    "    * We call these True-True pairs and label them with 1\n",
    "2. i=j and both are zero, then both cells are not part of any cluster. \n",
    "    * We call these Lone-Lone pairs and label them with 0\n",
    "3. i is nonzero and j=0, then cell i is part of a cluster while cell j is not. \n",
    "    * We call these Cluster-Lone pairs and label them with 2\n",
    "4. i=0 and j is nonzero, then cell i is not part of a cluste while cell j is. \n",
    "    * We call these Lone-Cluster pairs and label them with 3\n",
    "5. i is not the same as j and both are nonzero, then both cells are part of different clusters. \n",
    "    * We call these Cluster-Cluster pairs and label them with 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8d4fede-a6aa-46da-9c8a-a9d2bc9d09de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1250242)\n",
      "[[0 0 0 ... 0 0 2]\n",
      " [3 3 0 ... 0 0 2]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 2 0 0]]\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Initialize array for labels\n",
    "labels_for_neighbor_pairs = np.zeros((num_of_events, len(neighbor_pairs_list)), dtype=int)\n",
    "\n",
    "# Extracting the individual cells within a cell pair\n",
    "cell_0 = cell_to_cluster_index[:, neighbor_pairs_list[:, 0]]\n",
    "cell_1 = cell_to_cluster_index[:, neighbor_pairs_list[:, 1]]\n",
    "\n",
    "# Computing labels using vectorized operations\n",
    "same_cluster = cell_0 == cell_1 \n",
    "both_nonzero = (cell_0 != 0) & (cell_1 != 0)\n",
    "\n",
    "# Lone-Lone (0)\n",
    "labels_for_neighbor_pairs[same_cluster & (cell_0 == 0)] = 0\n",
    "\n",
    "# True-True (1)\n",
    "labels_for_neighbor_pairs[same_cluster & (cell_0 != 0)] = 1\n",
    "\n",
    "# Cluster-Cluster (4)\n",
    "labels_for_neighbor_pairs[~same_cluster & both_nonzero] = 4\n",
    "\n",
    "# Lone-Cluster (3)\n",
    "labels_for_neighbor_pairs[~same_cluster & (cell_0 == 0) & (cell_1 != 0)] = 3\n",
    "\n",
    "# Cluster-Lone (2)\n",
    "labels_for_neighbor_pairs[~same_cluster & (cell_0 != 0) & (cell_1 == 0)] = 2\n",
    "\n",
    "print(labels_for_neighbor_pairs.shape)\n",
    "print(labels_for_neighbor_pairs)\n",
    "print(np.unique(labels_for_neighbor_pairs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd492f7d-cd91-4387-b354-39178f815bd9",
   "metadata": {},
   "source": [
    "### Preparing the Data for Multi-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe3e7023-f51e-495e-a325-bebb07ae496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we collect the indices of the neighbor pairs by the pair type\n",
    "indices_for_tt_pairs = []  # Label 1\n",
    "indices_for_ll_pairs = []  # Label 0\n",
    "indices_for_cl_pairs = []  # Label 2\n",
    "indices_for_lc_pairs = []  # Label 3\n",
    "indices_for_cc_pairs = []  # Label 4\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    indices_for_tt_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 1)[0]))\n",
    "    indices_for_ll_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 0)[0]))\n",
    "    indices_for_cl_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 2)[0]))\n",
    "    indices_for_lc_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 3)[0]))\n",
    "    indices_for_cc_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 4)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99704084-bfba-44fd-8ef9-a1a63b520950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we collect the number of each pair type across the events\n",
    "number_of_tt_pairs = [len(indices_for_tt_pairs[i])for i in range(num_of_events)]\n",
    "number_of_ll_pairs = [len(indices_for_ll_pairs[i])for i in range(num_of_events)]\n",
    "number_of_cl_pairs = [len(indices_for_cl_pairs[i])for i in range(num_of_events)]\n",
    "number_of_lc_pairs = [len(indices_for_lc_pairs[i])for i in range(num_of_events)]\n",
    "number_of_cc_pairs = [len(indices_for_cc_pairs[i])for i in range(num_of_events)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc4086ac-778d-4cb5-b102-9e6bab748957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we perform a 80-20 split on the indices of neighbor pairs\n",
    "training_indices_tt = indices_for_tt_pairs[:80]\n",
    "training_indices_ll = indices_for_ll_pairs[:80]\n",
    "training_indices_cl = indices_for_cl_pairs[:80]\n",
    "training_indices_lc = indices_for_lc_pairs[:80]\n",
    "training_indices_cc = indices_for_cc_pairs[:80]\n",
    "\n",
    "testing_indices_tt = indices_for_tt_pairs[80:]\n",
    "testing_indices_ll = indices_for_ll_pairs[80:]\n",
    "testing_indices_cl = indices_for_cl_pairs[80:]\n",
    "testing_indices_lc = indices_for_lc_pairs[80:]\n",
    "testing_indices_cc = indices_for_cc_pairs[80:]\n",
    "\n",
    "# Here we perform a 80-20 split on the number of neighbor pairs\n",
    "training_num_tt = number_of_tt_pairs[:80]\n",
    "training_num_ll = number_of_ll_pairs[:80]\n",
    "training_num_cl = number_of_cl_pairs[:80]\n",
    "training_num_lc = number_of_lc_pairs[:80]\n",
    "training_num_cc = number_of_cc_pairs[:80]\n",
    "\n",
    "testing_num_tt = number_of_tt_pairs[80:]\n",
    "testing_num_ll = number_of_ll_pairs[80:]\n",
    "testing_num_cl = number_of_cl_pairs[80:]\n",
    "testing_num_lc = number_of_lc_pairs[80:]\n",
    "testing_num_cc = number_of_cc_pairs[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b51041e4-ab4a-42d3-b477-70038ceabcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of pairs for training:\n",
      "True-True 45600\n",
      "Lone-Lone 926119\n",
      "Cluster-Lone 41654\n",
      "Lone-Cluster 45689\n",
      "Cluster-Cluster 3334\n",
      "\n",
      "Minimum number of pairs for testing:\n",
      "True-True 51518\n",
      "Lone-Lone 906630\n",
      "Cluster-Lone 44444\n",
      "Lone-Cluster 48069\n",
      "Cluster-Cluster 4936\n"
     ]
    }
   ],
   "source": [
    "# We check the minimum number of each pair type across the events. When we\n",
    "# randomly sample from the indices, if our sample is greater than the minimum\n",
    "# numbers, then we will run into errors\n",
    "print(\"Minimum number of pairs for training:\")\n",
    "print(\"True-True\", min(training_num_tt))\n",
    "print(\"Lone-Lone\", min(training_num_ll))\n",
    "print(\"Cluster-Lone\", min(training_num_cl))\n",
    "print(\"Lone-Cluster\", min(training_num_lc))\n",
    "print(\"Cluster-Cluster\", min(training_num_cc))\n",
    "print('\\nMinimum number of pairs for testing:')\n",
    "print(\"True-True\", min(testing_num_tt))\n",
    "print(\"Lone-Lone\", min(testing_num_ll))\n",
    "print(\"Cluster-Lone\", min(testing_num_cl))\n",
    "print(\"Lone-Cluster\", min(testing_num_lc))\n",
    "print(\"Cluster-Cluster\", min(testing_num_cc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb64b1b6-ff88-40d7-b92f-b5a67119a5a9",
   "metadata": {},
   "source": [
    "### Making training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfc9affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_sample_num = 40000\n",
    "small_sample_num = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a405c2fb-e5ed-44ae-9d21-a65191e0ced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 40000)\n",
      "(80, 123000)\n",
      "(20, 40000)\n",
      "(20, 123000)\n"
     ]
    }
   ],
   "source": [
    "train_indices_tt_pairs = np.array([random.sample(row, big_sample_num) for row in training_indices_tt])\n",
    "train_indices_ll_pairs = np.array([random.sample(row, big_sample_num) for row in training_indices_ll])\n",
    "train_indices_cl_pairs = np.array([random.sample(row, big_sample_num) for row in training_indices_cl])\n",
    "train_indices_lc_pairs = np.array([random.sample(row, big_sample_num) for row in training_indices_lc])\n",
    "train_indices_cc_pairs = np.array([random.sample(row, small_sample_num) for row in training_indices_cc])\n",
    "train_indices_bkg_pairs = np.concatenate([train_indices_ll_pairs,\n",
    "                                          train_indices_cl_pairs,\n",
    "                                          train_indices_lc_pairs,\n",
    "                                          train_indices_cc_pairs], axis=1)\n",
    "\n",
    "test_indices_tt_pairs = np.array([random.sample(row, big_sample_num) for row in testing_indices_tt])\n",
    "test_indices_ll_pairs = np.array([random.sample(row, big_sample_num) for row in testing_indices_ll])\n",
    "test_indices_cl_pairs = np.array([random.sample(row, big_sample_num) for row in testing_indices_cl])\n",
    "test_indices_lc_pairs = np.array([random.sample(row, big_sample_num) for row in testing_indices_lc])\n",
    "test_indices_cc_pairs = np.array([random.sample(row, small_sample_num) for row in testing_indices_cc])\n",
    "test_indices_bkg_pairs = np.concatenate([test_indices_ll_pairs,\n",
    "                                         test_indices_cl_pairs,\n",
    "                                         test_indices_lc_pairs,\n",
    "                                         test_indices_cc_pairs], axis=1)\n",
    "\n",
    "print(train_indices_tt_pairs.shape)\n",
    "print(train_indices_bkg_pairs.shape)\n",
    "print(test_indices_tt_pairs.shape)\n",
    "print(test_indices_bkg_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08243102-f30f-4319-8c3a-0e8079f3f8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 163000)\n",
      "(20, 163000)\n"
     ]
    }
   ],
   "source": [
    "total_training_indices = np.concatenate((train_indices_tt_pairs, train_indices_bkg_pairs), axis=1)\n",
    "total_testing_indices = np.concatenate((test_indices_tt_pairs, test_indices_bkg_pairs), axis=1)\n",
    "print(total_training_indices.shape)\n",
    "print(total_testing_indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490145b9-1354-4a6e-b3fb-aede8bca58d3",
   "metadata": {},
   "source": [
    "### Randomizing the indicies and creating labels for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7422bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 163000)\n",
      "(20, 163000)\n"
     ]
    }
   ],
   "source": [
    "# Creating training labels\n",
    "labels_tt_train = np.ones((80, big_sample_num), dtype=int)  # True-True\n",
    "labels_ll_train = np.zeros((80, big_sample_num), dtype=int)  # Lone-Lone\n",
    "labels_cl_train = np.ones((80, big_sample_num), dtype=int)*2  # Cluster-Lone\n",
    "labels_lc_train = np.ones((80, big_sample_num), dtype=int)*3  # Lone-Cluster\n",
    "labels_cc_train = np.ones((80, small_sample_num), dtype=int)*4  # Cluster-Cluster\n",
    "labels_bkg_train = np.concatenate((labels_ll_train, labels_cl_train, labels_lc_train, labels_cc_train), axis=1)\n",
    "labels_training = np.concatenate((labels_tt_train, labels_bkg_train), axis=1)\n",
    "\n",
    "# Creating testing labels\n",
    "labels_tt_test = np.ones((20, big_sample_num), dtype=int)  # True-True\n",
    "labels_ll_test = np.zeros((20, big_sample_num), dtype=int)  # Lone-Lone\n",
    "labels_cl_test = np.ones((20, big_sample_num), dtype=int)*2  # Cluster-Lone\n",
    "labels_lc_test = np.ones((20, big_sample_num), dtype=int)*3  # Lone-Cluster\n",
    "labels_cc_test = np.ones((20, small_sample_num), dtype=int)*4  # Cluster-Cluster\n",
    "labels_bkg_test = np.concatenate((labels_ll_test, labels_cl_test, labels_lc_test, labels_cc_test), axis=1)\n",
    "labels_testing = np.concatenate((labels_tt_test, labels_bkg_test), axis=1)\n",
    "\n",
    "# Printing the shapes of the final training and testing labels\n",
    "print(labels_training.shape)\n",
    "print(labels_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe86e75a-a440-4b91-9bc5-c85485664412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 163000)\n",
      "(80, 163000)\n",
      "(20, 163000)\n",
      "(20, 163000)\n"
     ]
    }
   ],
   "source": [
    "# Here we randomize the training and testing information. But to do this, while keeping\n",
    "# the same permutation for both indicies and labels, we use np.random.permutation\n",
    "\n",
    "# Randomizing training data via iteration over the rows\n",
    "for i in range(total_training_indices.shape[0]):\n",
    "    perm = np.random.permutation(total_training_indices.shape[1])\n",
    "    total_training_indices[i] = total_training_indices[i, perm]\n",
    "    labels_training[i] = labels_training[i, perm]\n",
    "\n",
    "# Randomizing testing data via iteration over the rows\n",
    "for i in range(total_testing_indices.shape[0]):\n",
    "    perm = np.random.permutation(total_testing_indices.shape[1])\n",
    "    total_testing_indices[i] = total_testing_indices[i, perm]\n",
    "    labels_testing[i] = labels_testing[i, perm]\n",
    "\n",
    "print(total_training_indices.shape)\n",
    "print(labels_training.shape)\n",
    "print(total_testing_indices.shape)\n",
    "print(labels_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecd8ac0a-af7c-405c-a52e-486d736ab246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 163000, 2)\n",
      "(20, 163000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Arranging the neighbor pairs with the training indices\n",
    "total_train_neighbor_random = []\n",
    "for i in range(len(labels_training)):\n",
    "    total_train_neighbor_random.append(neighbor_pairs_list[total_training_indices[i]])\n",
    "total_train_neighbor_random = np.array(total_train_neighbor_random)\n",
    "print(total_train_neighbor_random.shape)\n",
    "\n",
    "# Arranging the neighbor pairs with the testing indices\n",
    "total_test_neighbor_random = []\n",
    "for i in range(len(labels_testing)):\n",
    "    total_test_neighbor_random.append(neighbor_pairs_list[total_training_indices[i]])\n",
    "total_test_neighbor_random = np.array(total_test_neighbor_random)\n",
    "print(total_test_neighbor_random.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b2b3ce1-eb4b-470f-9334-662f8fc076c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createArray(input_data, num_of_data, is_source, is_bi_directional):\n",
    "    # Initialize an empty list to store the output data\n",
    "    data = []\n",
    "\n",
    "    # Loop through each set of data in input_data\n",
    "    for i in range(num_of_data):\n",
    "        _data = []\n",
    "\n",
    "        # Loop through each pair of data in the current data set\n",
    "        for pair in input_data[i]:\n",
    "\n",
    "            # Process data depending on is_bi_directional flag\n",
    "            if is_bi_directional:\n",
    "                # If is_source is True, append both elements in original order\n",
    "                if is_source:\n",
    "                    _data.append(pair[0])\n",
    "                    _data.append(pair[1])\n",
    "                else:\n",
    "                    # If is_source is False, append elements in reversed order\n",
    "                    _data.append(pair[1])\n",
    "                    _data.append(pair[0])\n",
    "            else:\n",
    "                # If is_bi_directional is False, append only one element depending on is_source flag\n",
    "                if is_source:\n",
    "                    _data.append(pair[0])\n",
    "                else:\n",
    "                    _data.append(pair[1])\n",
    "\n",
    "        # Add the processed data set to the output list\n",
    "        data.append(_data)\n",
    "\n",
    "    # Return the final processed list of data\n",
    "    data = np.array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3faf7297-57ae-4816-8b00-f43bceb56eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 326000)\n",
      "(80, 326000)\n",
      "(80, 163000)\n",
      "(80, 163000)\n",
      "(20, 326000)\n",
      "(20, 326000)\n",
      "(20, 163000)\n",
      "(20, 163000)\n"
     ]
    }
   ],
   "source": [
    "# Creating bi-/uni-directional arrays for processing downstream\n",
    "\n",
    "# Training arrays\n",
    "train_edge_source_bi = createArray(total_train_neighbor_random, 80, True, True)\n",
    "train_edge_dest_bi = createArray(total_train_neighbor_random, 80, False, True)\n",
    "train_edge_source_uni = createArray(total_train_neighbor_random, 80, True, False)\n",
    "train_edge_dest_uni = createArray(total_train_neighbor_random, 80, False, False)\n",
    "\n",
    "# Testing arrays\n",
    "test_edge_source_bi = createArray(total_test_neighbor_random, 20, True, True)\n",
    "test_edge_dest_bi = createArray(total_test_neighbor_random, 20, False, True)\n",
    "test_edge_source_uni = createArray(total_test_neighbor_random, 20, True, False)\n",
    "test_edge_dest_uni = createArray(total_test_neighbor_random, 20, False, False)\n",
    "\n",
    "# Printing the shapes\n",
    "print(train_edge_source_bi.shape)\n",
    "print(train_edge_dest_bi.shape)\n",
    "print(train_edge_source_uni.shape)\n",
    "print(train_edge_dest_uni.shape)\n",
    "print(test_edge_source_bi.shape)\n",
    "print(test_edge_dest_bi.shape)\n",
    "print(test_edge_source_uni.shape)\n",
    "print(test_edge_dest_uni.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29840cb6-9362-4358-b8f1-d53eb2154fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering and creating neighbor pairs sorted by the indices of each neighbor type\n",
    "test_neighbor_pairs_tt = []\n",
    "test_neighbor_pairs_ll = []\n",
    "test_neighbor_pairs_cl = []\n",
    "test_neighbor_pairs_lc = []\n",
    "test_neighbor_pairs_cc = []\n",
    "\n",
    "for i in range(len(labels_testing)):\n",
    "    test_neighbor_pairs_tt.append(neighbor_pairs_list[test_indices_tt_pairs[i]])\n",
    "    test_neighbor_pairs_ll.append(neighbor_pairs_list[test_indices_ll_pairs[i]])\n",
    "    test_neighbor_pairs_cl.append(neighbor_pairs_list[test_indices_cl_pairs[i]])\n",
    "    test_neighbor_pairs_lc.append(neighbor_pairs_list[test_indices_lc_pairs[i]])\n",
    "    test_neighbor_pairs_cc.append(neighbor_pairs_list[test_indices_cc_pairs[i]])\n",
    "\n",
    "test_neighbor_pairs_tt = np.array(test_neighbor_pairs_tt)\n",
    "test_neighbor_pairs_ll = np.array(test_neighbor_pairs_ll)\n",
    "test_neighbor_pairs_cl = np.array(test_neighbor_pairs_cl)\n",
    "test_neighbor_pairs_lc = np.array(test_neighbor_pairs_lc)\n",
    "test_neighbor_pairs_cc = np.array(test_neighbor_pairs_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85e54d01-889f-43bd-8e0e-3e8e6bb1cb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 80000)\n",
      "(20, 80000)\n",
      "(20, 40000)\n",
      "(20, 40000)\n"
     ]
    }
   ],
   "source": [
    "test_edge_tt_source_bi = createArray(test_neighbor_pairs_tt, 20, True, True)\n",
    "test_edge_tt_dest_bi = createArray(test_neighbor_pairs_tt, 20, False, True)\n",
    "test_edge_tt_source_uni = createArray(test_neighbor_pairs_tt, 20, True, False)\n",
    "test_edge_tt_dest_uni = createArray(test_neighbor_pairs_tt, 20, False, False)\n",
    "\n",
    "test_edge_ll_source_bi = createArray(test_neighbor_pairs_ll, 20, True, True)\n",
    "test_edge_ll_dest_bi = createArray(test_neighbor_pairs_ll, 20, False, True)\n",
    "test_edge_ll_source_uni = createArray(test_neighbor_pairs_ll, 20, True, False)\n",
    "test_edge_ll_dest_uni = createArray(test_neighbor_pairs_ll, 20, False, False)\n",
    "\n",
    "test_edge_cl_source_bi = createArray(test_neighbor_pairs_cl, 20, True, True)\n",
    "test_edge_cl_dest_bi = createArray(test_neighbor_pairs_cl, 20, False, True)\n",
    "test_edge_cl_source_uni = createArray(test_neighbor_pairs_cl, 20, True, False)\n",
    "test_edge_cl_dest_uni = createArray(test_neighbor_pairs_cl, 20, False, False)\n",
    "\n",
    "test_edge_lc_source_bi = createArray(test_neighbor_pairs_lc, 20, True, True)\n",
    "test_edge_lc_dest_bi = createArray(test_neighbor_pairs_lc, 20, False, True)\n",
    "test_edge_lc_source_uni = createArray(test_neighbor_pairs_lc, 20, True, False)\n",
    "test_edge_lc_dest_uni = createArray(test_neighbor_pairs_lc, 20, False, False)\n",
    "\n",
    "test_edge_cc_source_bi = createArray(test_neighbor_pairs_cc, 20, True, True)\n",
    "test_edge_cc_dest_bi = createArray(test_neighbor_pairs_cc, 20, False, True)\n",
    "test_edge_cc_source_uni = createArray(test_neighbor_pairs_cc, 20, True, False)\n",
    "test_edge_cc_dest_uni = createArray(test_neighbor_pairs_cc, 20, False, False)\n",
    "\n",
    "\n",
    "# Print shapes\n",
    "print(test_edge_tt_source_bi.shape)\n",
    "print(test_edge_tt_dest_bi.shape)\n",
    "print(test_edge_tt_source_uni.shape)\n",
    "print(test_edge_tt_dest_uni.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5581d6-26fd-4e0f-b554-218de65b2305",
   "metadata": {},
   "source": [
    "## Making the features array for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4003ac05-3ef4-4c4f-96fc-5c1e16469225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a rearranged dictionary from the scaled data dictionary\n",
    "keys = list(scaled_data.keys())\n",
    "values = list(scaled_data.values())\n",
    "features_dict = dict(zip(keys, values))\n",
    "\n",
    "# Here these features are split into training and testing sets\n",
    "features_training = np.concatenate([value for key, value in list(features_dict.items())[:80]])\n",
    "features_testing = np.concatenate([value for key, value in list(features_dict.items())[80:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "712d339a-3b39-438c-9e93-7d75b72c2039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15012160, 5)\n",
      "(3753040, 5)\n"
     ]
    }
   ],
   "source": [
    "print(features_training.shape)\n",
    "print(features_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76e228e6-02da-43be-9fe5-c3af47f5dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training = features_training.reshape(80, 187652, 5)\n",
    "features_testing = features_testing.reshape(20, 187652, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee7222-0ee7-4008-97ad-9fdd10766054",
   "metadata": {},
   "source": [
    "# Creation of the NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "332e5119-9a08-4d0e-950c-04556c4d6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the scaled features into a torch tensor (inputs)\n",
    "x_train = torch.tensor(features_training, dtype=torch.float)\n",
    "x_test = torch.tensor(features_testing, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e09b258-1cac-406f-b69e-6fb047f0515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 2, 326000])\n",
      "torch.Size([80, 2, 163000])\n"
     ]
    }
   ],
   "source": [
    "# Here the correct dimension permutations are applied for the model\n",
    "def make_edge_index_tensor(source, dest):\n",
    "    edge_index = np.stack([np.array(source), np.array(dest)], axis=0)  # Stack into a single NumPy array\n",
    "    return torch.tensor(edge_index, dtype=torch.long).permute(1, 0, 2)\n",
    "\n",
    "# Training set (Bi-directional and Uni-directional)\n",
    "train_edge_indices_bi = make_edge_index_tensor(train_edge_source_bi, train_edge_dest_bi)\n",
    "train_edge_indices_uni = make_edge_index_tensor(train_edge_source_uni, train_edge_dest_uni)\n",
    "\n",
    "print(train_edge_indices_bi.shape)\n",
    "print(train_edge_indices_uni.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80f40fd9-5c34-4d29-9a5d-21676d27ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_data = {\n",
    "    \"tt_bi\": (test_edge_tt_source_bi, test_edge_tt_dest_bi),\n",
    "    \"tt_uni\": (test_edge_tt_source_uni, test_edge_tt_dest_uni),\n",
    "    \"ll_bi\": (test_edge_ll_source_bi, test_edge_ll_dest_bi),\n",
    "    \"ll_uni\": (test_edge_ll_source_uni, test_edge_ll_dest_uni),\n",
    "    \"cl_bi\": (test_edge_cl_source_bi, test_edge_cl_dest_bi),\n",
    "    \"cl_uni\": (test_edge_cl_source_uni, test_edge_cl_dest_uni),\n",
    "    \"lc_bi\": (test_edge_lc_source_bi, test_edge_lc_dest_bi),\n",
    "    \"lc_uni\": (test_edge_lc_source_uni, test_edge_lc_dest_uni),\n",
    "    \"cc_bi\": (test_edge_cc_source_bi, test_edge_cc_dest_bi),\n",
    "    \"cc_uni\": (test_edge_cc_source_uni, test_edge_cc_dest_uni),\n",
    "}\n",
    "\n",
    "# Create and permute tensors for all edge types\n",
    "edge_indices = {key: make_edge_index_tensor(sources, dests) for key, (sources, dests) in edge_index_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c471b72-7d99-44c0-a2ad-8f1774252674",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.expand_dims(labels_training, axis=1)\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = np.expand_dims(labels_testing, axis=1)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f0405-a0bd-4c37-bbf9-9f6b226fd439",
   "metadata": {},
   "source": [
    "### Creation of custom data lists, collate functions, and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca4c809d-b8a1-4904-94aa-39df68909474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class that inherents from the torch.utils.data.Dataset class\n",
    "# The pytorch class is abstract, meaning we need to define certain methods\n",
    "# like __len__() and __getitem__()\n",
    "class custom_dataset(torch.utils.data.Dataset):\n",
    "    # Class constructor that takes in data list and\n",
    "    # stores it as an instance, making it avaliable\n",
    "    # to other methods in the class\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    # Method return length of data set\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    # Method returns data point at index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "# Used to handle batch loading, shuffling, and parallel loading during\n",
    "# training and testing in the ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb267d0a-6165-448b-86fe-b9544af71501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with information regarding a homogenous graph (a graph\n",
    "# where all nodes represent instances of the same type [cells in the\n",
    "# detector] and all edges represent relations of the same type [connections\n",
    "# between cells])\n",
    "def create_data_list(bi_edge_indices, uni_edge_indices, x, y):\n",
    "    data_list = []\n",
    "    for i in range(len(bi_edge_indices)):\n",
    "        # Create the feature matrix\n",
    "        x_mat = x[i]\n",
    "        # Create graph connectivity matrix\n",
    "        edge_index = bi_edge_indices[i]\n",
    "        edge_index, _ = add_self_loops(edge_index)\n",
    "\n",
    "        # Convert y[i] to a PyTorch tensor\n",
    "        y_tensor = torch.tensor(y[i], dtype=torch.long) if not isinstance(\n",
    "            y[i], torch.Tensor) else y[i]\n",
    "\n",
    "        # Create the data object describing a homogeneous graph\n",
    "        data = Data(x=x_mat, edge_index=edge_index,\n",
    "                    edge_index_out=uni_edge_indices[i], y=y_tensor)\n",
    "        data = ToUndirected()(data)\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def collate_data(data_list):\n",
    "    return ([data.x for data in data_list],\n",
    "            [data.edge_index for data in data_list],\n",
    "            [data.edge_index_out for data in data_list],\n",
    "            torch.cat([data.y for data in data_list], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "133514e1-1863-4fbf-a65e-8d7ea40ebf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data lists for all edge types and categories\n",
    "\n",
    "data_list_train = create_data_list(train_edge_indices_bi, train_edge_indices_uni, x_train, y_train)  # Training Edges\n",
    "data_list_tt = create_data_list(edge_indices['tt_bi'], edge_indices['tt_uni'], x_test, np.expand_dims(labels_tt_test, axis=1))  # True-True Edges\n",
    "data_list_ll = create_data_list(edge_indices['ll_bi'], edge_indices['ll_uni'], x_test, np.expand_dims(labels_ll_test, axis=1))  # Lone-lone Edges\n",
    "data_list_cl = create_data_list(edge_indices['cl_bi'], edge_indices['cl_uni'], x_test, np.expand_dims(labels_cl_test, axis=1))  # Cluster-Lone Edges\n",
    "data_list_lc = create_data_list(edge_indices['lc_bi'], edge_indices['lc_uni'], x_test, np.expand_dims(labels_lc_test, axis=1))  # Lone-Cluster Edges\n",
    "data_list_cc = create_data_list(edge_indices['cc_bi'], edge_indices['cc_uni'], x_test, np.expand_dims(labels_cc_test, axis=1))  # Cluster-Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8017172-d912-4ed5-bfce-6cc96326670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size value\n",
    "batch_size = 1\n",
    "\n",
    "# Create the data loaders\n",
    "data_loader = {}\n",
    "data_list_mapping = {\n",
    "    \"train\": data_list_train,  # Training Edges\n",
    "    \"tt\": data_list_tt,           # True-True Edges\n",
    "    \"ll\": data_list_ll,           # Lone-Lone Edges\n",
    "    \"lc\": data_list_lc,           # Lone-Cluster Edges\n",
    "    \"cl\": data_list_cl,           # Cluster-Lone Edges\n",
    "    \"cc\": data_list_cc            # Cluster-Cluster Edges\n",
    "}\n",
    "\n",
    "# Total background dataset\n",
    "data_list_total_bkg = data_list_ll + data_list_cl + data_list_lc + data_list_cc\n",
    "data_loader_total_bkg = torch.utils.data.DataLoader(\n",
    "    custom_dataset(data_list_total_bkg),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=lambda batch: collate_data(batch)\n",
    ")\n",
    "# For the other datasets\n",
    "for key, data_list in data_list_mapping.items():\n",
    "    data_loader[key] = torch.utils.data.DataLoader(\n",
    "        custom_dataset(data_list),\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=lambda batch: collate_data(batch)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b60c0",
   "metadata": {},
   "source": [
    "### Choosing the Device to Run on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd668455",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e39f8e",
   "metadata": {},
   "source": [
    "### Creating the weights to be used in the CrossEntropyLoss() later\n",
    "The class weights will be calculated using this formula in mind (since this formula helps in the mitigation of rare-class overweighting)\n",
    "$${weight}_i = \\frac{{total\\space samples}}{{frequency}_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ea62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_training_flat = labels_training.flatten()\n",
    "\n",
    "# Compute class frequencies (counts) for the entire dataset\n",
    "train_label_counts = Counter(labels_training_flat)\n",
    "\n",
    "# Total number of samples (the number of elements in the flattened array)\n",
    "total_train_samples = labels_training_flat.size\n",
    "\n",
    "# Compute class weights using the new formula\n",
    "class_weights = {label: total_train_samples /\n",
    "                 count for label, count in train_label_counts.items()}\n",
    "\n",
    "# Convert weights to a tensor for PyTorch\n",
    "unique_classes = np.unique(labels_training_flat)  # Get unique classes\n",
    "weight_tensor = torch.tensor(\n",
    "    [class_weights[label] for label in unique_classes], dtype=torch.float).to(device)\n",
    "\n",
    "# Print class weights\n",
    "print(\"Class Weights (Training):\", class_weights)\n",
    "print(\"Weight Tensor (Training):\", weight_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47346e4e-769a-43b5-b1c6-6657be95239c",
   "metadata": {},
   "source": [
    "### Creation of the Multi-Edge Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d43fa6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEdgeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, layer_weights=False, debug=False):\n",
    "        super(MultiEdgeClassifier, self).__init__()\n",
    "        self.debug = debug\n",
    "        self.layer_weights_enabled = layer_weights  # Store setting\n",
    "\n",
    "        # Node embedding layer\n",
    "        self.node_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Initialize convolution and batch norm layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.layer_weights = nn.ParameterList() if layer_weights else None  # Only create if enabled\n",
    "\n",
    "        # First layer\n",
    "        self.convs.append(GCNConv(hidden_dim, 128))\n",
    "        self.bns.append(BatchNorm1d(128))\n",
    "        if layer_weights:\n",
    "            self.layer_weights.append(nn.Parameter(torch.tensor(1.0, requires_grad=True)))\n",
    "\n",
    "        # Additional layers\n",
    "        for i in range(1, num_layers):\n",
    "            in_channels = 128 if i == 1 else 64\n",
    "            out_channels = 64\n",
    "            self.convs.append(GCNConv(in_channels, out_channels))\n",
    "            self.bns.append(BatchNorm1d(out_channels))\n",
    "            if layer_weights:\n",
    "                self.layer_weights.append(nn.Parameter(torch.tensor(1.0, requires_grad=True)))\n",
    "\n",
    "        # Edge classification layer\n",
    "        self.fc = nn.Linear(128, output_dim)\n",
    "\n",
    "    def debug_print(self, message):\n",
    "        if self.debug:\n",
    "            print(message)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_index_out):\n",
    "        # Node embedding\n",
    "        x = self.node_embedding(x)\n",
    "        self.debug_print(f\"Node embedding output shape: {x.shape}\")\n",
    "\n",
    "        if x.dim() == 3 and x.size(0) == 1:  # Check and remove batch dimension\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "        # Loop through convolution layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            self.debug_print(f\"After GCNConv {i+1}: {x.shape}\")\n",
    "            if x.dim() == 3 and x.size(0) == 1:\n",
    "                x = x.squeeze(0)\n",
    "            x = self.bns[i](x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "            # Apply layer weight if enabled\n",
    "            if self.layer_weights_enabled:\n",
    "                x = x * self.layer_weights[i]\n",
    "                self.debug_print(f\"After Layer Weight {i+1}: {x.shape}\")\n",
    "\n",
    "        # Edge representations\n",
    "        edge_rep = torch.cat([x[edge_index_out[0]], x[edge_index_out[1]]], dim=1)\n",
    "        self.debug_print(f\"Edge representation shape: {edge_rep.shape}\")\n",
    "\n",
    "        # Return Logits\n",
    "        edge_scores = self.fc(edge_rep)\n",
    "        return edge_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b203153b-0470-45fb-89b9-f148d7a5855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "hidden_dim = 256\n",
    "output_dim = 5  # Multiclass classification\n",
    "num_layers = 6\n",
    "model = MultiEdgeClassifier(input_dim, hidden_dim, output_dim, num_layers, layer_weights=True)\n",
    "\n",
    "num_epochs = 300\n",
    "criterion = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed329397-631a-470f-8f39-822616f89e6c",
   "metadata": {},
   "source": [
    "### Creation of functions to run our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6d4f020-26fa-4b51-99f6-e1d5dbdbe8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, data_loader, optimizer, criterion):\n",
    "    # Sets the model into training mode\n",
    "    model.train()\n",
    "    # Sends model to GPU if available, otherwise uses the CPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Assumes there is only one batch in the data loader\n",
    "    # Retrieve the single batch from the data loader\n",
    "    batch_x, batch_edge_index, batch_edge_index_out, batch_y = next(\n",
    "        iter(data_loader))\n",
    "\n",
    "    # Sends the input features, the edge indices, and target\n",
    "    # labels to the GPU if available, otherwise the CPU\n",
    "    batch_x = torch.stack(batch_x).to(device)\n",
    "    batch_edge_index = [edge_index.to(device)\n",
    "                        for edge_index in batch_edge_index]\n",
    "    batch_edge_index_out = [edge_index.to(\n",
    "        device) for edge_index in batch_edge_index_out]\n",
    "\n",
    "    # Convert target labels to LongTensor (torch.int64)\n",
    "    batch_y = [y.long().to(device) for y in batch_y]\n",
    "\n",
    "    # Clears the gradients of the model parameters to ensure\n",
    "    # they are not accumulated across batches\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss tracking for subgraphs in the single batch\n",
    "    loss_per_batch = []\n",
    "\n",
    "    # Model processes each graph in the batch one by one\n",
    "    for i in range(len(batch_edge_index)):\n",
    "        # Pass the features and the edge indices into the model and store\n",
    "        # the output (logits)\n",
    "        _output = model(batch_x[i], batch_edge_index[i],\n",
    "                        batch_edge_index_out[i])\n",
    "\n",
    "        # Ensure that model outputs (logits) are of type float32\n",
    "        _output = _output.float()\n",
    "\n",
    "        # Calculate the difference between the model output and the targets\n",
    "        # via the provided criterion (loss function)\n",
    "        loss = criterion(_output.squeeze(), batch_y[i].squeeze())\n",
    "\n",
    "        # This difference is stored in the loss_per_batch list\n",
    "        loss_per_batch.append(loss)\n",
    "\n",
    "    # The average loss across all subgraphs within the single batch is calculated\n",
    "    total_loss_per_batch = sum(loss_per_batch) / len(loss_per_batch)\n",
    "\n",
    "    # Computes the loss gradients with respect to the model parameters\n",
    "    total_loss_per_batch.backward()\n",
    "\n",
    "    # Updates the model parameters using the gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Returns the total loss for the single batch\n",
    "    return total_loss_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b2ed4b9-3bd1-44b7-ba22-f33449b75069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, device, data_loader_true, data_loader_bkg_dict):\n",
    "    all_scores = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        # Process true edges (positive class, label 1)\n",
    "        batch_x, batch_edge_index, batch_edge_index_out, batch_y = next(\n",
    "            iter(data_loader_true))\n",
    "        batch_x = torch.stack(batch_x).to(device)\n",
    "        batch_edge_index = [edge_index.to(device)\n",
    "                            for edge_index in batch_edge_index]\n",
    "        batch_edge_index_out = [edge_index_out.to(\n",
    "            device) for edge_index_out in batch_edge_index_out]\n",
    "\n",
    "        for i in range(len(batch_edge_index)):\n",
    "            test_edge_scores = model(\n",
    "                batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "            test_edge_scores = F.softmax(test_edge_scores, dim=1)\n",
    "\n",
    "            all_scores.append(test_edge_scores)\n",
    "            true_labels.append(torch.ones(test_edge_scores.size(\n",
    "                0), dtype=torch.long, device=device))\n",
    "\n",
    "        # Process background edges\n",
    "        for background_type, data_loader_bkg in data_loader_bkg_dict.items():\n",
    "            batch_x, batch_edge_index, batch_edge_index_out, batch_y = next(\n",
    "                iter(data_loader_bkg))\n",
    "            batch_x = torch.stack(batch_x).to(device)\n",
    "            batch_edge_index = [edge_index.to(device)\n",
    "                                for edge_index in batch_edge_index]\n",
    "            batch_edge_index_out = [edge_index_out.to(\n",
    "                device) for edge_index_out in batch_edge_index_out]\n",
    "\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                test_edge_scores = model(\n",
    "                    batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                test_edge_scores = F.softmax(test_edge_scores, dim=1)\n",
    "\n",
    "                all_scores.append(test_edge_scores)\n",
    "                true_labels.append(torch.full((test_edge_scores.size(\n",
    "                    0),), background_type, dtype=torch.long, device=device))\n",
    "\n",
    "    # Concatenate all scores and labels\n",
    "    all_scores = torch.cat(all_scores, dim=0)\n",
    "    true_labels = torch.cat(true_labels, dim=0)\n",
    "\n",
    "    # Reorder scores and labels to match the desired order (0, 1, 2, 3, 4)\n",
    "    desired_order = [0, 1, 2, 3, 4]\n",
    "    # Ensure reordering matches desired labels\n",
    "    mask = torch.argsort(true_labels).argsort()\n",
    "    all_scores = all_scores[mask]\n",
    "    true_labels = true_labels[mask]\n",
    "\n",
    "    return all_scores.cpu().numpy(), true_labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b988ee58-ccda-4cb3-a100-940d8b80d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_for_train_and_test(model, loader, loss_fn, optimizer, training, device):\n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    all_logits = []  # Store raw logits in testing mode\n",
    "\n",
    "    for batch in loader:\n",
    "        if training:\n",
    "            batch_x, batch_edge_index, batch_edge_index_out, batch_y = batch\n",
    "            batch_y = batch_y.to(device)\n",
    "        else:\n",
    "            batch_x, batch_edge_index, batch_edge_index_out, *_ = batch\n",
    "\n",
    "        # Move features and edge indices to the device\n",
    "        batch_x = torch.stack(batch_x).to(device)\n",
    "        batch_edge_index = [edge_index.to(device)\n",
    "                            for edge_index in batch_edge_index]\n",
    "        batch_edge_index_out = [edge_index_out.to(\n",
    "            device) for edge_index_out in batch_edge_index_out]\n",
    "\n",
    "        if training:\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                logits = model(\n",
    "                    batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(logits, batch_y[i])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(batch_edge_index)):\n",
    "                    logits = model(\n",
    "                        batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                    all_logits.append(logits)  # Store raw logits\n",
    "                    num_batches += 1\n",
    "\n",
    "    average_loss = total_loss / num_batches if training else None\n",
    "    all_logits = torch.cat(all_logits, dim=0).cpu(\n",
    "    ).numpy() if all_logits else None\n",
    "\n",
    "    return average_loss, all_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f794192-3e03-41b3-92e4-05c42fe3cdae",
   "metadata": {},
   "source": [
    "## Running the model over num_epochs iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e288406-216e-43e2-adff-874f831bd12a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m logits_testing_bkg_classes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     total_loss_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Ensure tensor is detached for saving\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     loss_per_epoch\u001b[38;5;241m.\u001b[39mappend(total_loss_per_epoch\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[0;32mIn[57], line 34\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, device, data_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Model processes each graph in the batch one by one\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch_edge_index)):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Pass the features and the edge indices into the model and store\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# the output (logits)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     _output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_edge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_edge_index_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Ensure that model outputs (logits) are of type float32\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     _output \u001b[38;5;241m=\u001b[39m _output\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[43], line 68\u001b[0m, in \u001b[0;36mMultiEdgeClassifier.forward\u001b[0;34m(self, x, edge_index, edge_index_out)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Apply residual connection\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \n\u001b[0;32m---> 68\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual  \n\u001b[1;32m     69\u001b[0m     residual \u001b[38;5;241m=\u001b[39m x  \u001b[38;5;66;03m# Update residual for next layer\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Apply layer weight if enabled\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "data_loader_bkg_dict = {\n",
    "    0: data_loader['ll'],  # For label 0\n",
    "    2: data_loader['cl'],  # For label 2\n",
    "    3: data_loader['lc'],  # For label 3\n",
    "    4: data_loader['cc']   # For label 4\n",
    "}\n",
    "\n",
    "loss_per_epoch = []\n",
    "scores = []\n",
    "truth_labels = []\n",
    "avg_loss_training_true_class = []\n",
    "logits_training_true_class = []\n",
    "avg_loss_testing_true_class = []\n",
    "logits_testing_true_class = []\n",
    "avg_loss_training_bkg_classes = []\n",
    "logits_training_bkg_classes = []\n",
    "avg_loss_testing_bkg_classes = []\n",
    "logits_testing_bkg_classes = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    total_loss_per_epoch = train_model(model, device, data_loader['train'], optimizer, criterion)\n",
    "    # Ensure tensor is detached for saving\n",
    "    loss_per_epoch.append(total_loss_per_epoch.cpu().detach().numpy())\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Test the model\n",
    "    epoch_scores, epoch_true_labels = test_model(model, device, data_loader['tt'], data_loader_bkg_dict)\n",
    "\n",
    "    # Compute the average loss for true and background edges\n",
    "    avgLossTrueTrain, logitsTrueTrain = loss_for_train_and_test(model, data_loader['train'], criterion, optimizer, True, device)\n",
    "    avgLossTrueTest, logitsTrueTest = loss_for_train_and_test(model, data_loader['tt'], criterion, optimizer, False, device)\n",
    "    avgLossBkgTrain, logitsBkgTrain = loss_for_train_and_test(model, data_loader['train'], criterion, optimizer, True, device)\n",
    "    avgLossBkgTest, logtisBkgTest = loss_for_train_and_test(model, data_loader_total_bkg, criterion, optimizer, False, device)\n",
    "\n",
    "    # Store results\n",
    "    scores.append(epoch_scores)\n",
    "    truth_labels.append(epoch_true_labels)\n",
    "    avg_loss_training_true_class.append(avgLossTrueTrain)\n",
    "    logits_training_true_class.append(logitsTrueTrain)\n",
    "    avg_loss_testing_true_class.append(avgLossTrueTest)\n",
    "    logits_testing_true_class.append(logitsTrueTest)\n",
    "    avg_loss_training_bkg_classes.append(avgLossBkgTrain)\n",
    "    logits_training_bkg_classes.append(logitsBkgTrain)\n",
    "    avg_loss_testing_bkg_classes.append(avgLossBkgTest)\n",
    "    logits_testing_bkg_classes.append(logtisBkgTest)\n",
    "\n",
    "    # Print the loss for the current epoch\n",
    "    print(f\"Epoch: {epoch+1} | Total Loss Per Epoch: {total_loss_per_epoch.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029ed61-47ce-4157-aa55-5d75b7c08202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300, 163000, 5)\n",
      "(300, 163000)\n",
      "(300,)\n",
      "(300,)\n",
      "(300,)\n",
      "(300, 800000, 5)\n",
      "(300,)\n",
      "(300,)\n",
      "(300,)\n",
      "(300, 2460000, 5)\n"
     ]
    }
   ],
   "source": [
    "loss_per_epoch = np.array(loss_per_epoch)\n",
    "scores = np.array(scores)\n",
    "truth_labels = np.array(truth_labels)\n",
    "avg_loss_training_true_class = np.array(avg_loss_training_true_class)\n",
    "logits_training_true_class = np.array(logits_training_true_class)\n",
    "avg_loss_testing_true_class = np.array(avg_loss_testing_true_class)\n",
    "logits_testing_true_class = np.array(logits_testing_true_class)\n",
    "avg_loss_training_bkg_classes = np.array(avg_loss_training_bkg_classes)\n",
    "logits_training_bkg_classes = np.array(logits_training_bkg_classes)\n",
    "avg_loss_testing_bkg_classes = np.array(avg_loss_testing_bkg_classes)\n",
    "logits_testing_bkg_classes = np.array(logits_testing_bkg_classes)\n",
    "\n",
    "print(loss_per_epoch.shape)\n",
    "print(scores.shape)\n",
    "print(truth_labels.shape)\n",
    "print(avg_loss_training_true_class.shape)\n",
    "print(logits_training_true_class.shape)\n",
    "print(avg_loss_testing_true_class.shape)\n",
    "print(logits_testing_true_class.shape)\n",
    "print(avg_loss_training_bkg_classes.shape)\n",
    "print(logits_training_bkg_classes.shape)\n",
    "print(avg_loss_testing_bkg_classes.shape)\n",
    "print(logits_testing_bkg_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48f482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /storage/mxg1065/model_6layers_seed_42_a.pkl\n"
     ]
    }
   ],
   "source": [
    "def save_data_pickle(file_name, save_path, data_dict):\n",
    "    \"\"\"\n",
    "    Saves the given data dictionary into a Pickle (.pkl) file.\n",
    "    \n",
    "    Parameters:\n",
    "        file_name (str): Name of the Pickle file.\n",
    "        save_path (str): Directory where the file will be saved.\n",
    "        data_dict (dict): Dictionary containing data to be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Full file path\n",
    "    full_path = os.path.join(save_path, file_name)\n",
    "\n",
    "    with open(full_path, 'wb') as file:\n",
    "        pickle.dump(data_dict, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(f\"Data successfully saved to {full_path}\")\n",
    "\n",
    "# Create the dictionary\n",
    "data_dict = {\n",
    "    \"loss_per_epoch\": loss_per_epoch,\n",
    "    \"scores\": scores,\n",
    "    \"truth_labels\": truth_labels,\n",
    "    \"avg_loss_training_true_class\": avg_loss_training_true_class,\n",
    "    \"logits_training_true_class\": logits_training_true_class,\n",
    "    \"avg_loss_testing_true_class\": avg_loss_testing_true_class,\n",
    "    \"logits_testing_true_class\": logits_testing_true_class,\n",
    "    \"avg_loss_training_bkg_classes\": avg_loss_training_bkg_classes,\n",
    "    \"logits_training_bkg_classes\": logits_training_bkg_classes,\n",
    "    \"avg_loss_testing_bkg_classes\": avg_loss_testing_bkg_classes,\n",
    "    \"logits_testing_bkg_classes\": logits_testing_bkg_classes\n",
    "}\n",
    "\n",
    "# Save the file\n",
    "save_data_pickle('model_6layers_16heads_hybrid_GCNConvGAT_residual.pkl', \"/storage/mxg1065\", data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
