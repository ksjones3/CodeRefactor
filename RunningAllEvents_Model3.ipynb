{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b1226-a989-4de3-bfce-87f5c1a3dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "random.seed(42)  \n",
    "\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns  \n",
    "\n",
    "from collections import Counter  \n",
    "\n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F  \n",
    "import torch.optim as optim  \n",
    "\n",
    "from torch.nn import BatchNorm1d  \n",
    "from torch_geometric.data import Data  \n",
    "from torch_geometric.nn import GCNConv  \n",
    "from torch_geometric.transforms import ToUndirected  \n",
    "from torch_geometric.utils import add_self_loops  \n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix  \n",
    "from sklearn.preprocessing import MinMaxScaler, label_binarize\n",
    "\n",
    "import uproot  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f04274",
   "metadata": {},
   "source": [
    "# Model Description:\n",
    "\n",
    "* Num of Features: 5\n",
    "* Num of Classes: 5\n",
    "* Num of Layers in the Model: 5\n",
    "* Num of Epochs: 300\n",
    "* Loss function: nn.CrossEntropyLoss(weight=weight_tensor) where the weight tensor is given by $${weight}_i = \\frac{{total\\space samples}}{{frequency}_i}$$\n",
    "* Optimizer: optim.Adam(model.parameters(), lr=0.01)\n",
    "* Scheduler: torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)\n",
    "* Sampling:\n",
    "True-True: 3000\n",
    "Lone-Lone: 3000\n",
    "Cluster-Lone: 3000\n",
    "Lone-Cluster: 12000\n",
    "Cluster-Cluster: 3000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf1431b-1dc6-49ce-8f62-8c3230125cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['analysis;1']\n"
     ]
    }
   ],
   "source": [
    "file = uproot.open('/home/mxg1065/MyxAODAnalysis_super3D.outputs.root')\n",
    "print(file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "510207d7-9622-4c62-b9f3-5f5e724ff76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RunNumber', 'EventNumber', 'cell_eta', 'cell_phi', 'cell_x', 'cell_y', 'cell_z', 'cell_subCalo', 'cell_sampling', 'cell_size', 'cell_hashID', 'neighbor', 'seedCell_id', 'cell_e', 'cell_noiseSigma', 'cell_SNR', 'cell_time', 'cell_weight', 'cell_truth', 'cell_truth_indices', 'cell_shared_indices', 'cell_cluster_index', 'cluster_to_cell_indices', 'cluster_to_cell_weights', 'cell_to_cluster_e', 'cell_to_cluster_eta', 'cell_to_cluster_phi', 'cluster_eta', 'cluster_phi', 'cluster_e', 'cellsNo_cluster', 'clustersNo_event', 'jetEnergyWtdTimeAve', 'jetEta', 'jetPhi', 'jetE', 'jetPt', 'jetNumberPerEvent', 'cellIndices_per_jet']\n"
     ]
    }
   ],
   "source": [
    "tree = file['analysis;1']\n",
    "branches = tree.arrays()\n",
    "print(tree.keys()) # Variables per event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ff68c-72cf-46c9-8e01-baf83ebd9ed8",
   "metadata": {},
   "source": [
    "### Prepairing the Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00463fb9-9804-4bb7-9f7a-bdc68bb94568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 events and 187652 cells\n",
    "# Arrays containing information about the energy, noise, snr, \n",
    "cell_e = np.array(branches['cell_e'])\n",
    "cell_noise = np.array(branches['cell_noiseSigma'])\n",
    "cell_snr = np.array(branches['cell_SNR'])\n",
    "cell_eta = np.array(branches['cell_eta'])\n",
    "cell_phi = np.array(branches['cell_phi'])\n",
    "\n",
    "# Represents the index of the cluster that each cell corresponds to. If the index\n",
    "# is 0, that means that the given cell does not belong to a cluster.\n",
    "cell_to_cluster_index = np.array(branches['cell_cluster_index'])\n",
    "\n",
    "# For each entry, contains the IDs of cells neighboring a given cell\n",
    "neighbor = branches['neighbor']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "448dbc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Events: 100\n",
      "Number of Cells: 187652\n"
     ]
    }
   ],
   "source": [
    "num_of_events = len(cell_snr)\n",
    "num_of_cells = len(cell_snr[0])\n",
    "print(\"Number of Events:\", num_of_events)\n",
    "print(\"Number of Cells:\", num_of_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0285e4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02640459 0.24943821 0.09700817 0.23466232 0.5085498 ]\n",
      " [0.02567646 0.24627675 0.09700815 0.23466876 0.52418756]\n",
      " [0.02508186 0.24369499 0.09700817 0.23467347 0.5398244 ]\n",
      " ...\n",
      " [0.02632877 0.24791998 0.03177016 0.62017125 0.4921177 ]\n",
      " [0.02705116 0.2511391  0.07512318 0.6461531  0.4921177 ]\n",
      " [0.02638626 0.24820389 0.04149057 0.6731673  0.4921177 ]]\n",
      "(187652, 5)\n"
     ]
    }
   ],
   "source": [
    "# We use the data arrays to crete a data dictionary, where each entry corresponds\n",
    "# to the data of a given event; we scale this data.\n",
    "data = {}\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    data[f'data_{i}'] = np.concatenate((np.expand_dims(cell_snr[i], axis=1),\n",
    "                                        np.expand_dims(cell_e[i], axis=1),\n",
    "                                        np.expand_dims(cell_noise[i], axis=1),\n",
    "                                        np.expand_dims(cell_eta[i], axis=1),\n",
    "                                        np.expand_dims(cell_phi[i], axis=1)), axis=1)\n",
    "    \n",
    "# We combine the data into one array and apply the MinMaxScaler\n",
    "combined_data = np.vstack([data[key] for key in data])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_combined_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "# The scaled data is split to have the save structure as the original data dict\n",
    "scaled_data = {}\n",
    "start_idx = 0\n",
    "for i in range(num_of_events):\n",
    "    end_idx = start_idx + data[f\"data_{i}\"].shape[0]\n",
    "    scaled_data[f\"data_{i}\"] = scaled_combined_data[start_idx:end_idx]\n",
    "    start_idx = end_idx\n",
    "\n",
    "print(scaled_data[\"data_0\"])\n",
    "print(scaled_data[\"data_0\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54959f5c-b033-443c-8bf1-e73358d696d2",
   "metadata": {},
   "source": [
    "### Preparing Neighbor Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b8ff2a-44c4-4110-9abc-13979f761a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186986 187352]\n"
     ]
    }
   ],
   "source": [
    "# The IDs of the broken cells (those with zero noise) are collected\n",
    "broken_cells = []\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    cells = np.argwhere(cell_noise[i]==0)\n",
    "    broken_cells = np.squeeze(cells)\n",
    "\n",
    "print(broken_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70ed47c5-6530-467f-ba45-d57e898eabb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the values associated with neighbor[0] and neighbor[1] are all equal\n",
    "# we will just work with neighbor[0] to simplify our calculations\n",
    "neighbor = neighbor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d8b1108-24d8-4d37-a7eb-86a621744c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We loop through the neighbor awkward array and remove the IDs associated\n",
    "# with the broken cells.  Loops through all cells in the neighbor list. If the loop \n",
    "# reaches the cell numbers 186986 or 187352, loop skips over these inoperative cells. \n",
    "# The final list contains tuples (i,j) where i is the cell ID in question and the \n",
    "# js are the neighboring cell IDs\n",
    "neighbor_pairs_list = []\n",
    "\n",
    "for i in range(num_of_cells):\n",
    "    if i in broken_cells:\n",
    "        continue\n",
    "    for j in neighbor[i]:\n",
    "        if j in broken_cells:\n",
    "            continue\n",
    "        neighbor_pairs_list.append((i, int(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef500146-1e14-46c9-a3a7-8e6dbab0b566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully excluded broken cells.\n"
     ]
    }
   ],
   "source": [
    "# This code checks to see if the broken cells were removed\n",
    "found_broken_cells = []\n",
    "\n",
    "for pair in neighbor_pairs_list:\n",
    "    # Loop through each cell in pair\n",
    "    for cell in pair:\n",
    "        # If the cell is broken, appends to list\n",
    "        if cell in broken_cells:\n",
    "            found_broken_cells.append(cell)\n",
    "\n",
    "if found_broken_cells:\n",
    "    print(\"Error: Broken cells are still present in neighbor pairs.\")\n",
    "else:\n",
    "    print(\"Successfully excluded broken cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1af53e81-a169-4b8a-8878-a0a3bd1a643f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 90345 119588]\n",
      " [  4388  17680]\n",
      " [ 39760  39825]\n",
      " ...\n",
      " [159757 168717]\n",
      " [ 62911  78974]\n",
      " [135353 135609]]\n",
      "(1250242, 2)\n"
     ]
    }
   ],
   "source": [
    "# These functions remove permutation variants\n",
    "def canonical_form(t):\n",
    "    return tuple(sorted(t))\n",
    "\n",
    "def remove_permutation_variants(tuple_list):\n",
    "    unique_tuples = set(canonical_form(t) for t in tuple_list)\n",
    "    return [tuple(sorted(t)) for t in unique_tuples]\n",
    "\n",
    "neighbor_pairs_list = np.array(remove_permutation_variants(neighbor_pairs_list))\n",
    "print(neighbor_pairs_list)\n",
    "print(neighbor_pairs_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce75dc",
   "metadata": {},
   "source": [
    "### Plotting the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc2555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize dictionary to store features by class\n",
    "# features_by_class = {cls: {'SNR': [], 'Energy': [], 'Noise': []}\n",
    "#                      for cls in range(5)}\n",
    "\n",
    "# # Precompute features for all events in a vectorized way\n",
    "# for i in range(num_of_events):  # Iterate through events\n",
    "#     for pair_idx, pair in enumerate(neighbor_pairs_list):\n",
    "#         class_label = labels_for_neighbor_pairs[i][pair_idx]\n",
    "\n",
    "#         # Vectorized feature extraction for cell pairs\n",
    "#         cell_1_features = [cell_SNR[i][pair[0]], cell_e[i]\n",
    "#                            [pair[0]], cell_noiseSigma[i][pair[0]]]\n",
    "#         cell_2_features = [cell_SNR[i][pair[1]], cell_e[i]\n",
    "#                            [pair[1]], cell_noiseSigma[i][pair[1]]]\n",
    "\n",
    "#         # Append features to the corresponding class\n",
    "#         features_by_class[class_label]['SNR'] += [cell_1_features[0],\n",
    "#                                                   cell_2_features[0]]\n",
    "#         features_by_class[class_label]['Energy'] += [\n",
    "#             cell_1_features[1], cell_2_features[1]]\n",
    "#         features_by_class[class_label]['Noise'] += [cell_1_features[2],\n",
    "#                                                     cell_2_features[2]]\n",
    "\n",
    "# # Function to precompute bin edges for all features\n",
    "\n",
    "\n",
    "# def precompute_bins(features_by_class, snr_xlim=None, energy_xlim=None, noise_xlim=None, bins=50):\n",
    "#     bins_dict = {}\n",
    "\n",
    "#     # Compute bins for SNR\n",
    "#     if snr_xlim:\n",
    "#         bins_dict['SNR'] = np.linspace(snr_xlim[0], snr_xlim[1], bins + 1)\n",
    "#     else:\n",
    "#         all_snr = np.concatenate([features['SNR']\n",
    "#                                  for features in features_by_class.values()])\n",
    "#         bins_dict['SNR'] = np.histogram_bin_edges(all_snr, bins=bins)\n",
    "\n",
    "#     # Compute bins for Energy\n",
    "#     if energy_xlim:\n",
    "#         bins_dict['Energy'] = np.linspace(\n",
    "#             energy_xlim[0], energy_xlim[1], bins + 1)\n",
    "#     else:\n",
    "#         all_energy = np.concatenate([features['Energy']\n",
    "#                                     for features in features_by_class.values()])\n",
    "#         bins_dict['Energy'] = np.histogram_bin_edges(all_energy, bins=bins)\n",
    "\n",
    "#     # Compute bins for Noise\n",
    "#     if noise_xlim:\n",
    "#         bins_dict['Noise'] = np.linspace(\n",
    "#             noise_xlim[0], noise_xlim[1], bins + 1)\n",
    "#     else:\n",
    "#         all_noise = np.concatenate([features['Noise']\n",
    "#                                    for features in features_by_class.values()])\n",
    "#         bins_dict['Noise'] = np.histogram_bin_edges(all_noise, bins=bins)\n",
    "\n",
    "#     return bins_dict\n",
    "\n",
    "\n",
    "# # Optimized function to plot histograms using precomputed bins\n",
    "# def plot_histograms_optimized(features_by_class, bins_dict):\n",
    "#     for class_label, features in features_by_class.items():\n",
    "#         if len(features['SNR']) > 0:  # Ensure there are features to plot\n",
    "#             plt.figure(figsize=(15, 5))\n",
    "\n",
    "#             # Plot SNR histogram with precomputed bins\n",
    "#             plt.subplot(1, 3, 1)\n",
    "#             plt.hist(features['SNR'], bins=bins_dict['SNR'],\n",
    "#                      alpha=0.6, label=f'Class {class_label}')\n",
    "#             plt.xlabel('SNR')\n",
    "#             plt.ylabel('Frequency')\n",
    "#             plt.title(f'Class {class_label}: SNR')\n",
    "#             plt.grid(True)\n",
    "\n",
    "#             # Plot Energy histogram with precomputed bins\n",
    "#             plt.subplot(1, 3, 2)\n",
    "#             plt.hist(features['Energy'], bins=bins_dict['Energy'],\n",
    "#                      alpha=0.6, label=f'Class {class_label}')\n",
    "#             plt.xlabel('Energy')\n",
    "#             plt.ylabel('Frequency')\n",
    "#             plt.title(f'Class {class_label}: Energy')\n",
    "#             plt.grid(True)\n",
    "\n",
    "#             # Plot Noise histogram with precomputed bins\n",
    "#             plt.subplot(1, 3, 3)\n",
    "#             plt.hist(features['Noise'], bins=bins_dict['Noise'],\n",
    "#                      alpha=0.6, label=f'Class {class_label}')\n",
    "#             plt.xlabel('Noise')\n",
    "#             plt.ylabel('Frequency')\n",
    "#             plt.title(f'Class {class_label}: Noise')\n",
    "#             plt.grid(True)\n",
    "\n",
    "#             plt.tight_layout()\n",
    "#             plt.suptitle(\n",
    "#                 f'Feature Distributions for Class {class_label}', fontsize=16)\n",
    "#             plt.subplots_adjust(top=0.85)  # Adjust title to prevent overlap\n",
    "#             plt.show()\n",
    "\n",
    "\n",
    "# # Precompute bins for SNR, Energy, and Noise\n",
    "# bins_dict = precompute_bins(features_by_class, snr_xlim=(-6.5, 6.5),\n",
    "#                             energy_xlim=(-500, 500), noise_xlim=(0, 500))\n",
    "\n",
    "# # Call the optimized function to plot histograms\n",
    "# plot_histograms_optimized(features_by_class, bins_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3acb5305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dictionary to store cluster pairs for all events\n",
    "# all_events_cluster_pairs = {}\n",
    "\n",
    "# # Loop through each event\n",
    "# for i in range(num_of_events):\n",
    "#     # Get the cluster indices for the current event\n",
    "#     # Assuming it's a list of arrays for each event\n",
    "#     cluster_cells_belong_to_event = cell_to_cluster_index[i]\n",
    "\n",
    "#     # Map the neighbor pairs to their respective cluster indices for the current event\n",
    "#     cluster_pairs = [\n",
    "#         (cluster_cells_belong_to_event[pair[0]],\n",
    "#          cluster_cells_belong_to_event[pair[1]])\n",
    "#         for pair in neighbor_pairs_list\n",
    "#     ]\n",
    "\n",
    "#     # Store the cluster pairs for the current event\n",
    "#     all_events_cluster_pairs[f\"event_{i}\"] = cluster_pairs\n",
    "\n",
    "# # Now `all_events_cluster_pairs` is a dictionary containing cluster pairs for all events\n",
    "\n",
    "# # Visualize Cluster Pairs for All Events\n",
    "# # Create a 2x3 grid for the subplots\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(15, 10))  # Adjust size as needed\n",
    "# axes = axes.flatten()  # Flatten the 2x3 array for easier indexing\n",
    "\n",
    "# # Loop over labels (0 to 4)\n",
    "# for label in range(5):\n",
    "#     cell1_clusters_all_events = []\n",
    "#     cell2_clusters_all_events = []\n",
    "\n",
    "#     # Process each event\n",
    "#     for event_id, cluster_pairs in all_events_cluster_pairs.items():\n",
    "#         # Get the labels for neighbor pairs in the current event\n",
    "#         labels_for_event = labels_for_neighbor_pairs[int(\n",
    "#             event_id.split(\"_\")[1])]  # Get labels for this event\n",
    "\n",
    "#         # Filter pairs by the current label\n",
    "#         label_indices = [i for i, l in enumerate(\n",
    "#             labels_for_event) if l == label]\n",
    "#         pairs_for_label = [cluster_pairs[i] for i in label_indices]\n",
    "\n",
    "#         # Separate cluster indices for plotting\n",
    "#         cell1_clusters = [pair[0] for pair in pairs_for_label]\n",
    "#         cell2_clusters = [pair[1] for pair in pairs_for_label]\n",
    "\n",
    "#         # Accumulate data across events\n",
    "#         cell1_clusters_all_events.extend(cell1_clusters)\n",
    "#         cell2_clusters_all_events.extend(cell2_clusters)\n",
    "\n",
    "#     # Plot for the current label\n",
    "#     ax = axes[label]\n",
    "#     ax.scatter(cell1_clusters_all_events, cell2_clusters_all_events,\n",
    "#                label=f\"Label {label}\", alpha=0.5)\n",
    "#     ax.set_xlabel(\"Cell 1 Cluster Index\")\n",
    "#     ax.set_ylabel(\"Cell 2 Cluster Index\")\n",
    "#     ax.set_title(f\"Cluster Pairs for Label {label}\")\n",
    "\n",
    "# # Hide the last (empty) subplot if fewer than 6 subplots\n",
    "# if len(axes) > 5:\n",
    "#     axes[5].axis('off')\n",
    "\n",
    "# plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddec80a-83ec-424f-92c5-4370d7d4292b",
   "metadata": {},
   "source": [
    "### Creating Labels for the Neighbor Pairs\n",
    "\n",
    "For a given pair of cells and the IDs of the clusters that they belong to (i, j), if\n",
    "1. i=j and both are nonzero, then both cells are part of the same cluster. \n",
    "    * We call these True-True pairs and label them with 1\n",
    "2. i=j and both are zero, then both cells are not part of any cluster. \n",
    "    * We call these Lone-Lone pairs and label them with 0\n",
    "3. i is nonzero and j=0, then cell i is part of a cluster while cell j is not. \n",
    "    * We call these Cluster-Lone pairs and label them with 2\n",
    "4. i=0 and j is nonzero, then cell i is not part of a cluste while cell j is. \n",
    "    * We call these Lone-Cluster pairs and label them with 3\n",
    "5. i is not the same as j and both are nonzero, then both cells are part of different clusters. \n",
    "    * We call these Cluster-Cluster pairs and label them with 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8d4fede-a6aa-46da-9c8a-a9d2bc9d09de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1250242)\n",
      "[[0 0 0 ... 0 0 2]\n",
      " [3 3 0 ... 0 0 2]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 2 0 0]]\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Initialize array for labels\n",
    "labels_for_neighbor_pairs = np.zeros((num_of_events, len(neighbor_pairs_list)), dtype=int)\n",
    "\n",
    "# Extracting the individual cells within a cell pair\n",
    "cell_0 = cell_to_cluster_index[:, neighbor_pairs_list[:, 0]]\n",
    "cell_1 = cell_to_cluster_index[:, neighbor_pairs_list[:, 1]]\n",
    "\n",
    "# Computing labels using vectorized operations\n",
    "same_cluster = cell_0 == cell_1 \n",
    "both_nonzero = (cell_0 != 0) & (cell_1 != 0)\n",
    "\n",
    "# Lone-Lone (0)\n",
    "labels_for_neighbor_pairs[same_cluster & (cell_0 == 0)] = 0\n",
    "\n",
    "# True-True (1)\n",
    "labels_for_neighbor_pairs[same_cluster & (cell_0 != 0)] = 1\n",
    "\n",
    "# Cluster-Cluster (4)\n",
    "labels_for_neighbor_pairs[~same_cluster & both_nonzero] = 4\n",
    "\n",
    "# Lone-Cluster (3)\n",
    "labels_for_neighbor_pairs[~same_cluster & (cell_0 == 0) & (cell_1 != 0)] = 3\n",
    "\n",
    "# Cluster-Lone (2)\n",
    "labels_for_neighbor_pairs[~same_cluster & (cell_0 != 0) & (cell_1 == 0)] = 2\n",
    "\n",
    "print(labels_for_neighbor_pairs.shape)\n",
    "print(labels_for_neighbor_pairs)\n",
    "print(np.unique(labels_for_neighbor_pairs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd492f7d-cd91-4387-b354-39178f815bd9",
   "metadata": {},
   "source": [
    "### Preparing the Data for Multi-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe3e7023-f51e-495e-a325-bebb07ae496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we collect the indices of the neighbor pairs by the pair type\n",
    "indices_for_tt_pairs = []  # Label 1\n",
    "indices_for_ll_pairs = []  # Label 0\n",
    "indices_for_cl_pairs = []  # Label 2\n",
    "indices_for_lc_pairs = []  # Label 3\n",
    "indices_for_cc_pairs = []  # Label 4\n",
    "\n",
    "for i in range(num_of_events):\n",
    "    indices_for_tt_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 1)[0]))\n",
    "    indices_for_ll_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 0)[0]))\n",
    "    indices_for_cl_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 2)[0]))\n",
    "    indices_for_lc_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 3)[0]))\n",
    "    indices_for_cc_pairs.append(list(np.where(labels_for_neighbor_pairs[i] == 4)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99704084-bfba-44fd-8ef9-a1a63b520950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we collect the number of each pair type across the events\n",
    "number_of_tt_pairs = [len(indices_for_tt_pairs[i])for i in range(num_of_events)]\n",
    "number_of_ll_pairs = [len(indices_for_ll_pairs[i])for i in range(num_of_events)]\n",
    "number_of_cl_pairs = [len(indices_for_cl_pairs[i])for i in range(num_of_events)]\n",
    "number_of_lc_pairs = [len(indices_for_lc_pairs[i])for i in range(num_of_events)]\n",
    "number_of_cc_pairs = [len(indices_for_cc_pairs[i])for i in range(num_of_events)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc4086ac-778d-4cb5-b102-9e6bab748957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we perform a 70-30 split on the indices of neighbor pairs\n",
    "training_indices_tt = indices_for_tt_pairs[:70]\n",
    "training_indices_ll = indices_for_ll_pairs[:70]\n",
    "training_indices_cl = indices_for_cl_pairs[:70]\n",
    "training_indices_lc = indices_for_lc_pairs[:70]\n",
    "training_indices_cc = indices_for_cc_pairs[:70]\n",
    "\n",
    "testing_indices_tt = indices_for_tt_pairs[70:]\n",
    "testing_indices_ll = indices_for_ll_pairs[70:]\n",
    "testing_indices_cl = indices_for_cl_pairs[70:]\n",
    "testing_indices_lc = indices_for_lc_pairs[70:]\n",
    "testing_indices_cc = indices_for_cc_pairs[70:]\n",
    "\n",
    "# Here we perform a 70-30 split on the number of neighbor pairs\n",
    "training_num_tt = number_of_tt_pairs[:70]\n",
    "training_num_ll = number_of_ll_pairs[:70]\n",
    "training_num_cl = number_of_cl_pairs[:70]\n",
    "training_num_lc = number_of_lc_pairs[:70]\n",
    "training_num_cc = number_of_cc_pairs[:70]\n",
    "\n",
    "testing_num_tt = number_of_tt_pairs[70:]\n",
    "testing_num_ll = number_of_ll_pairs[70:]\n",
    "testing_num_cl = number_of_cl_pairs[70:]\n",
    "testing_num_lc = number_of_lc_pairs[70:]\n",
    "testing_num_cc = number_of_cc_pairs[70:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b51041e4-ab4a-42d3-b477-70038ceabcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of pairs for training:\n",
      "True-True 45600\n",
      "Lone-Lone 926119\n",
      "Cluster-Lone 41654\n",
      "Lone-Cluster 45689\n",
      "Cluster-Cluster 3334\n",
      "\n",
      "Minimum number of pairs for testing:\n",
      "True-True 51518\n",
      "Lone-Lone 906630\n",
      "Cluster-Lone 44444\n",
      "Lone-Cluster 48069\n",
      "Cluster-Cluster 4936\n"
     ]
    }
   ],
   "source": [
    "# We check the minimum number of each pair type across the events. When we\n",
    "# randomly sample from the indices, if our sample is greater than the minimum\n",
    "# numbers, then we will run into errors\n",
    "print(\"Minimum number of pairs for training:\")\n",
    "print(\"True-True\", min(training_num_tt))\n",
    "print(\"Lone-Lone\", min(training_num_ll))\n",
    "print(\"Cluster-Lone\", min(training_num_cl))\n",
    "print(\"Lone-Cluster\", min(training_num_lc))\n",
    "print(\"Cluster-Cluster\", min(training_num_cc))\n",
    "print('\\nMinimum number of pairs for testing:')\n",
    "print(\"True-True\", min(testing_num_tt))\n",
    "print(\"Lone-Lone\", min(testing_num_ll))\n",
    "print(\"Cluster-Lone\", min(testing_num_cl))\n",
    "print(\"Lone-Cluster\", min(testing_num_lc))\n",
    "print(\"Cluster-Cluster\", min(testing_num_cc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb64b1b6-ff88-40d7-b92f-b5a67119a5a9",
   "metadata": {},
   "source": [
    "### Making training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfc9affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_sample_num = 12000\n",
    "small_sample_num = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a405c2fb-e5ed-44ae-9d21-a65191e0ced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 3000)\n",
      "(70, 21000)\n",
      "(30, 3000)\n",
      "(30, 21000)\n"
     ]
    }
   ],
   "source": [
    "train_indices_tt_pairs = np.array([random.sample(row, small_sample_num) for row in training_indices_tt])\n",
    "train_indices_ll_pairs = np.array([random.sample(row, small_sample_num) for row in training_indices_ll])\n",
    "train_indices_cl_pairs = np.array([random.sample(row, small_sample_num) for row in training_indices_cl])\n",
    "train_indices_lc_pairs = np.array([random.sample(row, big_sample_num) for row in training_indices_lc])\n",
    "train_indices_cc_pairs = np.array([random.sample(row, small_sample_num) for row in training_indices_cc])\n",
    "train_indices_bkg_pairs = np.concatenate([train_indices_ll_pairs,\n",
    "                                          train_indices_cl_pairs,\n",
    "                                          train_indices_lc_pairs,\n",
    "                                          train_indices_cc_pairs], axis=1)\n",
    "\n",
    "test_indices_tt_pairs = np.array([random.sample(row, small_sample_num) for row in testing_indices_tt])\n",
    "test_indices_ll_pairs = np.array([random.sample(row, small_sample_num) for row in testing_indices_ll])\n",
    "test_indices_cl_pairs = np.array([random.sample(row, small_sample_num) for row in testing_indices_cl])\n",
    "test_indices_lc_pairs = np.array([random.sample(row, big_sample_num) for row in testing_indices_lc])\n",
    "test_indices_cc_pairs = np.array([random.sample(row, small_sample_num) for row in testing_indices_cc])\n",
    "test_indices_bkg_pairs = np.concatenate([test_indices_ll_pairs,\n",
    "                                         test_indices_cl_pairs,\n",
    "                                         test_indices_lc_pairs,\n",
    "                                         test_indices_cc_pairs], axis=1)\n",
    "\n",
    "print(train_indices_tt_pairs.shape)\n",
    "print(train_indices_bkg_pairs.shape)\n",
    "print(test_indices_tt_pairs.shape)\n",
    "print(test_indices_bkg_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08243102-f30f-4319-8c3a-0e8079f3f8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 24000)\n",
      "(30, 24000)\n"
     ]
    }
   ],
   "source": [
    "total_training_indices = np.concatenate((train_indices_tt_pairs, train_indices_bkg_pairs), axis=1)\n",
    "total_testing_indices = np.concatenate((test_indices_tt_pairs, test_indices_bkg_pairs), axis=1)\n",
    "print(total_training_indices.shape)\n",
    "print(total_testing_indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490145b9-1354-4a6e-b3fb-aede8bca58d3",
   "metadata": {},
   "source": [
    "### Randomizing the indicies and creating labels for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7422bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 24000)\n",
      "(30, 24000)\n"
     ]
    }
   ],
   "source": [
    "# Creating training labels\n",
    "labels_tt_train = np.ones((70, small_sample_num), dtype=int)  # True-True\n",
    "labels_ll_train = np.zeros((70, small_sample_num), dtype=int)  # Lone-Lone\n",
    "labels_cl_train = np.ones((70, small_sample_num), dtype=int)*2  # Cluster-Lone\n",
    "labels_lc_train = np.ones((70, big_sample_num), dtype=int)*3  # Lone-Cluster\n",
    "labels_cc_train = np.ones((70, small_sample_num), dtype=int)*4  # Cluster-Cluster\n",
    "labels_bkg_train = np.concatenate((labels_ll_train, labels_cl_train, labels_lc_train, labels_cc_train), axis=1)\n",
    "labels_training = np.concatenate((labels_tt_train, labels_bkg_train), axis=1)\n",
    "\n",
    "# Creating testing labels\n",
    "labels_tt_test = np.ones((30, small_sample_num), dtype=int)  # True-True\n",
    "labels_ll_test = np.zeros((30, small_sample_num), dtype=int)  # Lone-Lone\n",
    "labels_cl_test = np.ones((30, small_sample_num), dtype=int)*2  # Cluster-Lone\n",
    "labels_lc_test = np.ones((30, big_sample_num), dtype=int)*3  # Lone-Cluster\n",
    "labels_cc_test = np.ones((30, small_sample_num), dtype=int)*4  # Cluster-Cluster\n",
    "labels_bkg_test = np.concatenate((labels_ll_test, labels_cl_test, labels_lc_test, labels_cc_test), axis=1)\n",
    "labels_testing = np.concatenate((labels_tt_test, labels_bkg_test), axis=1)\n",
    "\n",
    "# Printing the shapes of the final training and testing labels\n",
    "print(labels_training.shape)\n",
    "print(labels_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe86e75a-a440-4b91-9bc5-c85485664412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 24000)\n",
      "(70, 24000)\n",
      "(30, 24000)\n",
      "(30, 24000)\n"
     ]
    }
   ],
   "source": [
    "# Here we randomize the training and testing information. But to do this, while keeping\n",
    "# the same permutation for both indicies and labels, we use np.random.permutation\n",
    "\n",
    "# Randomizing training data via iteration over the rows\n",
    "for i in range(total_training_indices.shape[0]):\n",
    "    perm = np.random.permutation(total_training_indices.shape[1])\n",
    "    total_training_indices[i] = total_training_indices[i, perm]\n",
    "    labels_training[i] = labels_training[i, perm]\n",
    "\n",
    "# Randomizing testing data via iteration over the rows\n",
    "for i in range(total_testing_indices.shape[0]):\n",
    "    perm = np.random.permutation(total_testing_indices.shape[1])\n",
    "    total_testing_indices[i] = total_testing_indices[i, perm]\n",
    "    labels_testing[i] = labels_testing[i, perm]\n",
    "\n",
    "print(total_training_indices.shape)\n",
    "print(labels_training.shape)\n",
    "print(total_testing_indices.shape)\n",
    "print(labels_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecd8ac0a-af7c-405c-a52e-486d736ab246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 24000, 2)\n",
      "(30, 24000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Arranging the neighbor pairs with the training indices\n",
    "total_train_neighbor_random = []\n",
    "for i in range(len(labels_training)):\n",
    "    total_train_neighbor_random.append(neighbor_pairs_list[total_training_indices[i]])\n",
    "total_train_neighbor_random = np.array(total_train_neighbor_random)\n",
    "print(total_train_neighbor_random.shape)\n",
    "\n",
    "# Arranging the neighbor pairs with the testing indices\n",
    "total_test_neighbor_random = []\n",
    "for i in range(len(labels_testing)):\n",
    "    total_test_neighbor_random.append(neighbor_pairs_list[total_training_indices[i]])\n",
    "total_test_neighbor_random = np.array(total_test_neighbor_random)\n",
    "print(total_test_neighbor_random.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b2b3ce1-eb4b-470f-9334-662f8fc076c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createArray(input_data, num_of_data, is_source, is_bi_directional):\n",
    "    # Initialize an empty list to store the output data\n",
    "    data = []\n",
    "\n",
    "    # Loop through each set of data in input_data\n",
    "    for i in range(num_of_data):\n",
    "        _data = []\n",
    "\n",
    "        # Loop through each pair of data in the current data set\n",
    "        for pair in input_data[i]:\n",
    "\n",
    "            # Process data depending on is_bi_directional flag\n",
    "            if is_bi_directional:\n",
    "                # If is_source is True, append both elements in original order\n",
    "                if is_source:\n",
    "                    _data.append(pair[0])\n",
    "                    _data.append(pair[1])\n",
    "                else:\n",
    "                    # If is_source is False, append elements in reversed order\n",
    "                    _data.append(pair[1])\n",
    "                    _data.append(pair[0])\n",
    "            else:\n",
    "                # If is_bi_directional is False, append only one element depending on is_source flag\n",
    "                if is_source:\n",
    "                    _data.append(pair[0])\n",
    "                else:\n",
    "                    _data.append(pair[1])\n",
    "\n",
    "        # Add the processed data set to the output list\n",
    "        data.append(_data)\n",
    "\n",
    "    # Return the final processed list of data\n",
    "    data = np.array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3faf7297-57ae-4816-8b00-f43bceb56eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 48000)\n",
      "(70, 48000)\n",
      "(70, 24000)\n",
      "(70, 24000)\n",
      "(30, 48000)\n",
      "(30, 48000)\n",
      "(30, 24000)\n",
      "(30, 24000)\n"
     ]
    }
   ],
   "source": [
    "# Creating bi-/uni-directional arrays for processing downstream\n",
    "\n",
    "# Training arrays\n",
    "train_edge_source_bi = createArray(total_train_neighbor_random, 70, True, True)\n",
    "train_edge_dest_bi = createArray(total_train_neighbor_random, 70, False, True)\n",
    "train_edge_source_uni = createArray(total_train_neighbor_random, 70, True, False)\n",
    "train_edge_dest_uni = createArray(total_train_neighbor_random, 70, False, False)\n",
    "\n",
    "# Testing arrays\n",
    "test_edge_source_bi = createArray(total_test_neighbor_random, 30, True, True)\n",
    "test_edge_dest_bi = createArray(total_test_neighbor_random, 30, False, True)\n",
    "test_edge_source_uni = createArray(total_test_neighbor_random, 30, True, False)\n",
    "test_edge_dest_uni = createArray(total_test_neighbor_random, 30, False, False)\n",
    "\n",
    "# Printing the shapes\n",
    "print(train_edge_source_bi.shape)\n",
    "print(train_edge_dest_bi.shape)\n",
    "print(train_edge_source_uni.shape)\n",
    "print(train_edge_dest_uni.shape)\n",
    "print(test_edge_source_bi.shape)\n",
    "print(test_edge_dest_bi.shape)\n",
    "print(test_edge_source_uni.shape)\n",
    "print(test_edge_dest_uni.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29840cb6-9362-4358-b8f1-d53eb2154fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering and creating neighbor pairs sorted by the indices of each neighbor type\n",
    "test_neighbor_pairs_tt = []\n",
    "test_neighbor_pairs_ll = []\n",
    "test_neighbor_pairs_cl = []\n",
    "test_neighbor_pairs_lc = []\n",
    "test_neighbor_pairs_cc = []\n",
    "\n",
    "for i in range(len(labels_testing)):\n",
    "    test_neighbor_pairs_tt.append(neighbor_pairs_list[test_indices_tt_pairs[i]])\n",
    "    test_neighbor_pairs_ll.append(neighbor_pairs_list[test_indices_ll_pairs[i]])\n",
    "    test_neighbor_pairs_cl.append(neighbor_pairs_list[test_indices_cl_pairs[i]])\n",
    "    test_neighbor_pairs_lc.append(neighbor_pairs_list[test_indices_lc_pairs[i]])\n",
    "    test_neighbor_pairs_cc.append(neighbor_pairs_list[test_indices_cc_pairs[i]])\n",
    "\n",
    "test_neighbor_pairs_tt = np.array(test_neighbor_pairs_tt)\n",
    "test_neighbor_pairs_ll = np.array(test_neighbor_pairs_ll)\n",
    "test_neighbor_pairs_cl = np.array(test_neighbor_pairs_cl)\n",
    "test_neighbor_pairs_lc = np.array(test_neighbor_pairs_lc)\n",
    "test_neighbor_pairs_cc = np.array(test_neighbor_pairs_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85e54d01-889f-43bd-8e0e-3e8e6bb1cb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 6000)\n",
      "(30, 6000)\n",
      "(30, 3000)\n",
      "(30, 3000)\n"
     ]
    }
   ],
   "source": [
    "test_edge_tt_source_bi = createArray(test_neighbor_pairs_tt, 30, True, True)\n",
    "test_edge_tt_dest_bi = createArray(test_neighbor_pairs_tt, 30, False, True)\n",
    "test_edge_tt_source_uni = createArray(test_neighbor_pairs_tt, 30, True, False)\n",
    "test_edge_tt_dest_uni = createArray(test_neighbor_pairs_tt, 30, False, False)\n",
    "\n",
    "test_edge_ll_source_bi = createArray(test_neighbor_pairs_ll, 30, True, True)\n",
    "test_edge_ll_dest_bi = createArray(test_neighbor_pairs_ll, 30, False, True)\n",
    "test_edge_ll_source_uni = createArray(test_neighbor_pairs_ll, 30, True, False)\n",
    "test_edge_ll_dest_uni = createArray(test_neighbor_pairs_ll, 30, False, False)\n",
    "\n",
    "test_edge_cl_source_bi = createArray(test_neighbor_pairs_cl, 30, True, True)\n",
    "test_edge_cl_dest_bi = createArray(test_neighbor_pairs_cl, 30, False, True)\n",
    "test_edge_cl_source_uni = createArray(test_neighbor_pairs_cl, 30, True, False)\n",
    "test_edge_cl_dest_uni = createArray(test_neighbor_pairs_cl, 30, False, False)\n",
    "\n",
    "test_edge_lc_source_bi = createArray(test_neighbor_pairs_lc, 30, True, True)\n",
    "test_edge_lc_dest_bi = createArray(test_neighbor_pairs_lc, 30, False, True)\n",
    "test_edge_lc_source_uni = createArray(test_neighbor_pairs_lc, 30, True, False)\n",
    "test_edge_lc_dest_uni = createArray(test_neighbor_pairs_lc, 30, False, False)\n",
    "\n",
    "test_edge_cc_source_bi = createArray(test_neighbor_pairs_cc, 30, True, True)\n",
    "test_edge_cc_dest_bi = createArray(test_neighbor_pairs_cc, 30, False, True)\n",
    "test_edge_cc_source_uni = createArray(test_neighbor_pairs_cc, 30, True, False)\n",
    "test_edge_cc_dest_uni = createArray(test_neighbor_pairs_cc, 30, False, False)\n",
    "\n",
    "\n",
    "# Print shapes\n",
    "print(test_edge_tt_source_bi.shape)\n",
    "print(test_edge_tt_dest_bi.shape)\n",
    "print(test_edge_tt_source_uni.shape)\n",
    "print(test_edge_tt_dest_uni.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5581d6-26fd-4e0f-b554-218de65b2305",
   "metadata": {},
   "source": [
    "## Making the features array for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4003ac05-3ef4-4c4f-96fc-5c1e16469225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a rearranged dictionary from the scaled data dictionary\n",
    "keys = list(scaled_data.keys())\n",
    "values = list(scaled_data.values())\n",
    "features_dict = dict(zip(keys, values))\n",
    "\n",
    "# Here these features are split into training and testing sets\n",
    "features_training = np.concatenate([value for key, value in list(features_dict.items())[:70]])\n",
    "features_testing = np.concatenate([value for key, value in list(features_dict.items())[70:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "712d339a-3b39-438c-9e93-7d75b72c2039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13135640, 5)\n",
      "(5629560, 5)\n"
     ]
    }
   ],
   "source": [
    "print(features_training.shape)\n",
    "print(features_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76e228e6-02da-43be-9fe5-c3af47f5dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training = features_training.reshape(70, 187652, 5)\n",
    "features_testing = features_testing.reshape(30, 187652, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee7222-0ee7-4008-97ad-9fdd10766054",
   "metadata": {},
   "source": [
    "# Creation of the NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "332e5119-9a08-4d0e-950c-04556c4d6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the scaled features into a torch tensor (inputs)\n",
    "x_train = torch.tensor(features_training, dtype=torch.float)\n",
    "x_test = torch.tensor(features_testing, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e09b258-1cac-406f-b69e-6fb047f0515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 2, 48000])\n",
      "torch.Size([70, 2, 24000])\n"
     ]
    }
   ],
   "source": [
    "# Here the correct dimension permutations are applied for the model\n",
    "def make_edge_index_tensor(source, dest):\n",
    "    edge_index = np.stack([np.array(source), np.array(dest)], axis=0)  # Stack into a single NumPy array\n",
    "    return torch.tensor(edge_index, dtype=torch.long).permute(1, 0, 2)\n",
    "\n",
    "# Training set (Bi-directional and Uni-directional)\n",
    "train_edge_indices_bi = make_edge_index_tensor(train_edge_source_bi, train_edge_dest_bi)\n",
    "train_edge_indices_uni = make_edge_index_tensor(train_edge_source_uni, train_edge_dest_uni)\n",
    "\n",
    "print(train_edge_indices_bi.shape)\n",
    "print(train_edge_indices_uni.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80f40fd9-5c34-4d29-9a5d-21676d27ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_data = {\n",
    "    \"tt_bi\": (test_edge_tt_source_bi, test_edge_tt_dest_bi),\n",
    "    \"tt_uni\": (test_edge_tt_source_uni, test_edge_tt_dest_uni),\n",
    "    \"ll_bi\": (test_edge_ll_source_bi, test_edge_ll_dest_bi),\n",
    "    \"ll_uni\": (test_edge_ll_source_uni, test_edge_ll_dest_uni),\n",
    "    \"cl_bi\": (test_edge_cl_source_bi, test_edge_cl_dest_bi),\n",
    "    \"cl_uni\": (test_edge_cl_source_uni, test_edge_cl_dest_uni),\n",
    "    \"lc_bi\": (test_edge_lc_source_bi, test_edge_lc_dest_bi),\n",
    "    \"lc_uni\": (test_edge_lc_source_uni, test_edge_lc_dest_uni),\n",
    "    \"cc_bi\": (test_edge_cc_source_bi, test_edge_cc_dest_bi),\n",
    "    \"cc_uni\": (test_edge_cc_source_uni, test_edge_cc_dest_uni),\n",
    "}\n",
    "\n",
    "# Create and permute tensors for all edge types\n",
    "edge_indices = {key: make_edge_index_tensor(sources, dests) for key, (sources, dests) in edge_index_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c471b72-7d99-44c0-a2ad-8f1774252674",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.expand_dims(labels_training, axis=1)\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = np.expand_dims(labels_testing, axis=1)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f0405-a0bd-4c37-bbf9-9f6b226fd439",
   "metadata": {},
   "source": [
    "### Creation of custom data lists, collate functions, and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca4c809d-b8a1-4904-94aa-39df68909474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class that inherents from the torch.utils.data.Dataset class\n",
    "# The pytorch class is abstract, meaning we need to define certain methods\n",
    "# like __len__() and __getitem__()\n",
    "class custom_dataset(torch.utils.data.Dataset):\n",
    "    # Class constructor that takes in data list and\n",
    "    # stores it as an instance, making it avaliable\n",
    "    # to other methods in the class\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    # Method return length of data set\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    # Method returns data point at index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "# Used to handle batch loading, shuffling, and parallel loading during\n",
    "# training and testing in the ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb267d0a-6165-448b-86fe-b9544af71501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with information regarding a homogenous graph (a graph\n",
    "# where all nodes represent instances of the same type [cells in the\n",
    "# detector] and all edges represent relations of the same type [connections\n",
    "# between cells])\n",
    "def create_data_list(bi_edge_indices, uni_edge_indices, x, y):\n",
    "    data_list = []\n",
    "    for i in range(len(bi_edge_indices)):\n",
    "        # Create the feature matrix\n",
    "        x_mat = x[i]\n",
    "        # Create graph connectivity matrix\n",
    "        edge_index = bi_edge_indices[i]\n",
    "        edge_index, _ = add_self_loops(edge_index)\n",
    "\n",
    "        # Convert y[i] to a PyTorch tensor\n",
    "        y_tensor = torch.tensor(y[i], dtype=torch.long) if not isinstance(\n",
    "            y[i], torch.Tensor) else y[i]\n",
    "\n",
    "        # Create the data object describing a homogeneous graph\n",
    "        data = Data(x=x_mat, edge_index=edge_index,\n",
    "                    edge_index_out=uni_edge_indices[i], y=y_tensor)\n",
    "        data = ToUndirected()(data)\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def collate_data(data_list):\n",
    "    return ([data.x for data in data_list],\n",
    "            [data.edge_index for data in data_list],\n",
    "            [data.edge_index_out for data in data_list],\n",
    "            torch.cat([data.y for data in data_list], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "133514e1-1863-4fbf-a65e-8d7ea40ebf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data lists for all edge types and categories\n",
    "\n",
    "data_list_train = create_data_list(train_edge_indices_bi, train_edge_indices_uni, x_train, y_train)  # Training Edges\n",
    "data_list_tt = create_data_list(edge_indices['tt_bi'], edge_indices['tt_uni'], x_test, np.expand_dims(labels_tt_test, axis=1))  # True-True Edges\n",
    "data_list_ll = create_data_list(edge_indices['ll_bi'], edge_indices['ll_uni'], x_test, np.expand_dims(labels_ll_test, axis=1))  # Lone-lone Edges\n",
    "data_list_cl = create_data_list(edge_indices['cl_bi'], edge_indices['cl_uni'], x_test, np.expand_dims(labels_cl_test, axis=1))  # Cluster-Lone Edges\n",
    "data_list_lc = create_data_list(edge_indices['lc_bi'], edge_indices['lc_uni'], x_test, np.expand_dims(labels_lc_test, axis=1))  # Lone-Cluster Edges\n",
    "data_list_cc = create_data_list(edge_indices['cc_bi'], edge_indices['cc_uni'], x_test, np.expand_dims(labels_cc_test, axis=1))  # Cluster-Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "223e2a41-744a-443a-8322-a7e93c444f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_list_train[0])\n",
    "# print(len(data_list_train))\n",
    "# print(data_list_tt[0])\n",
    "# print(len(data_list_tt))\n",
    "# print(data_list_ll[0])\n",
    "# print(len(data_list_ll))\n",
    "# print(data_list_cl[0])\n",
    "# print(len(data_list_cl))\n",
    "# print(data_list_lc[0])\n",
    "# print(len(data_list_lc))\n",
    "# print(data_list_cc[0])\n",
    "# print(len(data_list_cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8017172-d912-4ed5-bfce-6cc96326670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size value\n",
    "batch_size = 1\n",
    "\n",
    "# Create the data loaders\n",
    "data_loader = {}\n",
    "data_list_mapping = {\n",
    "    \"train\": data_list_train,  # Training Edges\n",
    "    \"tt\": data_list_tt,           # True-True Edges\n",
    "    \"ll\": data_list_ll,           # Lone-Lone Edges\n",
    "    \"lc\": data_list_lc,           # Lone-Cluster Edges\n",
    "    \"cl\": data_list_cl,           # Cluster-Lone Edges\n",
    "    \"cc\": data_list_cc            # Cluster-Cluster Edges\n",
    "}\n",
    "\n",
    "# Total background dataset\n",
    "data_list_total_bkg = data_list_ll + data_list_cl + data_list_lc + data_list_cc\n",
    "data_loader_total_bkg = torch.utils.data.DataLoader(\n",
    "    custom_dataset(data_list_total_bkg),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=lambda batch: collate_data(batch)\n",
    ")\n",
    "# For the other datasets\n",
    "for key, data_list in data_list_mapping.items():\n",
    "    data_loader[key] = torch.utils.data.DataLoader(\n",
    "        custom_dataset(data_list),\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=lambda batch: collate_data(batch)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b60c0",
   "metadata": {},
   "source": [
    "### Choosing the Device to Run on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd668455",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e39f8e",
   "metadata": {},
   "source": [
    "### Creating the weights to be used in the CrossEntropyLoss() later\n",
    "The class weights will be calculated using this formula in mind (since this formula helps in the mitigation of rare-class overweighting)\n",
    "$${weight}_i = \\frac{{total\\space samples}}{{frequency}_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e4ea62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights (Training): {np.int64(4): 8.0, np.int64(3): 2.0, np.int64(1): 8.0, np.int64(0): 8.0, np.int64(2): 8.0}\n",
      "Weight Tensor (Training): tensor([8., 8., 8., 2., 8.], device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "labels_training_flat = labels_training.flatten()\n",
    "\n",
    "# Compute class frequencies (counts) for the entire dataset\n",
    "train_label_counts = Counter(labels_training_flat)\n",
    "\n",
    "# Total number of samples (the number of elements in the flattened array)\n",
    "total_train_samples = labels_training_flat.size\n",
    "\n",
    "# Compute class weights using the new formula\n",
    "class_weights = {label: total_train_samples /\n",
    "                 count for label, count in train_label_counts.items()}\n",
    "\n",
    "# Convert weights to a tensor for PyTorch\n",
    "unique_classes = np.unique(labels_training_flat)  # Get unique classes\n",
    "weight_tensor = torch.tensor(\n",
    "    [class_weights[label] for label in unique_classes], dtype=torch.float).to(device)\n",
    "\n",
    "# Print class weights\n",
    "print(\"Class Weights (Training):\", class_weights)\n",
    "print(\"Weight Tensor (Training):\", weight_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a51d892",
   "metadata": {},
   "source": [
    "### Theoretical Insights into Focal Loss\n",
    "\n",
    "Focal Loss was introduced by Lin et al. (2017) in the paper *\"Focal Loss for Dense Object Detection\"* to address the challenge of class imbalance in tasks such as object detection, where the background class often dominates the dataset.\n",
    "\n",
    "It builds on Cross-Entropy Loss but incorporates two key mechanisms to focus learning on hard examples and down-weight the impact of easy examples.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Cross-Entropy Loss\n",
    "\n",
    "The standard Cross-Entropy Loss for a classification problem is given by:\n",
    "\n",
    "$$ \\text{CE \\space Loss} = - \\log{p_t} $$\n",
    "\n",
    "Where:\n",
    "- $p_t$ is the predicted probability of the true class $t$, computed as $p_t = \\text{softmax}(z_t)$.\n",
    "- $z_t$ is the raw score (logit) for the true class.\n",
    "\n",
    "Limitation:\n",
    "In highly imbalanced datasets, Cross-Entropy Loss often leads to models being biased toward the majority class, as it treats all examples equally and doesn't prioritize harder cases.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Focal Loss\n",
    "\n",
    "Focal Loss modifies Cross-Entropy Loss to focus on hard-to-classify examples by introducing two components:\n",
    "1. Modulating Factor $(1 - p_t)^\\gamma$\n",
    "2. Balancing Factor $\\alpha_t$\n",
    "\n",
    "The formula for Focal Loss is:\n",
    "\n",
    "$$ \\text{Focal Loss} = -\\alpha_t (1 - p_t)^\\gamma \\log{p_t} $$\n",
    "\n",
    "\n",
    "#### Key Components:\n",
    "1. $ (1 - p_t)^\\gamma $:\n",
    "   - Purpose: Down-weights the loss for well-classified examples (where $ p_t $ is high) and up-weights the loss for hard examples (where $ p_t $ is low).\n",
    "   - Effect of $ \\gamma $:\n",
    "     - $ \\gamma = 0 $: Reduces to Cross-Entropy Loss.\n",
    "     - $ \\gamma > 0 $: The larger $ \\gamma $, the more it focuses on hard examples.\n",
    "   - This term ensures that the model spends less time improving predictions on already well-classified samples.\n",
    "\n",
    "2. $ \\alpha_t $:\n",
    "   - Purpose: Balances the importance of each class to address class imbalance.\n",
    "   - Effect of $ \\alpha_t $:\n",
    "     - For minority classes, a higher $ \\alpha_t $ ensures their contribution to the loss is scaled up, encouraging the model to focus on underrepresented examples.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Intuition Behind Focal Loss\n",
    "\n",
    "- Well-Classified Examples: \n",
    "  - If $ p_t $ is close to 1 (high confidence for the true class), then $ (1 - p_t)^\\gamma $ becomes small, and the loss is down-weighted.\n",
    "  - This prevents the model from overemphasizing already correct predictions.\n",
    "\n",
    "- Hard Examples:\n",
    "  - If $ p_t $ is close to 0 (low confidence for the true class), then $ (1 - p_t)^\\gamma $ approaches 1, and the loss is not significantly reduced.\n",
    "  - This keeps the model focused on improving predictions for harder cases.\n",
    "\n",
    "By adjusting $ \\gamma $ and $ \\alpha_t $, Focal Loss achieves a balance between prioritizing minority classes and focusing on hard examples.\n",
    "\n",
    "\n",
    "\n",
    "### 4. Why Focal Loss Works\n",
    "\n",
    "1. Class Imbalance:\n",
    "   - In tasks like object detection or medical imaging, datasets are often imbalanced (e.g., background vs. objects in detection, healthy vs. diseased in medical imaging).\n",
    "   - Focal Loss reduces the overwhelming influence of easy-to-classify examples (e.g., background in object detection) by scaling down their loss.\n",
    "\n",
    "2. Hard Examples:\n",
    "   - Models tend to quickly learn to classify easy examples (majority class) but struggle with harder ones (minority class or edge cases).\n",
    "   - By amplifying the contribution of hard examples, Focal Loss ensures these samples receive more attention during training.\n",
    "\n",
    "3. Flexibility:\n",
    "   - The focusing parameter $ \\gamma $ allows the user to control how aggressively the loss focuses on hard examples.\n",
    "   - The balancing factor $ \\alpha_t $ enables compensation for class imbalance without requiring oversampling or undersampling.\n",
    "\n",
    "\n",
    "\n",
    "### 5. Impact of Focal Loss on Optimization\n",
    "\n",
    "- Gradient Behavior:\n",
    "  - In standard Cross-Entropy Loss, all examples contribute equally to the gradient.\n",
    "  - In Focal Loss, the gradient for well-classified examples is reduced due to $ (1 - p_t)^\\gamma $, while the gradient for hard examples is relatively higher. This leads to more targeted updates to the model's parameters.\n",
    "\n",
    "- Stability:\n",
    "  - By down-weighting easy examples, Focal Loss prevents gradients from being dominated by the majority class, leading to more stable and meaningful updates.\n",
    "\n",
    "\n",
    "\n",
    "### 6. Choosing Hyperparameters\n",
    "\n",
    "1. $ \\gamma $ (Focusing Parameter):\n",
    "   - Typical range: $ [1.0, 3.0] $.\n",
    "   - Higher $ \\gamma $ increases the focus on hard examples but may slow convergence.\n",
    "\n",
    "2. $ \\alpha $ (Class Weights):\n",
    "   - Use $ \\alpha_t $ to manually adjust for class imbalance.\n",
    "   - For highly imbalanced datasets, compute $ \\alpha_t $ as:\n",
    "     \\[\n",
    "     \\alpha_t = \\frac{\\text{Total Samples}}{\\text{Samples in Class $ t $}} \\quad \\text{(Normalized)}\n",
    "     \\]\n",
    "\n",
    "3. Learning Rate:\n",
    "   - Focal Loss can amplify gradients for hard examples, so consider using a slightly lower learning rate.\n",
    "\n",
    "\n",
    "\n",
    "### 7. Applications of Focal Loss\n",
    "\n",
    "- Object Detection:\n",
    "  - Used in RetinaNet (Lin et al.) to address the extreme foreground-background class imbalance.\n",
    "- Medical Imaging:\n",
    "  - Classifying rare conditions (e.g., tumor detection).\n",
    "- Natural Language Processing:\n",
    "  - Handling highly imbalanced sentiment or spam detection tasks.\n",
    "- Graph Neural Networks:\n",
    "  - Addressing node or edge classification with imbalanced class distributions.\n",
    "\n",
    "\n",
    "\n",
    "### 8. Limitations of Focal Loss\n",
    "\n",
    "1. Hyperparameter Sensitivity:\n",
    "   - Requires careful tuning of $ \\gamma $ and $ \\alpha_t $.\n",
    "   - Poor choices can lead to underfitting (if $ \\gamma $ is too high) or overfitting (if $ \\gamma $ is too low).\n",
    "\n",
    "2. Increased Computation:\n",
    "   - The $ (1 - p_t)^\\gamma $ term adds a slight computational overhead compared to standard Cross-Entropy Loss.\n",
    "\n",
    "3. Not Always Necessary:\n",
    "   - For datasets with minimal class imbalance, standard Cross-Entropy Loss may perform just as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d22c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.alpha = alpha\n",
    "#         self.gamma = gamma\n",
    "#         self.reduction = reduction\n",
    "\n",
    "#     def forward(self, logits, targets):\n",
    "#         # Apply softmax to logits to get probabilities\n",
    "#         probs = F.softmax(logits, dim=1)\n",
    "\n",
    "#         # Get the probabilities of the true class\n",
    "#         targets_one_hot = F.one_hot(\n",
    "#             targets, num_classes=logits.size(1)).float()\n",
    "#         probs = probs * targets_one_hot\n",
    "#         probs = probs.sum(dim=1)  # Shape: (batch_size,)\n",
    "\n",
    "#         # Compute the focal loss components\n",
    "#         log_probs = torch.log(probs + 1e-8)  # Add epsilon to avoid log(0)\n",
    "#         focal_weights = (1 - probs) ** self.gamma\n",
    "\n",
    "#         # Apply class weights (if provided)\n",
    "#         if self.alpha is not None:\n",
    "#             alpha_weights = self.alpha[targets]  # Shape: (batch_size,)\n",
    "#             focal_loss = -alpha_weights * focal_weights * log_probs\n",
    "#         else:\n",
    "#             focal_loss = -focal_weights * log_probs\n",
    "\n",
    "#         # Reduce the loss based on the reduction method\n",
    "#         if self.reduction == 'mean':\n",
    "#             return focal_loss.mean()\n",
    "#         elif self.reduction == 'sum':\n",
    "#             return focal_loss.sum()\n",
    "#         else:  # 'none'\n",
    "#             return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47346e4e-769a-43b5-b1c6-6657be95239c",
   "metadata": {},
   "source": [
    "### Creation of the Multi-Edge Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d43fa6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have different weights for the different layers\n",
    "\n",
    "class MultiEdgeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, debug=False):\n",
    "        super(MultiEdgeClassifier, self).__init__()\n",
    "        # Set the debug mode\n",
    "        self.debug = debug\n",
    "\n",
    "        # Node embedding layer\n",
    "        self.node_embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Initialize first convolution and batch norm layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        # List to store learnable weights for each layer\n",
    "        self.layer_weights = nn.ParameterList()\n",
    "\n",
    "        self.convs.append(GCNConv(hidden_dim, 128))\n",
    "        self.bns.append(BatchNorm1d(128))\n",
    "        self.layer_weights.append(nn.Parameter(torch.tensor(\n",
    "            1.0, requires_grad=True)))  # Weight for layer 1\n",
    "\n",
    "        # Additional conv and bn layers based on 'num_layers' param\n",
    "        for i in range(1, num_layers):\n",
    "            in_channels = 128 if i == 1 else 64\n",
    "            out_channels = 64\n",
    "            self.convs.append(GCNConv(in_channels, out_channels))\n",
    "            self.bns.append(BatchNorm1d(out_channels))\n",
    "            self.layer_weights.append(nn.Parameter(torch.tensor(\n",
    "                1.0, requires_grad=True)))  # Weight for each layer\n",
    "\n",
    "        # Edge classification layer\n",
    "        self.fc = nn.Linear(128, output_dim)\n",
    "\n",
    "    def debug_print(self, message):\n",
    "        if self.debug:\n",
    "            print(message)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_index_out):\n",
    "        # Node embedding\n",
    "        x = self.node_embedding(x)\n",
    "        self.debug_print(f\"Node embedding output shape: {x.shape}\")\n",
    "\n",
    "        if x.dim() == 3 and x.size(0) == 1:  # Check and remove batch dimension\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "        # Loop through convolution layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            self.debug_print(f\"After GCNConv {i+1}: {x.shape}\")\n",
    "            if x.dim() == 3 and x.size(0) == 1:  # Check and remove batch dimension\n",
    "                x = x.squeeze(0)\n",
    "            x = self.bns[i](x)\n",
    "            x = torch.relu(x)\n",
    "            # Multiply output of each layer by its corresponding weight\n",
    "            x = x * self.layer_weights[i]\n",
    "            self.debug_print(f\"After Layer Weight {i+1}: {x.shape}\")\n",
    "\n",
    "        # Edge representations\n",
    "        edge_rep = torch.cat(\n",
    "            [x[edge_index_out[0]], x[edge_index_out[1]]], dim=1)\n",
    "        self.debug_print(f\"Edge representation shape: {edge_rep.shape}\")\n",
    "\n",
    "        # Return Logits\n",
    "        edge_scores = self.fc(edge_rep)\n",
    "        return edge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b203153b-0470-45fb-89b9-f148d7a5855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "hidden_dim = 256\n",
    "output_dim = 5  # Multiclass classification\n",
    "num_layers = 5\n",
    "model = MultiEdgeClassifier(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "num_epochs = 300\n",
    "# criterion = nn.CrossEntropyLoss()  # Handles softmax internally\n",
    "criterion = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "# criterion = FocalLoss(alpha=weight_tensor, gamma=2.0, reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed329397-631a-470f-8f39-822616f89e6c",
   "metadata": {},
   "source": [
    "### Creation of functions to run our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6d4f020-26fa-4b51-99f6-e1d5dbdbe8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, data_loader, optimizer, criterion):\n",
    "    # Sets the model into training mode\n",
    "    model.train()\n",
    "    # Sends model to GPU if available, otherwise uses the CPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Assumes there is only one batch in the data loader\n",
    "    # Retrieve the single batch from the data loader\n",
    "    batch_x, batch_edge_index, batch_edge_index_out, batch_y = next(\n",
    "        iter(data_loader))\n",
    "\n",
    "    # Sends the input features, the edge indices, and target\n",
    "    # labels to the GPU if available, otherwise the CPU\n",
    "    batch_x = torch.stack(batch_x).to(device)\n",
    "    batch_edge_index = [edge_index.to(device)\n",
    "                        for edge_index in batch_edge_index]\n",
    "    batch_edge_index_out = [edge_index.to(\n",
    "        device) for edge_index in batch_edge_index_out]\n",
    "\n",
    "    # Convert target labels to LongTensor (torch.int64)\n",
    "    batch_y = [y.long().to(device) for y in batch_y]\n",
    "\n",
    "    # Clears the gradients of the model parameters to ensure\n",
    "    # they are not accumulated across batches\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Initialize loss tracking for subgraphs in the single batch\n",
    "    loss_per_batch = []\n",
    "\n",
    "    # Model processes each graph in the batch one by one\n",
    "    for i in range(len(batch_edge_index)):\n",
    "        # Pass the features and the edge indices into the model and store\n",
    "        # the output (logits)\n",
    "        _output = model(batch_x[i], batch_edge_index[i],\n",
    "                        batch_edge_index_out[i])\n",
    "\n",
    "        # Ensure that model outputs (logits) are of type float32\n",
    "        _output = _output.float()\n",
    "\n",
    "        # Calculate the difference between the model output and the targets\n",
    "        # via the provided criterion (loss function)\n",
    "        loss = criterion(_output.squeeze(), batch_y[i].squeeze())\n",
    "\n",
    "        # This difference is stored in the loss_per_batch list\n",
    "        loss_per_batch.append(loss)\n",
    "\n",
    "    # The average loss across all subgraphs within the single batch is calculated\n",
    "    total_loss_per_batch = sum(loss_per_batch) / len(loss_per_batch)\n",
    "\n",
    "    # Computes the loss gradients with respect to the model parameters\n",
    "    total_loss_per_batch.backward()\n",
    "\n",
    "    # Updates the model parameters using the gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Returns the total loss for the single batch\n",
    "    return total_loss_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b2ed4b9-3bd1-44b7-ba22-f33449b75069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, device, data_loader_true, data_loader_bkg_dict):\n",
    "    all_scores = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        # Process true edges (positive class, label 1)\n",
    "        batch_x, batch_edge_index, batch_edge_index_out, batch_y = next(\n",
    "            iter(data_loader_true))\n",
    "        batch_x = torch.stack(batch_x).to(device)\n",
    "        batch_edge_index = [edge_index.to(device)\n",
    "                            for edge_index in batch_edge_index]\n",
    "        batch_edge_index_out = [edge_index_out.to(\n",
    "            device) for edge_index_out in batch_edge_index_out]\n",
    "\n",
    "        for i in range(len(batch_edge_index)):\n",
    "            test_edge_scores = model(\n",
    "                batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "            test_edge_scores = F.softmax(test_edge_scores, dim=1)\n",
    "\n",
    "            all_scores.append(test_edge_scores)\n",
    "            true_labels.append(torch.ones(test_edge_scores.size(\n",
    "                0), dtype=torch.long, device=device))\n",
    "\n",
    "        # Process background edges\n",
    "        for background_type, data_loader_bkg in data_loader_bkg_dict.items():\n",
    "            batch_x, batch_edge_index, batch_edge_index_out, batch_y = next(\n",
    "                iter(data_loader_bkg))\n",
    "            batch_x = torch.stack(batch_x).to(device)\n",
    "            batch_edge_index = [edge_index.to(device)\n",
    "                                for edge_index in batch_edge_index]\n",
    "            batch_edge_index_out = [edge_index_out.to(\n",
    "                device) for edge_index_out in batch_edge_index_out]\n",
    "\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                test_edge_scores = model(\n",
    "                    batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                test_edge_scores = F.softmax(test_edge_scores, dim=1)\n",
    "\n",
    "                all_scores.append(test_edge_scores)\n",
    "                true_labels.append(torch.full((test_edge_scores.size(\n",
    "                    0),), background_type, dtype=torch.long, device=device))\n",
    "\n",
    "    # Concatenate all scores and labels\n",
    "    all_scores = torch.cat(all_scores, dim=0)\n",
    "    true_labels = torch.cat(true_labels, dim=0)\n",
    "\n",
    "    # Reorder scores and labels to match the desired order (0, 1, 2, 3, 4)\n",
    "    desired_order = [0, 1, 2, 3, 4]\n",
    "    # Ensure reordering matches desired labels\n",
    "    mask = torch.argsort(true_labels).argsort()\n",
    "    all_scores = all_scores[mask]\n",
    "    true_labels = true_labels[mask]\n",
    "\n",
    "    return all_scores.cpu().numpy(), true_labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b988ee58-ccda-4cb3-a100-940d8b80d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_for_train_and_test(model, loader, loss_fn, optimizer, training, device):\n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    all_logits = []  # Store raw logits in testing mode\n",
    "\n",
    "    for batch in loader:\n",
    "        if training:\n",
    "            batch_x, batch_edge_index, batch_edge_index_out, batch_y = batch\n",
    "            batch_y = batch_y.to(device)\n",
    "        else:\n",
    "            batch_x, batch_edge_index, batch_edge_index_out, *_ = batch\n",
    "\n",
    "        # Move features and edge indices to the device\n",
    "        batch_x = torch.stack(batch_x).to(device)\n",
    "        batch_edge_index = [edge_index.to(device)\n",
    "                            for edge_index in batch_edge_index]\n",
    "        batch_edge_index_out = [edge_index_out.to(\n",
    "            device) for edge_index_out in batch_edge_index_out]\n",
    "\n",
    "        if training:\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                logits = model(\n",
    "                    batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(logits, batch_y[i])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(batch_edge_index)):\n",
    "                    logits = model(\n",
    "                        batch_x[i], batch_edge_index[i], batch_edge_index_out[i])\n",
    "                    all_logits.append(logits)  # Store raw logits\n",
    "                    num_batches += 1\n",
    "\n",
    "    average_loss = total_loss / num_batches if training else None\n",
    "    all_logits = torch.cat(all_logits, dim=0).cpu(\n",
    "    ).numpy() if all_logits else None\n",
    "\n",
    "    return average_loss, all_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f794192-3e03-41b3-92e4-05c42fe3cdae",
   "metadata": {},
   "source": [
    "## Running the model over num_epochs iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e288406-216e-43e2-adff-874f831bd12a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Total Loss Per Epoch: 1.6927\n",
      "Epoch: 2 | Total Loss Per Epoch: 1.3514\n",
      "Epoch: 3 | Total Loss Per Epoch: 1.2910\n",
      "Epoch: 4 | Total Loss Per Epoch: 1.2835\n",
      "Epoch: 5 | Total Loss Per Epoch: 1.2640\n",
      "Epoch: 6 | Total Loss Per Epoch: 1.2545\n",
      "Epoch: 7 | Total Loss Per Epoch: 1.2489\n",
      "Epoch: 8 | Total Loss Per Epoch: 1.2442\n",
      "Epoch: 9 | Total Loss Per Epoch: 1.2355\n",
      "Epoch: 10 | Total Loss Per Epoch: 1.2298\n",
      "Epoch: 11 | Total Loss Per Epoch: 1.2278\n",
      "Epoch: 12 | Total Loss Per Epoch: 1.2253\n",
      "Epoch: 13 | Total Loss Per Epoch: 1.2217\n",
      "Epoch: 14 | Total Loss Per Epoch: 1.2221\n",
      "Epoch: 15 | Total Loss Per Epoch: 1.2212\n",
      "Epoch: 16 | Total Loss Per Epoch: 1.2189\n",
      "Epoch: 17 | Total Loss Per Epoch: 1.2178\n",
      "Epoch: 18 | Total Loss Per Epoch: 1.2174\n",
      "Epoch: 19 | Total Loss Per Epoch: 1.2171\n",
      "Epoch: 20 | Total Loss Per Epoch: 1.2155\n",
      "Epoch: 21 | Total Loss Per Epoch: 1.2141\n",
      "Epoch: 22 | Total Loss Per Epoch: 1.2134\n",
      "Epoch: 23 | Total Loss Per Epoch: 1.2131\n",
      "Epoch: 24 | Total Loss Per Epoch: 1.2125\n",
      "Epoch: 25 | Total Loss Per Epoch: 1.2120\n",
      "Epoch: 26 | Total Loss Per Epoch: 1.2144\n",
      "Epoch: 27 | Total Loss Per Epoch: 1.2128\n",
      "Epoch: 28 | Total Loss Per Epoch: 1.2115\n",
      "Epoch: 29 | Total Loss Per Epoch: 1.2113\n",
      "Epoch: 30 | Total Loss Per Epoch: 1.2098\n",
      "Epoch: 31 | Total Loss Per Epoch: 1.2093\n",
      "Epoch: 32 | Total Loss Per Epoch: 1.2083\n",
      "Epoch: 33 | Total Loss Per Epoch: 1.2079\n",
      "Epoch: 34 | Total Loss Per Epoch: 1.2074\n",
      "Epoch: 35 | Total Loss Per Epoch: 1.2067\n",
      "Epoch: 36 | Total Loss Per Epoch: 1.2055\n",
      "Epoch: 37 | Total Loss Per Epoch: 1.2045\n",
      "Epoch: 38 | Total Loss Per Epoch: 1.2031\n",
      "Epoch: 39 | Total Loss Per Epoch: 1.2022\n",
      "Epoch: 40 | Total Loss Per Epoch: 1.2019\n",
      "Epoch: 41 | Total Loss Per Epoch: 1.2010\n",
      "Epoch: 42 | Total Loss Per Epoch: 1.1995\n",
      "Epoch: 43 | Total Loss Per Epoch: 1.1990\n",
      "Epoch: 44 | Total Loss Per Epoch: 1.1988\n",
      "Epoch: 45 | Total Loss Per Epoch: 1.1984\n",
      "Epoch: 46 | Total Loss Per Epoch: 1.1975\n",
      "Epoch: 47 | Total Loss Per Epoch: 1.1968\n",
      "Epoch: 48 | Total Loss Per Epoch: 1.1965\n",
      "Epoch: 49 | Total Loss Per Epoch: 1.1962\n",
      "Epoch: 50 | Total Loss Per Epoch: 1.1961\n",
      "Epoch: 51 | Total Loss Per Epoch: 1.1965\n",
      "Epoch: 52 | Total Loss Per Epoch: 1.1968\n",
      "Epoch: 53 | Total Loss Per Epoch: 1.1963\n",
      "Epoch: 54 | Total Loss Per Epoch: 1.1955\n",
      "Epoch: 55 | Total Loss Per Epoch: 1.1947\n",
      "Epoch: 56 | Total Loss Per Epoch: 1.1942\n",
      "Epoch: 57 | Total Loss Per Epoch: 1.1941\n",
      "Epoch: 58 | Total Loss Per Epoch: 1.1938\n",
      "Epoch: 59 | Total Loss Per Epoch: 1.1937\n",
      "Epoch: 60 | Total Loss Per Epoch: 1.1935\n",
      "Epoch: 61 | Total Loss Per Epoch: 1.1941\n",
      "Epoch: 62 | Total Loss Per Epoch: 1.1942\n",
      "Epoch: 63 | Total Loss Per Epoch: 1.1937\n",
      "Epoch: 64 | Total Loss Per Epoch: 1.1933\n",
      "Epoch: 65 | Total Loss Per Epoch: 1.1931\n",
      "Epoch: 66 | Total Loss Per Epoch: 1.1927\n",
      "Epoch: 67 | Total Loss Per Epoch: 1.1925\n",
      "Epoch: 68 | Total Loss Per Epoch: 1.1922\n",
      "Epoch: 69 | Total Loss Per Epoch: 1.1917\n",
      "Epoch: 70 | Total Loss Per Epoch: 1.1911\n",
      "Epoch: 71 | Total Loss Per Epoch: 1.1907\n",
      "Epoch: 72 | Total Loss Per Epoch: 1.1903\n",
      "Epoch: 73 | Total Loss Per Epoch: 1.1901\n",
      "Epoch: 74 | Total Loss Per Epoch: 1.1898\n",
      "Epoch: 75 | Total Loss Per Epoch: 1.1896\n",
      "Epoch: 76 | Total Loss Per Epoch: 1.1893\n",
      "Epoch: 77 | Total Loss Per Epoch: 1.1891\n",
      "Epoch: 78 | Total Loss Per Epoch: 1.1891\n",
      "Epoch: 79 | Total Loss Per Epoch: 1.1891\n",
      "Epoch: 80 | Total Loss Per Epoch: 1.1890\n",
      "Epoch: 81 | Total Loss Per Epoch: 1.1890\n",
      "Epoch: 82 | Total Loss Per Epoch: 1.1891\n",
      "Epoch: 83 | Total Loss Per Epoch: 1.1891\n",
      "Epoch: 84 | Total Loss Per Epoch: 1.1891\n",
      "Epoch: 85 | Total Loss Per Epoch: 1.1890\n",
      "Epoch: 86 | Total Loss Per Epoch: 1.1888\n",
      "Epoch: 87 | Total Loss Per Epoch: 1.1887\n",
      "Epoch: 88 | Total Loss Per Epoch: 1.1884\n",
      "Epoch: 89 | Total Loss Per Epoch: 1.1882\n",
      "Epoch: 90 | Total Loss Per Epoch: 1.1881\n",
      "Epoch: 91 | Total Loss Per Epoch: 1.1880\n",
      "Epoch: 92 | Total Loss Per Epoch: 1.1880\n",
      "Epoch: 93 | Total Loss Per Epoch: 1.1879\n",
      "Epoch: 94 | Total Loss Per Epoch: 1.1879\n",
      "Epoch: 95 | Total Loss Per Epoch: 1.1879\n",
      "Epoch: 96 | Total Loss Per Epoch: 1.1878\n",
      "Epoch: 97 | Total Loss Per Epoch: 1.1878\n",
      "Epoch: 98 | Total Loss Per Epoch: 1.1877\n",
      "Epoch: 99 | Total Loss Per Epoch: 1.1878\n",
      "Epoch: 100 | Total Loss Per Epoch: 1.1877\n",
      "Epoch: 101 | Total Loss Per Epoch: 1.1875\n",
      "Epoch: 102 | Total Loss Per Epoch: 1.1873\n",
      "Epoch: 103 | Total Loss Per Epoch: 1.1872\n",
      "Epoch: 104 | Total Loss Per Epoch: 1.1872\n",
      "Epoch: 105 | Total Loss Per Epoch: 1.1874\n",
      "Epoch: 106 | Total Loss Per Epoch: 1.1875\n",
      "Epoch: 107 | Total Loss Per Epoch: 1.1875\n",
      "Epoch: 108 | Total Loss Per Epoch: 1.1875\n",
      "Epoch: 109 | Total Loss Per Epoch: 1.1875\n",
      "Epoch: 110 | Total Loss Per Epoch: 1.1874\n",
      "Epoch: 111 | Total Loss Per Epoch: 1.1873\n",
      "Epoch: 112 | Total Loss Per Epoch: 1.1873\n",
      "Epoch: 113 | Total Loss Per Epoch: 1.1872\n",
      "Epoch: 114 | Total Loss Per Epoch: 1.1871\n",
      "Epoch: 115 | Total Loss Per Epoch: 1.1870\n",
      "Epoch: 116 | Total Loss Per Epoch: 1.1869\n",
      "Epoch: 117 | Total Loss Per Epoch: 1.1869\n",
      "Epoch: 118 | Total Loss Per Epoch: 1.1869\n",
      "Epoch: 119 | Total Loss Per Epoch: 1.1869\n",
      "Epoch: 120 | Total Loss Per Epoch: 1.1869\n",
      "Epoch: 121 | Total Loss Per Epoch: 1.1869\n",
      "Epoch: 122 | Total Loss Per Epoch: 1.1868\n",
      "Epoch: 123 | Total Loss Per Epoch: 1.1868\n",
      "Epoch: 124 | Total Loss Per Epoch: 1.1868\n",
      "Epoch: 125 | Total Loss Per Epoch: 1.1868\n",
      "Epoch: 126 | Total Loss Per Epoch: 1.1867\n",
      "Epoch: 127 | Total Loss Per Epoch: 1.1866\n",
      "Epoch: 128 | Total Loss Per Epoch: 1.1866\n",
      "Epoch: 129 | Total Loss Per Epoch: 1.1865\n",
      "Epoch: 130 | Total Loss Per Epoch: 1.1865\n",
      "Epoch: 131 | Total Loss Per Epoch: 1.1865\n",
      "Epoch: 132 | Total Loss Per Epoch: 1.1864\n",
      "Epoch: 133 | Total Loss Per Epoch: 1.1863\n",
      "Epoch: 134 | Total Loss Per Epoch: 1.1863\n",
      "Epoch: 135 | Total Loss Per Epoch: 1.1862\n",
      "Epoch: 136 | Total Loss Per Epoch: 1.1862\n",
      "Epoch: 137 | Total Loss Per Epoch: 1.1861\n",
      "Epoch: 138 | Total Loss Per Epoch: 1.1861\n",
      "Epoch: 139 | Total Loss Per Epoch: 1.1860\n",
      "Epoch: 140 | Total Loss Per Epoch: 1.1860\n",
      "Epoch: 141 | Total Loss Per Epoch: 1.1859\n",
      "Epoch: 142 | Total Loss Per Epoch: 1.1859\n",
      "Epoch: 143 | Total Loss Per Epoch: 1.1859\n",
      "Epoch: 144 | Total Loss Per Epoch: 1.1859\n",
      "Epoch: 145 | Total Loss Per Epoch: 1.1859\n",
      "Epoch: 146 | Total Loss Per Epoch: 1.1859\n",
      "Epoch: 147 | Total Loss Per Epoch: 1.1859\n",
      "Epoch: 148 | Total Loss Per Epoch: 1.1859\n",
      "Epoch: 149 | Total Loss Per Epoch: 1.1859\n",
      "Epoch: 150 | Total Loss Per Epoch: 1.1858\n",
      "Epoch: 151 | Total Loss Per Epoch: 1.1858\n",
      "Epoch: 152 | Total Loss Per Epoch: 1.1858\n",
      "Epoch: 153 | Total Loss Per Epoch: 1.1859\n",
      "Epoch: 154 | Total Loss Per Epoch: 1.1858\n",
      "Epoch: 155 | Total Loss Per Epoch: 1.1858\n",
      "Epoch: 156 | Total Loss Per Epoch: 1.1858\n",
      "Epoch: 157 | Total Loss Per Epoch: 1.1858\n",
      "Epoch: 158 | Total Loss Per Epoch: 1.1858\n",
      "Epoch: 159 | Total Loss Per Epoch: 1.1857\n",
      "Epoch: 160 | Total Loss Per Epoch: 1.1857\n",
      "Epoch: 161 | Total Loss Per Epoch: 1.1856\n",
      "Epoch: 162 | Total Loss Per Epoch: 1.1856\n",
      "Epoch: 163 | Total Loss Per Epoch: 1.1856\n",
      "Epoch: 164 | Total Loss Per Epoch: 1.1855\n",
      "Epoch: 165 | Total Loss Per Epoch: 1.1855\n",
      "Epoch: 166 | Total Loss Per Epoch: 1.1855\n",
      "Epoch: 167 | Total Loss Per Epoch: 1.1855\n",
      "Epoch: 168 | Total Loss Per Epoch: 1.1855\n",
      "Epoch: 169 | Total Loss Per Epoch: 1.1854\n",
      "Epoch: 170 | Total Loss Per Epoch: 1.1854\n",
      "Epoch: 171 | Total Loss Per Epoch: 1.1854\n",
      "Epoch: 172 | Total Loss Per Epoch: 1.1854\n",
      "Epoch: 173 | Total Loss Per Epoch: 1.1853\n",
      "Epoch: 174 | Total Loss Per Epoch: 1.1853\n",
      "Epoch: 175 | Total Loss Per Epoch: 1.1853\n",
      "Epoch: 176 | Total Loss Per Epoch: 1.1853\n",
      "Epoch: 177 | Total Loss Per Epoch: 1.1852\n",
      "Epoch: 178 | Total Loss Per Epoch: 1.1852\n",
      "Epoch: 179 | Total Loss Per Epoch: 1.1852\n",
      "Epoch: 180 | Total Loss Per Epoch: 1.1852\n",
      "Epoch: 181 | Total Loss Per Epoch: 1.1851\n",
      "Epoch: 182 | Total Loss Per Epoch: 1.1851\n",
      "Epoch: 183 | Total Loss Per Epoch: 1.1851\n",
      "Epoch: 184 | Total Loss Per Epoch: 1.1851\n",
      "Epoch: 185 | Total Loss Per Epoch: 1.1850\n",
      "Epoch: 186 | Total Loss Per Epoch: 1.1850\n",
      "Epoch: 187 | Total Loss Per Epoch: 1.1850\n",
      "Epoch: 188 | Total Loss Per Epoch: 1.1850\n",
      "Epoch: 189 | Total Loss Per Epoch: 1.1850\n",
      "Epoch: 190 | Total Loss Per Epoch: 1.1849\n",
      "Epoch: 191 | Total Loss Per Epoch: 1.1849\n",
      "Epoch: 192 | Total Loss Per Epoch: 1.1849\n",
      "Epoch: 193 | Total Loss Per Epoch: 1.1849\n",
      "Epoch: 194 | Total Loss Per Epoch: 1.1849\n",
      "Epoch: 195 | Total Loss Per Epoch: 1.1848\n",
      "Epoch: 196 | Total Loss Per Epoch: 1.1848\n",
      "Epoch: 197 | Total Loss Per Epoch: 1.1848\n",
      "Epoch: 198 | Total Loss Per Epoch: 1.1848\n",
      "Epoch: 199 | Total Loss Per Epoch: 1.1848\n",
      "Epoch: 200 | Total Loss Per Epoch: 1.1847\n",
      "Epoch: 201 | Total Loss Per Epoch: 1.1847\n",
      "Epoch: 202 | Total Loss Per Epoch: 1.1847\n",
      "Epoch: 203 | Total Loss Per Epoch: 1.1847\n",
      "Epoch: 204 | Total Loss Per Epoch: 1.1847\n",
      "Epoch: 205 | Total Loss Per Epoch: 1.1847\n",
      "Epoch: 206 | Total Loss Per Epoch: 1.1846\n",
      "Epoch: 207 | Total Loss Per Epoch: 1.1846\n",
      "Epoch: 208 | Total Loss Per Epoch: 1.1846\n",
      "Epoch: 209 | Total Loss Per Epoch: 1.1846\n",
      "Epoch: 210 | Total Loss Per Epoch: 1.1846\n",
      "Epoch: 211 | Total Loss Per Epoch: 1.1846\n",
      "Epoch: 212 | Total Loss Per Epoch: 1.1846\n",
      "Epoch: 213 | Total Loss Per Epoch: 1.1846\n",
      "Epoch: 214 | Total Loss Per Epoch: 1.1845\n",
      "Epoch: 215 | Total Loss Per Epoch: 1.1845\n",
      "Epoch: 216 | Total Loss Per Epoch: 1.1845\n",
      "Epoch: 217 | Total Loss Per Epoch: 1.1845\n",
      "Epoch: 218 | Total Loss Per Epoch: 1.1845\n",
      "Epoch: 219 | Total Loss Per Epoch: 1.1845\n",
      "Epoch: 220 | Total Loss Per Epoch: 1.1845\n",
      "Epoch: 221 | Total Loss Per Epoch: 1.1845\n",
      "Epoch: 222 | Total Loss Per Epoch: 1.1845\n",
      "Epoch: 223 | Total Loss Per Epoch: 1.1845\n",
      "Epoch: 224 | Total Loss Per Epoch: 1.1845\n",
      "Epoch: 225 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 226 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 227 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 228 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 229 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 230 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 231 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 232 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 233 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 234 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 235 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 236 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 237 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 238 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 239 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 240 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 241 | Total Loss Per Epoch: 1.1844\n",
      "Epoch: 242 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 243 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 244 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 245 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 246 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 247 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 248 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 249 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 250 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 251 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 252 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 253 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 254 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 255 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 256 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 257 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 258 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 259 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 260 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 261 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 262 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 263 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 264 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 265 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 266 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 267 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 268 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 269 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 270 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 271 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 272 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 273 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 274 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 275 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 276 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 277 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 278 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 279 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 280 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 281 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 282 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 283 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 284 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 285 | Total Loss Per Epoch: 1.1843\n",
      "Epoch: 286 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 287 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 288 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 289 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 290 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 291 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 292 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 293 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 294 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 295 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 296 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 297 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 298 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 299 | Total Loss Per Epoch: 1.1842\n",
      "Epoch: 300 | Total Loss Per Epoch: 1.1842\n"
     ]
    }
   ],
   "source": [
    "data_loader_bkg_dict = {\n",
    "    0: data_loader['ll'],  # For label 0\n",
    "    2: data_loader['cl'],  # For label 2\n",
    "    3: data_loader['lc'],  # For label 3\n",
    "    4: data_loader['cc']   # For label 4\n",
    "}\n",
    "\n",
    "loss_per_epoch = []\n",
    "scores = []\n",
    "truth_labels = []\n",
    "avg_loss_training_true_class = []\n",
    "logits_training_true_class = []\n",
    "avg_loss_testing_true_class = []\n",
    "logits_testing_true_class = []\n",
    "avg_loss_training_bkg_classes = []\n",
    "logits_training_bkg_classes = []\n",
    "avg_loss_testing_bkg_classes = []\n",
    "logits_testing_bkg_classes = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    total_loss_per_epoch = train_model(\n",
    "        model, device, data_loader['train'], optimizer, criterion)\n",
    "    # Ensure tensor is detached for saving\n",
    "    loss_per_epoch.append(total_loss_per_epoch.cpu().detach().numpy())\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Test the model\n",
    "    epoch_scores, epoch_true_labels = test_model(\n",
    "        model, device, data_loader['tt'], data_loader_bkg_dict)\n",
    "\n",
    "    # Compute the average loss for true and background edges\n",
    "    avgLossTrueTrain, logitsTrueTrain = loss_for_train_and_test(\n",
    "        model, data_loader['train'], criterion, optimizer, True, device)\n",
    "    avgLossTrueTest, logitsTrueTest = loss_for_train_and_test(\n",
    "        model, data_loader['tt'], criterion, optimizer, False, device)\n",
    "    avgLossBkgTrain, logitsBkgTrain = loss_for_train_and_test(\n",
    "        model, data_loader['train'], criterion, optimizer, True, device)\n",
    "    avgLossBkgTest, logtisBkgTest = loss_for_train_and_test(\n",
    "        model, data_loader_total_bkg, criterion, optimizer, False, device)\n",
    "\n",
    "    # Store results\n",
    "    scores.append(epoch_scores)\n",
    "    truth_labels.append(epoch_true_labels)\n",
    "    avg_loss_training_true_class.append(avgLossTrueTrain)\n",
    "    logits_training_true_class.append(logitsTrueTrain)\n",
    "    avg_loss_testing_true_class.append(avgLossTrueTest)\n",
    "    logits_testing_true_class.append(logitsTrueTest)\n",
    "    avg_loss_training_bkg_classes.append(avgLossBkgTrain)\n",
    "    logits_training_bkg_classes.append(logitsBkgTrain)\n",
    "    avg_loss_testing_bkg_classes.append(avgLossBkgTest)\n",
    "    logits_testing_bkg_classes.append(logtisBkgTest)\n",
    "\n",
    "    # Print the loss for the current epoch\n",
    "    print(f\"Epoch: {epoch+1} | Total Loss Per Epoch: {total_loss_per_epoch.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6029ed61-47ce-4157-aa55-5d75b7c08202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300, 24000, 5)\n",
      "(300, 24000)\n",
      "(300,)\n",
      "(300,)\n",
      "(300,)\n",
      "(300, 90000, 5)\n",
      "(300,)\n",
      "(300,)\n",
      "(300,)\n",
      "(300, 630000, 5)\n"
     ]
    }
   ],
   "source": [
    "loss_per_epoch = np.array(loss_per_epoch)\n",
    "scores = np.array(scores)\n",
    "truth_labels = np.array(truth_labels)\n",
    "avg_loss_training_true_class = np.array(avg_loss_training_true_class)\n",
    "logits_training_true_class = np.array(logits_training_true_class)\n",
    "avg_loss_testing_true_class = np.array(avg_loss_testing_true_class)\n",
    "logits_testing_true_class = np.array(logits_testing_true_class)\n",
    "avg_loss_training_bkg_classes = np.array(avg_loss_training_bkg_classes)\n",
    "logits_training_bkg_classes = np.array(logits_training_bkg_classes)\n",
    "avg_loss_testing_bkg_classes = np.array(avg_loss_testing_bkg_classes)\n",
    "logits_testing_bkg_classes = np.array(logits_testing_bkg_classes)\n",
    "\n",
    "print(loss_per_epoch.shape)\n",
    "print(scores.shape)\n",
    "print(truth_labels.shape)\n",
    "print(avg_loss_training_true_class.shape)\n",
    "print(logits_training_true_class.shape)\n",
    "print(avg_loss_testing_true_class.shape)\n",
    "print(logits_testing_true_class.shape)\n",
    "print(avg_loss_training_bkg_classes.shape)\n",
    "print(logits_training_bkg_classes.shape)\n",
    "print(avg_loss_testing_bkg_classes.shape)\n",
    "print(logits_testing_bkg_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_of_epochs = np.arange(len(loss_per_epoch))\n",
    "\n",
    "plt.plot(array_of_epochs, loss_per_epoch, label='Loss per Epoch')\n",
    "plt.plot(array_of_epochs, avg_loss_training_true_class,'dimgray', label='Avg Loss True Train')\n",
    "plt.plot(array_of_epochs, avg_loss_testing_true_class,'silver', label='Avg Loss True Test')\n",
    "plt.plot(array_of_epochs, avg_loss_training_bkg_classes,'olive', label='Avg Loss Bkg Train')\n",
    "plt.plot(array_of_epochs, avg_loss_testing_bkg_classes,'lightgreen', label='Avg Loss Bkg Test')\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Epoch for Multi-Class Classification\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3761c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract last epoch data (epoch 299)\n",
    "epoch_idx = -1  # Last epoch\n",
    "y_score = scores[epoch_idx]  # Shape (51000, 5)\n",
    "y_true = truth_labels[epoch_idx]  # Shape (51000,)\n",
    "\n",
    "# Class name mapping\n",
    "class_names = {\n",
    "    0: \"Lone-Lone\",\n",
    "    1: \"True-True\",\n",
    "    2: \"Cluster-Lone\",\n",
    "    3: \"Lone-Cluster\",\n",
    "    4: \"Cluster-Cluster\"\n",
    "}\n",
    "\n",
    "# Binarize the true labels for multi-class ROC calculation\n",
    "num_classes = y_score.shape[1]\n",
    "y_true_binarized = label_binarize(y_true, classes=np.arange(num_classes))\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "for i in range(num_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_true_binarized[:, i], y_score[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.3f})\")\n",
    "\n",
    "# Plot diagonal reference line\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves for Last Epoch (Epoch 299)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class names\n",
    "class_names = ['Lone-Lone', 'True-True', 'Cluster-Lone', 'Lone-Cluster', 'Cluster-Cluster']\n",
    "colors = ['Blue', 'Orange', 'Green', 'Red', 'Purple']\n",
    "\n",
    "# Extract last epoch data\n",
    "epoch_index = -1  # Last epoch\n",
    "epoch_scores = scores[epoch_index]  # Shape: (24000, 5)\n",
    "epoch_truth_labels = truth_labels[epoch_index]  # Shape: (24000,)\n",
    "\n",
    "# Compute optimal thresholds using Youden's J statistic\n",
    "optimal_thresholds = {}\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    y_true_binary = (epoch_truth_labels == i).astype(int)  # Convert to binary labels for ROC\n",
    "    y_scores_class = epoch_scores[:, i]  # Scores for class i\n",
    "    \n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true_binary, y_scores_class)\n",
    "    \n",
    "    # Compute Youden’s J statistic\n",
    "    J_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(J_scores)  # Find the best threshold index\n",
    "    optimal_thresholds[class_name] = thresholds[optimal_idx]  # Store the optimal threshold\n",
    "\n",
    "# Print computed thresholds\n",
    "print(f\"Optimal Lone-Lone Threshold: {optimal_thresholds['Lone-Lone']:.3f}\")\n",
    "print(f\"Optimal True-True Threshold: {optimal_thresholds['True-True']:.3f}\")\n",
    "print(f\"Optimal Cluster-Lone Threshold: {optimal_thresholds['Cluster-Lone']:.3f}\")\n",
    "print(f\"Optimal Lone-Cluster Threshold: {optimal_thresholds['Lone-Cluster']:.3f}\")\n",
    "print(f\"Optimal Cluster-Cluster Threshold: {optimal_thresholds['Cluster-Cluster']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba09d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (15, 10)\n",
    "\n",
    "# Class-wise score distributions\n",
    "rows, cols = 2, 3\n",
    "fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Add a global title for the entire figure\n",
    "fig.suptitle(\"Class-wise Score Distributions with Optimal Threshold Overlays\", fontsize=16)\n",
    "\n",
    "for class_idx, class_name in enumerate(class_names):\n",
    "    ax = axes[class_idx]\n",
    "    ax.set_title(f'{class_name}')  # Use class name instead of \"Output Class {class_idx}\"\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "    # Get the optimal threshold for this class\n",
    "    optimal_thresh = optimal_thresholds[class_name]\n",
    "\n",
    "    for truth_type in sorted(np.unique(epoch_truth_labels)):\n",
    "        # Get scores for the current truth type\n",
    "        scores_for_truth_type = epoch_scores[epoch_truth_labels == truth_type, class_idx]\n",
    "\n",
    "        # Calculate the fraction of samples with scores > optimal threshold\n",
    "        if scores_for_truth_type.size > 0:  # Avoid issues with empty slices\n",
    "            fraction_above_threshold = np.mean(scores_for_truth_type > optimal_thresh)\n",
    "        else:\n",
    "            fraction_above_threshold = 0.0  # Default if no samples exist\n",
    "\n",
    "        # Plot normalized histogram\n",
    "        ax.hist(\n",
    "            scores_for_truth_type,\n",
    "            bins=50,\n",
    "            density=True,  # Normalize the histogram\n",
    "            alpha=0.6,\n",
    "            label=f'Truth {truth_type} (> {fraction_above_threshold:.3%})',\n",
    "            color=colors[truth_type % len(colors)]\n",
    "        )\n",
    "\n",
    "    # Add vertical line for the optimal threshold\n",
    "    ax.axvline(optimal_thresh, color='black', linestyle='dashed', linewidth=2, label=f'Threshold: {optimal_thresh:.3f}')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "# Hide the extra subplot if the number of classes is less than total subplots\n",
    "if len(axes) > len(class_names):\n",
    "    fig.delaxes(axes[-1])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the title\n",
    "plt.show()\n",
    "\n",
    "# Truth-type wise score distributions\n",
    "rows, cols = 2, 3\n",
    "fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "axes = axes.flatten()\n",
    "\n",
    "unique_truth_types = sorted(np.unique(epoch_truth_labels))\n",
    "\n",
    "# Add a global title for the entire figure\n",
    "fig.suptitle(\n",
    "    \"Truth Type-wise Score Distributions for All Output Classes at Final Epoch\", fontsize=16)\n",
    "\n",
    "# Handle cases where only one truth type is present\n",
    "if len(unique_truth_types) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for truth_type_idx, truth_type in enumerate(unique_truth_types):\n",
    "    ax = axes[truth_type_idx]\n",
    "    ax.set_title(f'Truth Type {truth_type}')\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "    # Determine which class is associated with this truth type\n",
    "    associated_class_name = class_names[truth_type]\n",
    "    associated_class_idx = truth_type  # Since classes and truth types are aligned\n",
    "    optimal_threshold = optimal_thresholds[associated_class_name]  # Get the threshold\n",
    "\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        # Get scores for the current output class and the current truth type\n",
    "        scores_for_truth_type_class = epoch_scores[epoch_truth_labels == truth_type, class_idx]\n",
    "\n",
    "        # Calculate the fraction of scores above the associated class threshold\n",
    "        if scores_for_truth_type_class.size > 0:\n",
    "            fraction_above_threshold = np.mean(scores_for_truth_type_class > optimal_threshold) * 100\n",
    "        else:\n",
    "            fraction_above_threshold = 0.0  # Default if no samples exist\n",
    "\n",
    "        # Plot normalized histogram for the current output class\n",
    "        ax.hist(\n",
    "            scores_for_truth_type_class,\n",
    "            bins=50,\n",
    "            density=True,  # Normalize the histogram\n",
    "            alpha=0.6,\n",
    "            label=f'Class {class_idx} ({fraction_above_threshold:.3f}%)',\n",
    "            color=colors[class_idx % len(colors)]\n",
    "        )\n",
    "\n",
    "    # Add threshold line ONLY for the associated class\n",
    "    ax.axvline(optimal_threshold, color='black', linestyle='dashed', linewidth=2, label=f'Threshold: {optimal_threshold:.3f}')\n",
    "\n",
    "    # Add legend with updated labels including fraction above threshold\n",
    "    ax.legend()\n",
    "\n",
    "# Remove unused subplots\n",
    "for idx in range(len(unique_truth_types), len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color map choice\n",
    "cmap = 'YlGnBu'\n",
    "\n",
    "# Create a plot for each class\n",
    "for class_idx, class_name in enumerate(class_names):\n",
    "    # Get the optimal threshold for this class\n",
    "    optimal_thresh = optimal_thresholds[class_name]\n",
    "    \n",
    "    # Initialize predictions array for this class-based threshold\n",
    "    y_pred = np.zeros_like(epoch_truth_labels)\n",
    "    \n",
    "    # Predict the class for each sample using only this class's threshold\n",
    "    for class_idx_pred in range(len(class_names)):\n",
    "        y_scores_class = epoch_scores[:, class_idx_pred]  # Scores for this class\n",
    "        predicted_class = (y_scores_class >= optimal_thresh).astype(int)\n",
    "        \n",
    "        # Assign the predicted class to the sample for this class threshold\n",
    "        y_pred[predicted_class == 1] = class_idx_pred\n",
    "    \n",
    "    # Generate the confusion matrix for this class-based threshold\n",
    "    cm = confusion_matrix(epoch_truth_labels, y_pred)\n",
    "    \n",
    "    # Plot the confusion matrix using Seaborn\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap, xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix with Threshold for {class_name}')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
